---
pmid: '11805826'
title: Functional organization of the yeast proteome by systematic analysis of protein
  complexes.
authors:
- Gavin AC
- Bösche M
- Krause R
- Grandi P
- Marzioch M
- Bauer A
- Schultz J
- Rick JM
- Michon AM
- Cruciat CM
- Remor M
- Höfert C
- Schelder M
- Brajenovic M
- Ruffner H
- Merino A
- Klein K
- Hudak M
- Dickson D
- Rudi T
- Gnau V
- Bauch A
- Bastuck S
- Huhse B
- Leutwein C
- Heurtier MA
- Copley RR
- Edelmann A
- Querfurth E
- Rybin V
- Drewes G
- Raida M
- Bouwmeester T
- Bork P
- Seraphin B
- Kuster B
- Neubauer G
- Superti-Furga G
journal: Nature
year: '2002'
full_text_available: true
pmcid: PMC12337455
doi: 10.1038/415141a
---

# Functional organization of the yeast proteome by systematic analysis of protein complexes.
**Authors:** Gavin AC, Bösche M, Krause R, Grandi P, Marzioch M, Bauer A, Schultz J, Rick JM, Michon AM, Cruciat CM, Remor M, Höfert C, Schelder M, Brajenovic M, Ruffner H, Merino A, Klein K, Hudak M, Dickson D, Rudi T, Gnau V, Bauch A, Bastuck S, Huhse B, Leutwein C, Heurtier MA, Copley RR, Edelmann A, Querfurth E, Rybin V, Drewes G, Raida M, Bouwmeester T, Bork P, Seraphin B, Kuster B, Neubauer G, Superti-Furga G
**Journal:** Nature (2002)
**DOI:** [10.1038/415141a](https://doi.org/10.1038/415141a)
**PMC:** [PMC12337455](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC12337455/)

## Abstract

1. Nature. 2002 Jan 10;415(6868):141-7. doi: 10.1038/415141a.

Functional organization of the yeast proteome by systematic analysis of protein 
complexes.

Gavin AC(1), Bösche M, Krause R, Grandi P, Marzioch M, Bauer A, Schultz J, Rick 
JM, Michon AM, Cruciat CM, Remor M, Höfert C, Schelder M, Brajenovic M, Ruffner 
H, Merino A, Klein K, Hudak M, Dickson D, Rudi T, Gnau V, Bauch A, Bastuck S, 
Huhse B, Leutwein C, Heurtier MA, Copley RR, Edelmann A, Querfurth E, Rybin V, 
Drewes G, Raida M, Bouwmeester T, Bork P, Seraphin B, Kuster B, Neubauer G, 
Superti-Furga G.

Author information:
(1)Cellzome AG, Meyerhofstrasse 1, 69117 Heidelberg, Germany. 
anne-claude.gavin@cellzome.com

Comment in
    Nature. 2002 Jan 10;415(6868):123-4. doi: 10.1038/415123a.

Most cellular processes are carried out by multiprotein complexes. The 
identification and analysis of their components provides insight into how the 
ensemble of expressed proteins (proteome) is organized into functional units. We 
used tandem-affinity purification (TAP) and mass spectrometry in a large-scale 
approach to characterize multiprotein complexes in Saccharomyces cerevisiae. We 
processed 1,739 genes, including 1,143 human orthologues of relevance to human 
biology, and purified 589 protein assemblies. Bioinformatic analysis of these 
assemblies defined 232 distinct multiprotein complexes and proposed new cellular 
roles for 344 proteins, including 231 proteins with no previous functional 
annotation. Comparison of yeast and human complexes showed that conservation 
across species extends from single proteins to their molecular environment. Our 
analysis provides an outline of the eukaryotic proteome as a network of protein 
complexes at a level of organization beyond binary interactions. This 
higher-order map contains fundamental biological information and offers the 
context for a more reasoned and informed approach to drug discovery.

DOI: 10.1038/415141a
PMID: 11805826 [Indexed for MEDLINE]

## Full Text

Background
Protein–protein interactions (PPIs) play a critically important role in a wide range of biological processes [1], including cellular signal transduction, metabolic regulation, and disease pathogenesis [2]. In addition to serving as the basis of intricate biological networks within cells, PPIs are also pivotal for understanding cellular function and essential life processes [3, 4]. By investigating PPIs, scientists can unravel the specific functions and mechanisms of proteins inside cells, thereby making valuable contributions to various fields, including molecular biology, drug design, and drug target identification [5, 6].
Traditional methods for PPI prediction contain yeast two-hybrid and tandem affinity purification [7, 8]; due to the constraints of experiments, these methods are time-consuming and labor-intensive. Moreover, these experimental approaches frequently encounter issues of false positives and false negatives, compromising data accuracy and reliability [9].
To enhance prediction efficiency and reduce costs, the academic community has progressively shifted its focus towards computational approaches exemplified by machine learning (ML) [10, 11]. In PPI prediction studies, common machine learning classifiers include support vector machines (SVM) [12], random forests (RF) [13], logistic regression (LR) [14], and k-nearest neighbors (KNN) [15]. In PPI prediction, the majority of approaches rely on manual feature engineering of protein sequences, structures, and evolutionary information, which will be input into various classifiers for prediction [16], including sequence features (amino acid composition, physicochemical properties) [17], structural features (protein secondary or three-dimensional structures composition) [18], and evolutionary features(position-specific scoring matrix and homology protein alignment information) [19]. Although machine learning-based studies on PPI have significantly enhanced prediction efficiency, these methods typically require researchers to possess interdisciplinary expertise in both biology and computer science to design or select effective features [20]. However, PPI inherently encompass more intricate structural and evolutionary information, which manual feature engineering fails to comprehensively capture [21]. This limitation hinders models from fully uncovering latent features, thereby restricting their predictive performance.To overcome the limitations of ML methods, deep learning (DL) employs multi-layer neural network architectures that can automatically learn hierarchical feature representations from large-scale data, thereby reducing the reliance on manual feature engineering [22, 23]. Compared to conventional machine learning approaches, deep learning enables end-to-end training on raw inputs (such as sequences and graph structures), offering significant advantages in capturing complex nonlinear relationships and high-order feature interactions [24]. Du [25] designed the Deep-PPI model, which employs Res2Vec to embed each residue into a fixed-dimensional feature vector. For each protein sequence, the residues are concatenated and processed through a deep learning model, integrating the outputs of a deep neural network (DNN) for classification. Somaye [26] developed the DPPI model, which utilizes a sliding window approach to extract critical features of target amino acids and transforms each original sequence into a high-dimensional position-specific profile representation. The final prediction module consists of a linear layer that calculates the probability of interaction between two proteins. Additionally, inspired by the success of natural language processing (NLP) models, researchers have developed protein language models (PLMs) such as ESM [27], ProteinBERT [28], and ProtTrans [29] to characterize protein features. Furthermore, the graph-based structure of protein data can provide a multitude of valuable features for deep learning models [30]. In such graphs, each protein sequence is represented as a node, and each edge signifies the relationship between the two connected nodes. Consequently, graph neural networks (GNNs) are extensively employed in the application of PPI prediction.These approaches primarily rely on single-modal information, such as sequence features, structural features, and network topological features, which introduces inherent limitations in PPI prediction [31]. For instance, sequence-based methods typically depend on the similarity and conservation of amino acid sequences, making it challenging to adequately capture the three-dimensional structures and dynamic conformational changes of proteins [32]. Although structure-based methods can provide more detailed interaction information, the substantial difficulty in acquiring protein structural data restricts their applicability in large-scale PPI predictions [33]. Similarly, network topology-based approaches heavily depend on existing PPI network information, rendering effective predictions difficult in the absence of prior knowledge [34]. Consequently, single-modal information proves insufficient in addressing the diverse and complex nature of PPI prediction tasks.To address the challenges mentioned above, this study proposes the Multimodal Encoding Subgraph Model (MESM). The overall architecture of the MESM learning process is shown in Fig. 1. MESM decouples the end-to-end PPI prediction process into two main stages: pre-training of multimodal proteins and extraction of protein interaction information based on multimodal proteins and the PPI network. MESM uses an unsupervised learning approach to pre-train protein data and effectively integrates protein sequence features, graph structure features, and three-dimensional spatial features through a multimodal encoding mechanism. During the pre-training phase, MESM uses a Sequence Variational Autoencoder (SVAE) to extract protein sequence information, learns graph structure features through a Variational Graph Autoencoder (VGAE), and captures the three-dimensional spatial structure features of proteins using a PointNet Autoencoder (PAE). Finally, MESM integrates these three modalities and achieves effective multimodal information fusion using a Fusion Autoencoder (FAE). In the PPI prediction task, MESM explores protein interaction features in the PPI network from three perspectives: graph structure, overall graph, and subgraphs, using GraphGPS, Graph Convolutional Network (GCN), and SubgraphGCN models, respectively. To address the uneven distribution of the seven types of PPIs, we extracted seven corresponding graphs from the overall PPI network graph, enhancing the model's learning capacity for different PPI types. In this way, MESM not only improves the efficiency of protein interaction feature extraction but also strengthens the ability to recognize different types of interactions. MESM is a deep learning framework specifically designed for the PPI prediction task, aiming to effectively predict the interaction types between proteins by learning from the multimodal protein information and the PPI network during the training process. MESM demonstrated outstanding performance across four datasets, outperforming the state-of-the-art baseline methods.Fig. 1The overall architecture of MESM. a The pretraining of multimodal protein features. First, SVAE, VGAE, and PAE are used to learn sequence features, structure features, and point cloud features from the protein's sequence, structure, and point cloud data, respectively. Then, these three different types of features are concatenated to obtain fused features, which are input into FAE to learn multimodal protein features and balance the influence of each modality. b The overall process of PPI prediction. First, the processed multimodal protein features and seven PPI networks are sequentially input into the PPI prediction layer with independent weights. Then, after extracting protein interaction information from the seven PPI-type network graphs, the original multimodal protein features and the learned seven protein features are input into the classifier for PPI prediction. c PPI prediction layer. First, GraphGPS is used to extract graph structure information from the PPI network graph, and then GAT is used to reassign the weights of protein nodes. Then, SubgraphGCN and GCN are used to further extract protein interaction information from the subgraph and full graph perspectives, and the subgraph and full graph information are fused. Finally, protein features generated based on the network graph of a specific PPI type are obtained

Protein–protein interactions (PPIs) play a critically important role in a wide range of biological processes [1], including cellular signal transduction, metabolic regulation, and disease pathogenesis [2]. In addition to serving as the basis of intricate biological networks within cells, PPIs are also pivotal for understanding cellular function and essential life processes [3, 4]. By investigating PPIs, scientists can unravel the specific functions and mechanisms of proteins inside cells, thereby making valuable contributions to various fields, including molecular biology, drug design, and drug target identification [5, 6].

Traditional methods for PPI prediction contain yeast two-hybrid and tandem affinity purification [7, 8]; due to the constraints of experiments, these methods are time-consuming and labor-intensive. Moreover, these experimental approaches frequently encounter issues of false positives and false negatives, compromising data accuracy and reliability [9].

To enhance prediction efficiency and reduce costs, the academic community has progressively shifted its focus towards computational approaches exemplified by machine learning (ML) [10, 11]. In PPI prediction studies, common machine learning classifiers include support vector machines (SVM) [12], random forests (RF) [13], logistic regression (LR) [14], and k-nearest neighbors (KNN) [15]. In PPI prediction, the majority of approaches rely on manual feature engineering of protein sequences, structures, and evolutionary information, which will be input into various classifiers for prediction [16], including sequence features (amino acid composition, physicochemical properties) [17], structural features (protein secondary or three-dimensional structures composition) [18], and evolutionary features(position-specific scoring matrix and homology protein alignment information) [19]. Although machine learning-based studies on PPI have significantly enhanced prediction efficiency, these methods typically require researchers to possess interdisciplinary expertise in both biology and computer science to design or select effective features [20]. However, PPI inherently encompass more intricate structural and evolutionary information, which manual feature engineering fails to comprehensively capture [21]. This limitation hinders models from fully uncovering latent features, thereby restricting their predictive performance.

To overcome the limitations of ML methods, deep learning (DL) employs multi-layer neural network architectures that can automatically learn hierarchical feature representations from large-scale data, thereby reducing the reliance on manual feature engineering [22, 23]. Compared to conventional machine learning approaches, deep learning enables end-to-end training on raw inputs (such as sequences and graph structures), offering significant advantages in capturing complex nonlinear relationships and high-order feature interactions [24]. Du [25] designed the Deep-PPI model, which employs Res2Vec to embed each residue into a fixed-dimensional feature vector. For each protein sequence, the residues are concatenated and processed through a deep learning model, integrating the outputs of a deep neural network (DNN) for classification. Somaye [26] developed the DPPI model, which utilizes a sliding window approach to extract critical features of target amino acids and transforms each original sequence into a high-dimensional position-specific profile representation. The final prediction module consists of a linear layer that calculates the probability of interaction between two proteins. Additionally, inspired by the success of natural language processing (NLP) models, researchers have developed protein language models (PLMs) such as ESM [27], ProteinBERT [28], and ProtTrans [29] to characterize protein features. Furthermore, the graph-based structure of protein data can provide a multitude of valuable features for deep learning models [30]. In such graphs, each protein sequence is represented as a node, and each edge signifies the relationship between the two connected nodes. Consequently, graph neural networks (GNNs) are extensively employed in the application of PPI prediction.

These approaches primarily rely on single-modal information, such as sequence features, structural features, and network topological features, which introduces inherent limitations in PPI prediction [31]. For instance, sequence-based methods typically depend on the similarity and conservation of amino acid sequences, making it challenging to adequately capture the three-dimensional structures and dynamic conformational changes of proteins [32]. Although structure-based methods can provide more detailed interaction information, the substantial difficulty in acquiring protein structural data restricts their applicability in large-scale PPI predictions [33]. Similarly, network topology-based approaches heavily depend on existing PPI network information, rendering effective predictions difficult in the absence of prior knowledge [34]. Consequently, single-modal information proves insufficient in addressing the diverse and complex nature of PPI prediction tasks.

To address the challenges mentioned above, this study proposes the Multimodal Encoding Subgraph Model (MESM). The overall architecture of the MESM learning process is shown in Fig. 1. MESM decouples the end-to-end PPI prediction process into two main stages: pre-training of multimodal proteins and extraction of protein interaction information based on multimodal proteins and the PPI network. MESM uses an unsupervised learning approach to pre-train protein data and effectively integrates protein sequence features, graph structure features, and three-dimensional spatial features through a multimodal encoding mechanism. During the pre-training phase, MESM uses a Sequence Variational Autoencoder (SVAE) to extract protein sequence information, learns graph structure features through a Variational Graph Autoencoder (VGAE), and captures the three-dimensional spatial structure features of proteins using a PointNet Autoencoder (PAE). Finally, MESM integrates these three modalities and achieves effective multimodal information fusion using a Fusion Autoencoder (FAE). In the PPI prediction task, MESM explores protein interaction features in the PPI network from three perspectives: graph structure, overall graph, and subgraphs, using GraphGPS, Graph Convolutional Network (GCN), and SubgraphGCN models, respectively. To address the uneven distribution of the seven types of PPIs, we extracted seven corresponding graphs from the overall PPI network graph, enhancing the model's learning capacity for different PPI types. In this way, MESM not only improves the efficiency of protein interaction feature extraction but also strengthens the ability to recognize different types of interactions. MESM is a deep learning framework specifically designed for the PPI prediction task, aiming to effectively predict the interaction types between proteins by learning from the multimodal protein information and the PPI network during the training process. MESM demonstrated outstanding performance across four datasets, outperforming the state-of-the-art baseline methods.Fig. 1The overall architecture of MESM. a The pretraining of multimodal protein features. First, SVAE, VGAE, and PAE are used to learn sequence features, structure features, and point cloud features from the protein's sequence, structure, and point cloud data, respectively. Then, these three different types of features are concatenated to obtain fused features, which are input into FAE to learn multimodal protein features and balance the influence of each modality. b The overall process of PPI prediction. First, the processed multimodal protein features and seven PPI networks are sequentially input into the PPI prediction layer with independent weights. Then, after extracting protein interaction information from the seven PPI-type network graphs, the original multimodal protein features and the learned seven protein features are input into the classifier for PPI prediction. c PPI prediction layer. First, GraphGPS is used to extract graph structure information from the PPI network graph, and then GAT is used to reassign the weights of protein nodes. Then, SubgraphGCN and GCN are used to further extract protein interaction information from the subgraph and full graph perspectives, and the subgraph and full graph information are fused. Finally, protein features generated based on the network graph of a specific PPI type are obtained

The overall architecture of MESM. a The pretraining of multimodal protein features. First, SVAE, VGAE, and PAE are used to learn sequence features, structure features, and point cloud features from the protein's sequence, structure, and point cloud data, respectively. Then, these three different types of features are concatenated to obtain fused features, which are input into FAE to learn multimodal protein features and balance the influence of each modality. b The overall process of PPI prediction. First, the processed multimodal protein features and seven PPI networks are sequentially input into the PPI prediction layer with independent weights. Then, after extracting protein interaction information from the seven PPI-type network graphs, the original multimodal protein features and the learned seven protein features are input into the classifier for PPI prediction. c PPI prediction layer. First, GraphGPS is used to extract graph structure information from the PPI network graph, and then GAT is used to reassign the weights of protein nodes. Then, SubgraphGCN and GCN are used to further extract protein interaction information from the subgraph and full graph perspectives, and the subgraph and full graph information are fused. Finally, protein features generated based on the network graph of a specific PPI type are obtained

ResultsBenchmark comparisonTo validate the effectiveness of our proposed method, we compared it with various baseline methods, including GNN-PPI, AFTGAN, DL-P   PI, MAPE-PPI, and BaPPI. These baseline methods have their unique features in the PPI prediction task, with their specific implementations and performances described as follows.GNN-PPI [35] utilizes one-dimensional convolution (Conv1d) and bidirectional gated recurrent units (BiGRU) to extract protein-related feature representations, and then employs Graph Isomorphism Networks (GIN) [36] to aggregate these features, generating protein graph embeddings for prediction. This approach effectively captures the complex relationships between proteins through deep learning techniques, demonstrating strong performance.AFTGAN [37] combines an Attention-Free Transformer (AFT) to extract protein sequence features and utilizes GAT to learn relationship features between protein pairs. This method provides advantages in extracting long sequence features and modeling protein interaction relationships.DL-PPI [38] captures protein sequence features and then processes them using the Inception method, ultimately employing GIN and Feature Relational Reasoning Network (FRN) to learn and predict potential relationship information between proteins. This hierarchical design effectively enhances the model’s adaptability to diverse data.MAPE-PPI [39] developed a microenvironment-aware pre-training method for protein embedding learning, combining vector quantized variational autoencoders (VQ-VAE) [40] and heterogeneous graph neural networks, successfully producing high-quality protein embedding features based on the sequence and structural information of amino acid residues in proteins, significantly reducing training time.BaPPI [41] extracts protein sequence features using operations such as Hyena Operator, Conv1d, and BiGRU, and employs GraphSAGE and antisymmetric graph convolutional networks to learn and predict the relationships between protein pairs. Additionally, seven PPI network graphs were constructed for seven types of PPI, which extract interaction information for different PPI types, enhancing the prediction performance of the model.We applied three different data partitioning algorithms on four datasets (SHS27k, SHS148k, SYS30k, and SYS60k): Breadth-First Search (BFS), Depth-First Search (DFS), and random partition. The corresponding experimental results are shown in Table 1. We display the best results in bold and the second-best results italics in all tables. The results indicate that the predictive performance of nearly all methods shows a gradual improvement according to BFS, DFS, and random partition. This phenomenon suggests that the DFS partition can select sparsely distributed proteins in the PPI network compared to the BFS partition, while the random partition selects proteins that are more dispersed in the PPI network. During training, when the model faces a sparser distribution of proteins, it usually achieves better results in the testing phase. Therefore, the BFS and DFS partition pose greater challenges to the model’s generalization ability.
Table 1Performance comparison of MESM and baselines across four datasetsDatasetPartitionschemeMethodsGNN-PPIAFTGANDL-PPIMAPE-PPIBaPPIMESMSHS27kBFS72.19 ± 4.4568.96 ± 4.0871.04 ± 4.072.00 ± 3.5477.94 ± 3.5288.65 ± 2.47DFS78.17 ± 1.7774.83 ± 1.7977.89 ± 2.0577.23 ± 1.6579.66 ± 1.7289.63 ± 0.93Random88.54 ± 0.2982.73 ± 0.5488.78 ± 0.389.09 ± 0.3788.73 ± 0.6394.35 ± 0.30SHS148kBFS76.41 ± 3.075.64 ± 2.9976.03 ± 2.178.75 ± 3.4983.25 ± 1.5788.58 ± 1.86DFS83.14 ± 1.2280.17 ± 0.8281.97 ± 1.682.76 ± 1.4181.98 ± 1.0990.39 ± 0.56Random92.37 ± 0.1486.47 ± 0.5692.38 ± 0.193.19 ± 0.1492.90 ± 0.1694.11 ± 0.52SYS30kBFS76.84 ± 0.5575.89 ± 1.2577.10 ± 0.6476.52 ± 0.5282.05 ± 1.0390.95 ± 0.37DFS86.27 ± 3.0384.33 ± 3.1486.23 ± 2.7785.88 ± 3.487.42 ± 2.0894.17 ± 1.31Random92.03 ± 0.2881.99 ± 0.6391.56 ± 0.4291.35 ± 0.487.80 ± 0.6394.60 ± 0.15SYS60kBFS81.54 ± 2.0779.38 ± 1.080.67 ± 2.2382.90 ± 2.2685.06 ± 1.291.90 ± 1.10DFS85.19 ± 0.7481.66 ± 0.7284.31 ± 0.5787.25 ± 0.2886.97 ± 0.1993.71 ± 0.21Random95.72 ± 0.1384.44 ± 0.5195.48 ± 0.1395.43 ± 0.2390.78 ± 0.1895.44 ± 0.28Furthermore, under BFS and DFS partition, MESM outperforms other methods, particularly when switching from random partition to BFS or DFS, showing a noticeably smaller degree of performance drop. When transitioning from smaller datasets SHS27k and SYS30k to larger datasets SHS148k and SYS60k, baseline methods generally exhibit superior performance. Notably, the predictive performance of MESM on smaller datasets approaches that of larger datasets, indicating that MESM can effectively utilize the PPI network and its neighboring nodes to extract meaningful information even with limited PPI data, thereby demonstrating stronger feature extraction capabilities when predicting unknown protein–protein interactions. Overall, compared to other methods, MESM typically shows lower standard deviation, which effectively validates its superiority in terms of generalization ability and robustness.As shown in Additional file 1: Table 1 and Additional file 1: Table 2, MESM demonstrated significant advantages across multiple performance metrics on the SHS27k, SHS148k, SYS30k, and SYS60k datasets, including Accuracy, Recall, Precision, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), Area Under the Precision-Recall Curve (AUC-PR), and Micro-F1. In terms of Accuracy, MESM achieved higher overall prediction accuracy by optimizing feature representation capabilities. The significant improvement in Recall indicates that MESM has a stronger ability to capture positive samples. The simultaneous optimization of Precision further demonstrates MESM’s effectiveness in reducing misjudgments. The notable improvement in AUC-ROC reflects MESM’s more stable ability to distinguish between positive and negative samples. The excellent performance in AUC-PR further validates MESM’s ability to maintain high prediction accuracy even under imbalanced data distributions. Additionally, the consistently leading Micro-F1 scores across different datasets and dataset partitioning algorithms further confirm MESM’s superiority in overall sample classification accuracy.
Table 2Ablation experimental resultsDatasetPartition schemew/o multimodalw/o SEw/o subgraphMESMSHS27kBFS84.34 ± 4.1187.39 ± 2.9485.75 ± 3.5788.65 ± 2.47DFS84.72 ± 6.5688.41 ± 1.2787.19 ± 1.1189.63 ± 0.93Random88.27 ± 6.6094.03 ± 0.1892.76 ± 0.4294.35 ± 0.30SHS148kBFS82.28 ± 6.5787.39 ± 1.2586.05 ± 1.5088.58 ± 1.86DFS85.57 ± 5.5189.59 ± 0.2087.36 ± 1.1490.39 ± 0.56Random88.12 ± 7.6793.89 ± 0.0892.06 ± 0.6194.11 ± 0.52SYS30kBFS85.70 ± 5.4590.39 ± 0.7588.44 ± 0.6690.95 ± 0.37DFS90.64 ± 3.9993.74 ± 1.1292.42 ± 1.7594.17 ± 1.31Random90.67 ± 4.4494.05 ± 0.3492.89 ± 0.0894.60 ± 0.15SYS60kBFS75.49 ± 7.3590.17 ± 1.1589.24 ± 1.3991.90 ± 1.10DFS82.98 ± 5.1992.06 ± 0.5191.42 ± 0.2893.71 ± 0.21Random85.27 ± 5.4893.96 ± 0.2893.31 ± 0.5095.44 ± 0.28Ablation analysisThis section systematically evaluates the contribution of each component of our method to overall performance through ablation experiments. Our model consists of three key components: (1) multimodal feature extraction based on protein sequences, structures, and point clouds; (2) encoding the local structures of the PPI network graph; and (3) learning local subgraph information. To assess the impact of each module on the model’s predictive performance, we designed three ablation experiments, and the results are shown in Table 2.First, in the w/o multimodal experiment, the model utilized the sequence feature extraction method from GNN-PPI, missing the multimodal features. This setup prevents the model from leveraging the three-dimensional structural information of proteins, directly affecting its ability to learn complex protein characteristics. In this experiment, the ablation operation led to average performance declines of 5.59%, 6.27%, 4.56%, and 13.32% on datasets SHS27k, SHS148k, SYS30k, and SYS60k, respectively. This result indicates that a diverse representation of features enables the model to understand the complex relationships between proteins across various contexts, highlighting the significance of incorporating multimodal features generated from protein three-dimensional structures to enhance the model’s learning capability.Next, in the w/o SE experiment, local structure encoding and GraphGPS were disabled, leaving the model with a global perspective lacking detailed local structural information. This modification prevented the model from fully capturing the subtle interactions between proteins in the PPI network, further validating the positive role of local structure encoding in the feature extraction process. Although the average performance decline of the model on the four datasets was relatively small, at 1.04%, 0.82%, 0.55%, and 1.73%, respectively, local structure encoding still provided a degree of performance improvement across all datasets and evaluation methods, demonstrating its importance in the overall model performance.Finally, in the w/o subgraph experiment, subgraph extraction and SubgraphGCN were disabled, utilizing GCN to capture the global features of the graph. This setup caused the model to lose focus on specific local subgraphs, preventing it from capturing crucial details that are vital for prediction. Therefore, the absence of local subgraph features also significantly impacted the model’s performance. In this experiment, the model’s average performance declined by 2.56%, 2.8%, 2.14%, and 2.52% on the four datasets, respectively. This further indicates that local subgraph features have a non-negligible impact on the model’s predictive ability.In the three ablation experiments (w/o multimodal, w/o SE, w/o subgraph), the model’s average performance declined by 7.44%, 1.04%, and 2.51%, respectively. It can be observed that multimodal protein features contributed the most, followed by the subgraph mechanism and the graph structure encoding mechanism. This indicates that in PPI prediction tasks, the richer the protein structural information carried by the protein node features in the PPI network graph, the greater the benefit the model derives from the original protein features. Under the subgraph mechanism, the model can focus more on the local neighborhood information of each node, learning more useful interaction information from it. Finally, although the graph structure encoding information has a lower priority compared to subgraph information, it helps the model capture longer dependencies and learn the structural information of each node in the graph, providing a deeper understanding of long-range interactions. Overall, the complementarity and synergy between these components significantly enhance the model’s performance in the task of predicting protein–protein interactions.Meanwhile, we applied the t-SNE (t-Distributed Stochastic Neighbor Embedding) method [42] to evaluate the efficacy of these three components in the MESM method. Before performing t-SNE mapping, we first standardized the data to ensure the quality and comparability of the input features. This preprocessing step helps eliminate discrepancies between different feature dimensions, making the dimensionality reduction results more reliable.Under different modes, we used t-SNE to reduce high-dimensional features to two-dimensional space for visualizing clustering results. As shown in Fig. 2, compared to the ablation mode lacking multimodal protein features, subfigures b, c, and d demonstrate that data points of different categories exhibit a more natural clustering trend, forming distinct cluster structures. Data points within the same cluster show relatively high density, indicating that if multimodal protein features are missing during feature extraction, the model’s performance in capturing features between similar protein pairs to enhance classification accuracy will be reduced to some extent. Moreover, in the absence of missing multimodal protein features, the boundaries between different categories in the clustering result graph are more distinct compared to other methods, indicating that multimodal protein features are more useful for the model to handle the diversity of different protein pairs. At the same time, compared to the absence of graph structure encoding information, the data point distribution in the result graph lacking subgraph information is more scattered. This result shows that multimodal protein features have the highest importance in the MESM method, directly affecting the model’s prediction performance, while the impact of subgraph information is secondary. Although the impact of graph structure encoding information is relatively minor, it is equally effective in maintaining the stability of the model’s prediction performance.Fig. 2a–d Respectively illustrate the distribution of prediction results for the seven PPI types under w/o multimodal, w/o SE, w/o subgraph, and MESM. It can be observed that in the absence of multimodal protein features, subgraph information, and graph structure encoding information, there are varying degrees of scattered data points and chaotic category distributions. This indicates that in MESM, multimodal protein features are the most critical, directly affecting core performance, while subgraph information also has a significant impact, and graph structure encoding information, though relatively secondary, still provides some optimization effectsAs shown in Table 3, we conducted comparative experiments between the ZLPR loss function and the ASL loss function. The experimental results show that, compared to using the ASL loss function, MESM achieved average performance improvements of 1.87%, 2.78%, 5.96%, and 2.23% on SHS27k, SHS148k, SYS30k, and SYS60k, respectively, when using the ZLPR loss function. This indicates that when the model learns from graph data of seven different interaction types, ZLPR can better balance positive and negative class labels, further mitigating the impact of imbalanced PPI type distribution.
Table 3Performance comparison of MESM using ASL and ZLPR loss functionsDatasetPartition schemeLoss functionASLZLPRSHS27kBFS86.54±3.1888.59±2.77DFS87.36±1.0789.32±1.13Random92.79±0.3594.38±0.25SHS148kBFS86.35±2.6788.63±1.80DFS87.00±0.8590.40±0.70Random91.28±1.3393.95±0.78SYS30kBFS88.55±0.7590.78±0.40DFS92.78±1.3994.14±1.17Random92.35±0.1694.72±0.14SYS60kBFS89.58±1.3091.93±1.28DFS91.65±0.4393.58±0.21Random92.99±0.2595.39±0.36Evaluation of predictive performance on unknown proteinsTo further evaluate the model’s predictive performance on unknown proteins, we adopted the three subset concepts [35] proposed by GNN-PPI: Known Subset (BS), Partially Known Subset (ES), and Unknown Subset (NS). Specifically, BS refers to protein pairs that have participated in the training set, ES refers to interaction pairs where at least one protein has been included in training, and NS indicates that neither of the two proteins has been involved in training.As shown in Fig. 3, we conducted an in-depth analysis of the generalization capabilities of GNN-PPI, MAPE-PPI, BaPPI, and MESM on the SHS27k test set. MESM achieved the best performance in almost all evaluation scenarios. Compared to the BaPPI algorithm, MESM improved the average performance by 11.17% across all subsets under three partitioning algorithms. This result demonstrates that MESM not only effectively handles interactions between known proteins but also more efficiently addresses interactions involving unknown proteins. Under the random partitioning scenario, the combined proportion of the BS and ES subsets is 99.69%, meaning that during testing, one or both proteins in a protein pair are often those encountered during training. The significant improvement of MESM on the ES and NS subsets indicates that MESM has a clearer understanding of interactions between unknown proteins and other proteins during testing. In the case of BFS and DFS partitioning, the proportions of the ES and NS subsets increase significantly compared to random partitioning. Consequently, different models often experience a certain degree of performance decline under these two partitioning strategies. However, MESM, leveraging the GraphGPS and SubgraphGCN modules, can better learn the patterns of interaction information between protein pairs from the PPI network graphs corresponding to the seven PPI types, providing more accurate predictions when faced with unknown proteins. Future research could focus on further enhancing the model’s ability to handle unknown proteins, exploring how to improve its adaptability to novel proteins by introducing additional functional features or adjusting the model architecture.Fig. 3Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on varying proportions of BS, ES, and NS subsets using three partitioning algorithms on the SHS27k datasetAnalysis of model generalizationTo better assess the generalization capability of the models, we applied those trained on the smaller datasets SHS27k and SYS30k to test on larger datasets SHS148k and SYS60k. This approach implies that when testing on larger datasets, the proteins encountered by the model are mostly unseen, placing higher demands on the model’s learning and reasoning capabilities.The experimental results are shown in Fig. 4, indicating that when models are applied to larger datasets, the predictive performance of most methods declines to varying degrees. This phenomenon reflects that with changing data distribution, most models show inadequate adaptability to new situations, negatively affecting their efficacy in predicting unknown protein–protein interactions. However, MESM performed outstandingly in all four evaluation modes. Under BFS and DFS for SHS148k, although MESM ranked second, it still demonstrated better performance compared to other methods. This result indicates that MESM exhibits a stronger adaptability when handling unknown proteins and can maintain relatively stable predictive performance across different data distributions. This advantage can be attributed to the multimodal feature extraction, local structure encoding, and local subgraph mechanisms in the MESM approach. These designs not only help the model fully absorb the knowledge acquired from the training set but also enhance its ability to capture features when facing different types of proteins.Fig. 4Evaluation of the generalization performance of GNN-PPI, MAPE-PPI, BaPPI, and MESM on the unknown test set under three partitioning algorithms. a The model trained on SHS27k is used to test SHS148k. b The model trained on SYS30k is used to test SYS60kAnalysis of predictive performance of each model under different interaction typesThe ratio of seven PPI types under different partitions is shown in Table 4. It can be observed that under random partitioning, the proportions of the seven PPI types in the test set are similar to those in the SHS27k dataset. However, under BFS and DFS partitions, the occurrence frequencies of each PPI type varied to different extents. This variation may reflect the diversity of sample types and the differences in interaction features under specific partitioning strategies, thereby affecting the model’s learning and prediction capabilities.
Table 4Ratio(%) of 7 PPI types in the test set under three partition schemes in SHS27kTypesBFSDFSRandomReaction17.4219.3218.38Binding22.3926.5623.42Ptmod6.917.416.97Activation20.0212.6018.59Inhibition7.377.207.69Catalysis18.8825.2220.88Expression7.011.694.07The performance of each method on different PPI types is shown in Fig. 5. Notably, MESM achieved significant performance improvements over other methods on the three PPI types with the lowest proportions (Ptmod, Inhibition, and Expression). For instance, under BFS partition, MESM’s performance improved by 23.32%, 29.72%, and 18.93% for the Ptmod, Inhibition, and Expression types, respectively, compared to BaPPI. These results not only highlight MESM’s advantages in handling scarce type data but also indicate that the model efficiently captures the complex features of these interactions.Fig. 5Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on 7 PPI types using three partitioning algorithms on the SHS27k datasetAs shown in Table 4 and Fig. 6, compared to the two PPI types, Expression and Ptmod, which account for about 7%, the four PPI types—Activation, Binding, Catalysis, and Reaction—with proportions exceeding 15% have a greater impact on MESM’s prediction performance. The higher the proportion of a PPI type, the more the features of protein nodes in its corresponding PPI network graph are updated by the model, considering protein and interaction information specific to that PPI type. However, the PPI type Inhibition, which also accounts for about 7%, has an impact on MESM’s prediction performance that is close to that of the four PPI types with higher proportions. This indicates that the PPI network graphs corresponding to PPI types with lower proportions also contain meaningful information for improving prediction performance. Therefore, MESM can better learn this interaction information from the seven PPI network graphs and integrate them, thereby enhancing the model’s prediction performance.
Fig. 6The performance of MESM on the SHS27k dataset using the BFS partitioning algorithm under Modes A ~ H. The first seven modes (A ~ G) represent the model’s predictive performance when trained without data from the Reaction, Binding, Ptmod, Activation, Inhibition, Catalysis, and Expression interaction types, respectively. For example, Mode A indicates that the model is trained on the SHS27k dataset without using the independent PPI network graph derived from the Reaction interaction type, but instead using only the remaining six independent graphs. Mode H, on the other hand, represents the model being trained using all seven independent PPI network graphs
As shown in Fig. 6, the predictive performance is best in H mode, while the performance decline compared to H mode for the first seven modes is as follows: D (3.92%), B (2.42%), E (2.33%), F (2.09%), A (2.05%), G (1.86%), and C (1.85%). These results indicate that protein features under different interaction types contribute to performance improvement to varying degrees, in the order of Activation, Binding, Inhibition, Catalysis, Reaction, Expression, and Ptmod. The results show that the absence of interaction information corresponding to any PPI type leads to varying degrees of performance decline. Since protein pairs in the dataset have at least one and up to seven PPI types, the composition of edge sets in the PPI network graphs corresponding to different PPI types varies (e.g., a pair of proteins may have an edge in the Activation PPI network graph but not in the Expression PPI network graph). Therefore, when using graph neural networks, the model develops different understandings of the PPI network graphs corresponding to different PPI types, enabling it to focus more on learning the interaction information in the current PPI network graph. By extracting corresponding proteins from the network graphs of different PPI types and combining all seven protein features, MESM can make more confident predictions when determining the interaction probability of protein pairs with multiple interaction types.Impact of constructing networks for seven PPI types on the predictive performance of MESMAs shown in Fig. 7, we compared the normal version of MESM (NMESM), which does not construct networks for the seven PPI types, with other methods using BFS and DFS partitioning algorithms on the four datasets. The results show that under BFS and DFS partition, NMESM still outperformed other methods that only used the overall PPI network graph. This result further validates the importance of constructing specific PPI type network graphs under different partitioning algorithms for improving model performance, and it also demonstrates the significant impact of the three key components of MESM on its performance.Fig. 7Comparison of the performance of the normal version of MESM (NMESM), GNN-PPI, and MAPE-PPI using BFS and DFS partition on the four datasets

Benchmark comparisonTo validate the effectiveness of our proposed method, we compared it with various baseline methods, including GNN-PPI, AFTGAN, DL-P   PI, MAPE-PPI, and BaPPI. These baseline methods have their unique features in the PPI prediction task, with their specific implementations and performances described as follows.GNN-PPI [35] utilizes one-dimensional convolution (Conv1d) and bidirectional gated recurrent units (BiGRU) to extract protein-related feature representations, and then employs Graph Isomorphism Networks (GIN) [36] to aggregate these features, generating protein graph embeddings for prediction. This approach effectively captures the complex relationships between proteins through deep learning techniques, demonstrating strong performance.AFTGAN [37] combines an Attention-Free Transformer (AFT) to extract protein sequence features and utilizes GAT to learn relationship features between protein pairs. This method provides advantages in extracting long sequence features and modeling protein interaction relationships.DL-PPI [38] captures protein sequence features and then processes them using the Inception method, ultimately employing GIN and Feature Relational Reasoning Network (FRN) to learn and predict potential relationship information between proteins. This hierarchical design effectively enhances the model’s adaptability to diverse data.MAPE-PPI [39] developed a microenvironment-aware pre-training method for protein embedding learning, combining vector quantized variational autoencoders (VQ-VAE) [40] and heterogeneous graph neural networks, successfully producing high-quality protein embedding features based on the sequence and structural information of amino acid residues in proteins, significantly reducing training time.BaPPI [41] extracts protein sequence features using operations such as Hyena Operator, Conv1d, and BiGRU, and employs GraphSAGE and antisymmetric graph convolutional networks to learn and predict the relationships between protein pairs. Additionally, seven PPI network graphs were constructed for seven types of PPI, which extract interaction information for different PPI types, enhancing the prediction performance of the model.We applied three different data partitioning algorithms on four datasets (SHS27k, SHS148k, SYS30k, and SYS60k): Breadth-First Search (BFS), Depth-First Search (DFS), and random partition. The corresponding experimental results are shown in Table 1. We display the best results in bold and the second-best results italics in all tables. The results indicate that the predictive performance of nearly all methods shows a gradual improvement according to BFS, DFS, and random partition. This phenomenon suggests that the DFS partition can select sparsely distributed proteins in the PPI network compared to the BFS partition, while the random partition selects proteins that are more dispersed in the PPI network. During training, when the model faces a sparser distribution of proteins, it usually achieves better results in the testing phase. Therefore, the BFS and DFS partition pose greater challenges to the model’s generalization ability.
Table 1Performance comparison of MESM and baselines across four datasetsDatasetPartitionschemeMethodsGNN-PPIAFTGANDL-PPIMAPE-PPIBaPPIMESMSHS27kBFS72.19 ± 4.4568.96 ± 4.0871.04 ± 4.072.00 ± 3.5477.94 ± 3.5288.65 ± 2.47DFS78.17 ± 1.7774.83 ± 1.7977.89 ± 2.0577.23 ± 1.6579.66 ± 1.7289.63 ± 0.93Random88.54 ± 0.2982.73 ± 0.5488.78 ± 0.389.09 ± 0.3788.73 ± 0.6394.35 ± 0.30SHS148kBFS76.41 ± 3.075.64 ± 2.9976.03 ± 2.178.75 ± 3.4983.25 ± 1.5788.58 ± 1.86DFS83.14 ± 1.2280.17 ± 0.8281.97 ± 1.682.76 ± 1.4181.98 ± 1.0990.39 ± 0.56Random92.37 ± 0.1486.47 ± 0.5692.38 ± 0.193.19 ± 0.1492.90 ± 0.1694.11 ± 0.52SYS30kBFS76.84 ± 0.5575.89 ± 1.2577.10 ± 0.6476.52 ± 0.5282.05 ± 1.0390.95 ± 0.37DFS86.27 ± 3.0384.33 ± 3.1486.23 ± 2.7785.88 ± 3.487.42 ± 2.0894.17 ± 1.31Random92.03 ± 0.2881.99 ± 0.6391.56 ± 0.4291.35 ± 0.487.80 ± 0.6394.60 ± 0.15SYS60kBFS81.54 ± 2.0779.38 ± 1.080.67 ± 2.2382.90 ± 2.2685.06 ± 1.291.90 ± 1.10DFS85.19 ± 0.7481.66 ± 0.7284.31 ± 0.5787.25 ± 0.2886.97 ± 0.1993.71 ± 0.21Random95.72 ± 0.1384.44 ± 0.5195.48 ± 0.1395.43 ± 0.2390.78 ± 0.1895.44 ± 0.28Furthermore, under BFS and DFS partition, MESM outperforms other methods, particularly when switching from random partition to BFS or DFS, showing a noticeably smaller degree of performance drop. When transitioning from smaller datasets SHS27k and SYS30k to larger datasets SHS148k and SYS60k, baseline methods generally exhibit superior performance. Notably, the predictive performance of MESM on smaller datasets approaches that of larger datasets, indicating that MESM can effectively utilize the PPI network and its neighboring nodes to extract meaningful information even with limited PPI data, thereby demonstrating stronger feature extraction capabilities when predicting unknown protein–protein interactions. Overall, compared to other methods, MESM typically shows lower standard deviation, which effectively validates its superiority in terms of generalization ability and robustness.As shown in Additional file 1: Table 1 and Additional file 1: Table 2, MESM demonstrated significant advantages across multiple performance metrics on the SHS27k, SHS148k, SYS30k, and SYS60k datasets, including Accuracy, Recall, Precision, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), Area Under the Precision-Recall Curve (AUC-PR), and Micro-F1. In terms of Accuracy, MESM achieved higher overall prediction accuracy by optimizing feature representation capabilities. The significant improvement in Recall indicates that MESM has a stronger ability to capture positive samples. The simultaneous optimization of Precision further demonstrates MESM’s effectiveness in reducing misjudgments. The notable improvement in AUC-ROC reflects MESM’s more stable ability to distinguish between positive and negative samples. The excellent performance in AUC-PR further validates MESM’s ability to maintain high prediction accuracy even under imbalanced data distributions. Additionally, the consistently leading Micro-F1 scores across different datasets and dataset partitioning algorithms further confirm MESM’s superiority in overall sample classification accuracy.
Table 2Ablation experimental resultsDatasetPartition schemew/o multimodalw/o SEw/o subgraphMESMSHS27kBFS84.34 ± 4.1187.39 ± 2.9485.75 ± 3.5788.65 ± 2.47DFS84.72 ± 6.5688.41 ± 1.2787.19 ± 1.1189.63 ± 0.93Random88.27 ± 6.6094.03 ± 0.1892.76 ± 0.4294.35 ± 0.30SHS148kBFS82.28 ± 6.5787.39 ± 1.2586.05 ± 1.5088.58 ± 1.86DFS85.57 ± 5.5189.59 ± 0.2087.36 ± 1.1490.39 ± 0.56Random88.12 ± 7.6793.89 ± 0.0892.06 ± 0.6194.11 ± 0.52SYS30kBFS85.70 ± 5.4590.39 ± 0.7588.44 ± 0.6690.95 ± 0.37DFS90.64 ± 3.9993.74 ± 1.1292.42 ± 1.7594.17 ± 1.31Random90.67 ± 4.4494.05 ± 0.3492.89 ± 0.0894.60 ± 0.15SYS60kBFS75.49 ± 7.3590.17 ± 1.1589.24 ± 1.3991.90 ± 1.10DFS82.98 ± 5.1992.06 ± 0.5191.42 ± 0.2893.71 ± 0.21Random85.27 ± 5.4893.96 ± 0.2893.31 ± 0.5095.44 ± 0.28

Benchmark comparison

To validate the effectiveness of our proposed method, we compared it with various baseline methods, including GNN-PPI, AFTGAN, DL-P   PI, MAPE-PPI, and BaPPI. These baseline methods have their unique features in the PPI prediction task, with their specific implementations and performances described as follows.

GNN-PPI [35] utilizes one-dimensional convolution (Conv1d) and bidirectional gated recurrent units (BiGRU) to extract protein-related feature representations, and then employs Graph Isomorphism Networks (GIN) [36] to aggregate these features, generating protein graph embeddings for prediction. This approach effectively captures the complex relationships between proteins through deep learning techniques, demonstrating strong performance.

AFTGAN [37] combines an Attention-Free Transformer (AFT) to extract protein sequence features and utilizes GAT to learn relationship features between protein pairs. This method provides advantages in extracting long sequence features and modeling protein interaction relationships.

DL-PPI [38] captures protein sequence features and then processes them using the Inception method, ultimately employing GIN and Feature Relational Reasoning Network (FRN) to learn and predict potential relationship information between proteins. This hierarchical design effectively enhances the model’s adaptability to diverse data.

MAPE-PPI [39] developed a microenvironment-aware pre-training method for protein embedding learning, combining vector quantized variational autoencoders (VQ-VAE) [40] and heterogeneous graph neural networks, successfully producing high-quality protein embedding features based on the sequence and structural information of amino acid residues in proteins, significantly reducing training time.

BaPPI [41] extracts protein sequence features using operations such as Hyena Operator, Conv1d, and BiGRU, and employs GraphSAGE and antisymmetric graph convolutional networks to learn and predict the relationships between protein pairs. Additionally, seven PPI network graphs were constructed for seven types of PPI, which extract interaction information for different PPI types, enhancing the prediction performance of the model.

We applied three different data partitioning algorithms on four datasets (SHS27k, SHS148k, SYS30k, and SYS60k): Breadth-First Search (BFS), Depth-First Search (DFS), and random partition. The corresponding experimental results are shown in Table 1. We display the best results in bold and the second-best results italics in all tables. The results indicate that the predictive performance of nearly all methods shows a gradual improvement according to BFS, DFS, and random partition. This phenomenon suggests that the DFS partition can select sparsely distributed proteins in the PPI network compared to the BFS partition, while the random partition selects proteins that are more dispersed in the PPI network. During training, when the model faces a sparser distribution of proteins, it usually achieves better results in the testing phase. Therefore, the BFS and DFS partition pose greater challenges to the model’s generalization ability.

Table 1Performance comparison of MESM and baselines across four datasetsDatasetPartitionschemeMethodsGNN-PPIAFTGANDL-PPIMAPE-PPIBaPPIMESMSHS27kBFS72.19 ± 4.4568.96 ± 4.0871.04 ± 4.072.00 ± 3.5477.94 ± 3.5288.65 ± 2.47DFS78.17 ± 1.7774.83 ± 1.7977.89 ± 2.0577.23 ± 1.6579.66 ± 1.7289.63 ± 0.93Random88.54 ± 0.2982.73 ± 0.5488.78 ± 0.389.09 ± 0.3788.73 ± 0.6394.35 ± 0.30SHS148kBFS76.41 ± 3.075.64 ± 2.9976.03 ± 2.178.75 ± 3.4983.25 ± 1.5788.58 ± 1.86DFS83.14 ± 1.2280.17 ± 0.8281.97 ± 1.682.76 ± 1.4181.98 ± 1.0990.39 ± 0.56Random92.37 ± 0.1486.47 ± 0.5692.38 ± 0.193.19 ± 0.1492.90 ± 0.1694.11 ± 0.52SYS30kBFS76.84 ± 0.5575.89 ± 1.2577.10 ± 0.6476.52 ± 0.5282.05 ± 1.0390.95 ± 0.37DFS86.27 ± 3.0384.33 ± 3.1486.23 ± 2.7785.88 ± 3.487.42 ± 2.0894.17 ± 1.31Random92.03 ± 0.2881.99 ± 0.6391.56 ± 0.4291.35 ± 0.487.80 ± 0.6394.60 ± 0.15SYS60kBFS81.54 ± 2.0779.38 ± 1.080.67 ± 2.2382.90 ± 2.2685.06 ± 1.291.90 ± 1.10DFS85.19 ± 0.7481.66 ± 0.7284.31 ± 0.5787.25 ± 0.2886.97 ± 0.1993.71 ± 0.21Random95.72 ± 0.1384.44 ± 0.5195.48 ± 0.1395.43 ± 0.2390.78 ± 0.1895.44 ± 0.28

Performance comparison of MESM and baselines across four datasets

Furthermore, under BFS and DFS partition, MESM outperforms other methods, particularly when switching from random partition to BFS or DFS, showing a noticeably smaller degree of performance drop. When transitioning from smaller datasets SHS27k and SYS30k to larger datasets SHS148k and SYS60k, baseline methods generally exhibit superior performance. Notably, the predictive performance of MESM on smaller datasets approaches that of larger datasets, indicating that MESM can effectively utilize the PPI network and its neighboring nodes to extract meaningful information even with limited PPI data, thereby demonstrating stronger feature extraction capabilities when predicting unknown protein–protein interactions. Overall, compared to other methods, MESM typically shows lower standard deviation, which effectively validates its superiority in terms of generalization ability and robustness.

As shown in Additional file 1: Table 1 and Additional file 1: Table 2, MESM demonstrated significant advantages across multiple performance metrics on the SHS27k, SHS148k, SYS30k, and SYS60k datasets, including Accuracy, Recall, Precision, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), Area Under the Precision-Recall Curve (AUC-PR), and Micro-F1. In terms of Accuracy, MESM achieved higher overall prediction accuracy by optimizing feature representation capabilities. The significant improvement in Recall indicates that MESM has a stronger ability to capture positive samples. The simultaneous optimization of Precision further demonstrates MESM’s effectiveness in reducing misjudgments. The notable improvement in AUC-ROC reflects MESM’s more stable ability to distinguish between positive and negative samples. The excellent performance in AUC-PR further validates MESM’s ability to maintain high prediction accuracy even under imbalanced data distributions. Additionally, the consistently leading Micro-F1 scores across different datasets and dataset partitioning algorithms further confirm MESM’s superiority in overall sample classification accuracy.
Table 2Ablation experimental resultsDatasetPartition schemew/o multimodalw/o SEw/o subgraphMESMSHS27kBFS84.34 ± 4.1187.39 ± 2.9485.75 ± 3.5788.65 ± 2.47DFS84.72 ± 6.5688.41 ± 1.2787.19 ± 1.1189.63 ± 0.93Random88.27 ± 6.6094.03 ± 0.1892.76 ± 0.4294.35 ± 0.30SHS148kBFS82.28 ± 6.5787.39 ± 1.2586.05 ± 1.5088.58 ± 1.86DFS85.57 ± 5.5189.59 ± 0.2087.36 ± 1.1490.39 ± 0.56Random88.12 ± 7.6793.89 ± 0.0892.06 ± 0.6194.11 ± 0.52SYS30kBFS85.70 ± 5.4590.39 ± 0.7588.44 ± 0.6690.95 ± 0.37DFS90.64 ± 3.9993.74 ± 1.1292.42 ± 1.7594.17 ± 1.31Random90.67 ± 4.4494.05 ± 0.3492.89 ± 0.0894.60 ± 0.15SYS60kBFS75.49 ± 7.3590.17 ± 1.1589.24 ± 1.3991.90 ± 1.10DFS82.98 ± 5.1992.06 ± 0.5191.42 ± 0.2893.71 ± 0.21Random85.27 ± 5.4893.96 ± 0.2893.31 ± 0.5095.44 ± 0.28

Ablation experimental results

Ablation analysisThis section systematically evaluates the contribution of each component of our method to overall performance through ablation experiments. Our model consists of three key components: (1) multimodal feature extraction based on protein sequences, structures, and point clouds; (2) encoding the local structures of the PPI network graph; and (3) learning local subgraph information. To assess the impact of each module on the model’s predictive performance, we designed three ablation experiments, and the results are shown in Table 2.First, in the w/o multimodal experiment, the model utilized the sequence feature extraction method from GNN-PPI, missing the multimodal features. This setup prevents the model from leveraging the three-dimensional structural information of proteins, directly affecting its ability to learn complex protein characteristics. In this experiment, the ablation operation led to average performance declines of 5.59%, 6.27%, 4.56%, and 13.32% on datasets SHS27k, SHS148k, SYS30k, and SYS60k, respectively. This result indicates that a diverse representation of features enables the model to understand the complex relationships between proteins across various contexts, highlighting the significance of incorporating multimodal features generated from protein three-dimensional structures to enhance the model’s learning capability.Next, in the w/o SE experiment, local structure encoding and GraphGPS were disabled, leaving the model with a global perspective lacking detailed local structural information. This modification prevented the model from fully capturing the subtle interactions between proteins in the PPI network, further validating the positive role of local structure encoding in the feature extraction process. Although the average performance decline of the model on the four datasets was relatively small, at 1.04%, 0.82%, 0.55%, and 1.73%, respectively, local structure encoding still provided a degree of performance improvement across all datasets and evaluation methods, demonstrating its importance in the overall model performance.Finally, in the w/o subgraph experiment, subgraph extraction and SubgraphGCN were disabled, utilizing GCN to capture the global features of the graph. This setup caused the model to lose focus on specific local subgraphs, preventing it from capturing crucial details that are vital for prediction. Therefore, the absence of local subgraph features also significantly impacted the model’s performance. In this experiment, the model’s average performance declined by 2.56%, 2.8%, 2.14%, and 2.52% on the four datasets, respectively. This further indicates that local subgraph features have a non-negligible impact on the model’s predictive ability.In the three ablation experiments (w/o multimodal, w/o SE, w/o subgraph), the model’s average performance declined by 7.44%, 1.04%, and 2.51%, respectively. It can be observed that multimodal protein features contributed the most, followed by the subgraph mechanism and the graph structure encoding mechanism. This indicates that in PPI prediction tasks, the richer the protein structural information carried by the protein node features in the PPI network graph, the greater the benefit the model derives from the original protein features. Under the subgraph mechanism, the model can focus more on the local neighborhood information of each node, learning more useful interaction information from it. Finally, although the graph structure encoding information has a lower priority compared to subgraph information, it helps the model capture longer dependencies and learn the structural information of each node in the graph, providing a deeper understanding of long-range interactions. Overall, the complementarity and synergy between these components significantly enhance the model’s performance in the task of predicting protein–protein interactions.Meanwhile, we applied the t-SNE (t-Distributed Stochastic Neighbor Embedding) method [42] to evaluate the efficacy of these three components in the MESM method. Before performing t-SNE mapping, we first standardized the data to ensure the quality and comparability of the input features. This preprocessing step helps eliminate discrepancies between different feature dimensions, making the dimensionality reduction results more reliable.Under different modes, we used t-SNE to reduce high-dimensional features to two-dimensional space for visualizing clustering results. As shown in Fig. 2, compared to the ablation mode lacking multimodal protein features, subfigures b, c, and d demonstrate that data points of different categories exhibit a more natural clustering trend, forming distinct cluster structures. Data points within the same cluster show relatively high density, indicating that if multimodal protein features are missing during feature extraction, the model’s performance in capturing features between similar protein pairs to enhance classification accuracy will be reduced to some extent. Moreover, in the absence of missing multimodal protein features, the boundaries between different categories in the clustering result graph are more distinct compared to other methods, indicating that multimodal protein features are more useful for the model to handle the diversity of different protein pairs. At the same time, compared to the absence of graph structure encoding information, the data point distribution in the result graph lacking subgraph information is more scattered. This result shows that multimodal protein features have the highest importance in the MESM method, directly affecting the model’s prediction performance, while the impact of subgraph information is secondary. Although the impact of graph structure encoding information is relatively minor, it is equally effective in maintaining the stability of the model’s prediction performance.Fig. 2a–d Respectively illustrate the distribution of prediction results for the seven PPI types under w/o multimodal, w/o SE, w/o subgraph, and MESM. It can be observed that in the absence of multimodal protein features, subgraph information, and graph structure encoding information, there are varying degrees of scattered data points and chaotic category distributions. This indicates that in MESM, multimodal protein features are the most critical, directly affecting core performance, while subgraph information also has a significant impact, and graph structure encoding information, though relatively secondary, still provides some optimization effectsAs shown in Table 3, we conducted comparative experiments between the ZLPR loss function and the ASL loss function. The experimental results show that, compared to using the ASL loss function, MESM achieved average performance improvements of 1.87%, 2.78%, 5.96%, and 2.23% on SHS27k, SHS148k, SYS30k, and SYS60k, respectively, when using the ZLPR loss function. This indicates that when the model learns from graph data of seven different interaction types, ZLPR can better balance positive and negative class labels, further mitigating the impact of imbalanced PPI type distribution.
Table 3Performance comparison of MESM using ASL and ZLPR loss functionsDatasetPartition schemeLoss functionASLZLPRSHS27kBFS86.54±3.1888.59±2.77DFS87.36±1.0789.32±1.13Random92.79±0.3594.38±0.25SHS148kBFS86.35±2.6788.63±1.80DFS87.00±0.8590.40±0.70Random91.28±1.3393.95±0.78SYS30kBFS88.55±0.7590.78±0.40DFS92.78±1.3994.14±1.17Random92.35±0.1694.72±0.14SYS60kBFS89.58±1.3091.93±1.28DFS91.65±0.4393.58±0.21Random92.99±0.2595.39±0.36

Ablation analysis

This section systematically evaluates the contribution of each component of our method to overall performance through ablation experiments. Our model consists of three key components: (1) multimodal feature extraction based on protein sequences, structures, and point clouds; (2) encoding the local structures of the PPI network graph; and (3) learning local subgraph information. To assess the impact of each module on the model’s predictive performance, we designed three ablation experiments, and the results are shown in Table 2.

First, in the w/o multimodal experiment, the model utilized the sequence feature extraction method from GNN-PPI, missing the multimodal features. This setup prevents the model from leveraging the three-dimensional structural information of proteins, directly affecting its ability to learn complex protein characteristics. In this experiment, the ablation operation led to average performance declines of 5.59%, 6.27%, 4.56%, and 13.32% on datasets SHS27k, SHS148k, SYS30k, and SYS60k, respectively. This result indicates that a diverse representation of features enables the model to understand the complex relationships between proteins across various contexts, highlighting the significance of incorporating multimodal features generated from protein three-dimensional structures to enhance the model’s learning capability.

Next, in the w/o SE experiment, local structure encoding and GraphGPS were disabled, leaving the model with a global perspective lacking detailed local structural information. This modification prevented the model from fully capturing the subtle interactions between proteins in the PPI network, further validating the positive role of local structure encoding in the feature extraction process. Although the average performance decline of the model on the four datasets was relatively small, at 1.04%, 0.82%, 0.55%, and 1.73%, respectively, local structure encoding still provided a degree of performance improvement across all datasets and evaluation methods, demonstrating its importance in the overall model performance.

Finally, in the w/o subgraph experiment, subgraph extraction and SubgraphGCN were disabled, utilizing GCN to capture the global features of the graph. This setup caused the model to lose focus on specific local subgraphs, preventing it from capturing crucial details that are vital for prediction. Therefore, the absence of local subgraph features also significantly impacted the model’s performance. In this experiment, the model’s average performance declined by 2.56%, 2.8%, 2.14%, and 2.52% on the four datasets, respectively. This further indicates that local subgraph features have a non-negligible impact on the model’s predictive ability.

In the three ablation experiments (w/o multimodal, w/o SE, w/o subgraph), the model’s average performance declined by 7.44%, 1.04%, and 2.51%, respectively. It can be observed that multimodal protein features contributed the most, followed by the subgraph mechanism and the graph structure encoding mechanism. This indicates that in PPI prediction tasks, the richer the protein structural information carried by the protein node features in the PPI network graph, the greater the benefit the model derives from the original protein features. Under the subgraph mechanism, the model can focus more on the local neighborhood information of each node, learning more useful interaction information from it. Finally, although the graph structure encoding information has a lower priority compared to subgraph information, it helps the model capture longer dependencies and learn the structural information of each node in the graph, providing a deeper understanding of long-range interactions. Overall, the complementarity and synergy between these components significantly enhance the model’s performance in the task of predicting protein–protein interactions.

Meanwhile, we applied the t-SNE (t-Distributed Stochastic Neighbor Embedding) method [42] to evaluate the efficacy of these three components in the MESM method. Before performing t-SNE mapping, we first standardized the data to ensure the quality and comparability of the input features. This preprocessing step helps eliminate discrepancies between different feature dimensions, making the dimensionality reduction results more reliable.

Under different modes, we used t-SNE to reduce high-dimensional features to two-dimensional space for visualizing clustering results. As shown in Fig. 2, compared to the ablation mode lacking multimodal protein features, subfigures b, c, and d demonstrate that data points of different categories exhibit a more natural clustering trend, forming distinct cluster structures. Data points within the same cluster show relatively high density, indicating that if multimodal protein features are missing during feature extraction, the model’s performance in capturing features between similar protein pairs to enhance classification accuracy will be reduced to some extent. Moreover, in the absence of missing multimodal protein features, the boundaries between different categories in the clustering result graph are more distinct compared to other methods, indicating that multimodal protein features are more useful for the model to handle the diversity of different protein pairs. At the same time, compared to the absence of graph structure encoding information, the data point distribution in the result graph lacking subgraph information is more scattered. This result shows that multimodal protein features have the highest importance in the MESM method, directly affecting the model’s prediction performance, while the impact of subgraph information is secondary. Although the impact of graph structure encoding information is relatively minor, it is equally effective in maintaining the stability of the model’s prediction performance.Fig. 2a–d Respectively illustrate the distribution of prediction results for the seven PPI types under w/o multimodal, w/o SE, w/o subgraph, and MESM. It can be observed that in the absence of multimodal protein features, subgraph information, and graph structure encoding information, there are varying degrees of scattered data points and chaotic category distributions. This indicates that in MESM, multimodal protein features are the most critical, directly affecting core performance, while subgraph information also has a significant impact, and graph structure encoding information, though relatively secondary, still provides some optimization effects

a–d Respectively illustrate the distribution of prediction results for the seven PPI types under w/o multimodal, w/o SE, w/o subgraph, and MESM. It can be observed that in the absence of multimodal protein features, subgraph information, and graph structure encoding information, there are varying degrees of scattered data points and chaotic category distributions. This indicates that in MESM, multimodal protein features are the most critical, directly affecting core performance, while subgraph information also has a significant impact, and graph structure encoding information, though relatively secondary, still provides some optimization effects

As shown in Table 3, we conducted comparative experiments between the ZLPR loss function and the ASL loss function. The experimental results show that, compared to using the ASL loss function, MESM achieved average performance improvements of 1.87%, 2.78%, 5.96%, and 2.23% on SHS27k, SHS148k, SYS30k, and SYS60k, respectively, when using the ZLPR loss function. This indicates that when the model learns from graph data of seven different interaction types, ZLPR can better balance positive and negative class labels, further mitigating the impact of imbalanced PPI type distribution.
Table 3Performance comparison of MESM using ASL and ZLPR loss functionsDatasetPartition schemeLoss functionASLZLPRSHS27kBFS86.54±3.1888.59±2.77DFS87.36±1.0789.32±1.13Random92.79±0.3594.38±0.25SHS148kBFS86.35±2.6788.63±1.80DFS87.00±0.8590.40±0.70Random91.28±1.3393.95±0.78SYS30kBFS88.55±0.7590.78±0.40DFS92.78±1.3994.14±1.17Random92.35±0.1694.72±0.14SYS60kBFS89.58±1.3091.93±1.28DFS91.65±0.4393.58±0.21Random92.99±0.2595.39±0.36

Performance comparison of MESM using ASL and ZLPR loss functions

Evaluation of predictive performance on unknown proteinsTo further evaluate the model’s predictive performance on unknown proteins, we adopted the three subset concepts [35] proposed by GNN-PPI: Known Subset (BS), Partially Known Subset (ES), and Unknown Subset (NS). Specifically, BS refers to protein pairs that have participated in the training set, ES refers to interaction pairs where at least one protein has been included in training, and NS indicates that neither of the two proteins has been involved in training.As shown in Fig. 3, we conducted an in-depth analysis of the generalization capabilities of GNN-PPI, MAPE-PPI, BaPPI, and MESM on the SHS27k test set. MESM achieved the best performance in almost all evaluation scenarios. Compared to the BaPPI algorithm, MESM improved the average performance by 11.17% across all subsets under three partitioning algorithms. This result demonstrates that MESM not only effectively handles interactions between known proteins but also more efficiently addresses interactions involving unknown proteins. Under the random partitioning scenario, the combined proportion of the BS and ES subsets is 99.69%, meaning that during testing, one or both proteins in a protein pair are often those encountered during training. The significant improvement of MESM on the ES and NS subsets indicates that MESM has a clearer understanding of interactions between unknown proteins and other proteins during testing. In the case of BFS and DFS partitioning, the proportions of the ES and NS subsets increase significantly compared to random partitioning. Consequently, different models often experience a certain degree of performance decline under these two partitioning strategies. However, MESM, leveraging the GraphGPS and SubgraphGCN modules, can better learn the patterns of interaction information between protein pairs from the PPI network graphs corresponding to the seven PPI types, providing more accurate predictions when faced with unknown proteins. Future research could focus on further enhancing the model’s ability to handle unknown proteins, exploring how to improve its adaptability to novel proteins by introducing additional functional features or adjusting the model architecture.Fig. 3Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on varying proportions of BS, ES, and NS subsets using three partitioning algorithms on the SHS27k dataset

Evaluation of predictive performance on unknown proteins

To further evaluate the model’s predictive performance on unknown proteins, we adopted the three subset concepts [35] proposed by GNN-PPI: Known Subset (BS), Partially Known Subset (ES), and Unknown Subset (NS). Specifically, BS refers to protein pairs that have participated in the training set, ES refers to interaction pairs where at least one protein has been included in training, and NS indicates that neither of the two proteins has been involved in training.

As shown in Fig. 3, we conducted an in-depth analysis of the generalization capabilities of GNN-PPI, MAPE-PPI, BaPPI, and MESM on the SHS27k test set. MESM achieved the best performance in almost all evaluation scenarios. Compared to the BaPPI algorithm, MESM improved the average performance by 11.17% across all subsets under three partitioning algorithms. This result demonstrates that MESM not only effectively handles interactions between known proteins but also more efficiently addresses interactions involving unknown proteins. Under the random partitioning scenario, the combined proportion of the BS and ES subsets is 99.69%, meaning that during testing, one or both proteins in a protein pair are often those encountered during training. The significant improvement of MESM on the ES and NS subsets indicates that MESM has a clearer understanding of interactions between unknown proteins and other proteins during testing. In the case of BFS and DFS partitioning, the proportions of the ES and NS subsets increase significantly compared to random partitioning. Consequently, different models often experience a certain degree of performance decline under these two partitioning strategies. However, MESM, leveraging the GraphGPS and SubgraphGCN modules, can better learn the patterns of interaction information between protein pairs from the PPI network graphs corresponding to the seven PPI types, providing more accurate predictions when faced with unknown proteins. Future research could focus on further enhancing the model’s ability to handle unknown proteins, exploring how to improve its adaptability to novel proteins by introducing additional functional features or adjusting the model architecture.Fig. 3Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on varying proportions of BS, ES, and NS subsets using three partitioning algorithms on the SHS27k dataset

Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on varying proportions of BS, ES, and NS subsets using three partitioning algorithms on the SHS27k dataset

Analysis of model generalizationTo better assess the generalization capability of the models, we applied those trained on the smaller datasets SHS27k and SYS30k to test on larger datasets SHS148k and SYS60k. This approach implies that when testing on larger datasets, the proteins encountered by the model are mostly unseen, placing higher demands on the model’s learning and reasoning capabilities.The experimental results are shown in Fig. 4, indicating that when models are applied to larger datasets, the predictive performance of most methods declines to varying degrees. This phenomenon reflects that with changing data distribution, most models show inadequate adaptability to new situations, negatively affecting their efficacy in predicting unknown protein–protein interactions. However, MESM performed outstandingly in all four evaluation modes. Under BFS and DFS for SHS148k, although MESM ranked second, it still demonstrated better performance compared to other methods. This result indicates that MESM exhibits a stronger adaptability when handling unknown proteins and can maintain relatively stable predictive performance across different data distributions. This advantage can be attributed to the multimodal feature extraction, local structure encoding, and local subgraph mechanisms in the MESM approach. These designs not only help the model fully absorb the knowledge acquired from the training set but also enhance its ability to capture features when facing different types of proteins.Fig. 4Evaluation of the generalization performance of GNN-PPI, MAPE-PPI, BaPPI, and MESM on the unknown test set under three partitioning algorithms. a The model trained on SHS27k is used to test SHS148k. b The model trained on SYS30k is used to test SYS60k

Analysis of model generalization

To better assess the generalization capability of the models, we applied those trained on the smaller datasets SHS27k and SYS30k to test on larger datasets SHS148k and SYS60k. This approach implies that when testing on larger datasets, the proteins encountered by the model are mostly unseen, placing higher demands on the model’s learning and reasoning capabilities.

The experimental results are shown in Fig. 4, indicating that when models are applied to larger datasets, the predictive performance of most methods declines to varying degrees. This phenomenon reflects that with changing data distribution, most models show inadequate adaptability to new situations, negatively affecting their efficacy in predicting unknown protein–protein interactions. However, MESM performed outstandingly in all four evaluation modes. Under BFS and DFS for SHS148k, although MESM ranked second, it still demonstrated better performance compared to other methods. This result indicates that MESM exhibits a stronger adaptability when handling unknown proteins and can maintain relatively stable predictive performance across different data distributions. This advantage can be attributed to the multimodal feature extraction, local structure encoding, and local subgraph mechanisms in the MESM approach. These designs not only help the model fully absorb the knowledge acquired from the training set but also enhance its ability to capture features when facing different types of proteins.Fig. 4Evaluation of the generalization performance of GNN-PPI, MAPE-PPI, BaPPI, and MESM on the unknown test set under three partitioning algorithms. a The model trained on SHS27k is used to test SHS148k. b The model trained on SYS30k is used to test SYS60k

Evaluation of the generalization performance of GNN-PPI, MAPE-PPI, BaPPI, and MESM on the unknown test set under three partitioning algorithms. a The model trained on SHS27k is used to test SHS148k. b The model trained on SYS30k is used to test SYS60k

Analysis of predictive performance of each model under different interaction typesThe ratio of seven PPI types under different partitions is shown in Table 4. It can be observed that under random partitioning, the proportions of the seven PPI types in the test set are similar to those in the SHS27k dataset. However, under BFS and DFS partitions, the occurrence frequencies of each PPI type varied to different extents. This variation may reflect the diversity of sample types and the differences in interaction features under specific partitioning strategies, thereby affecting the model’s learning and prediction capabilities.
Table 4Ratio(%) of 7 PPI types in the test set under three partition schemes in SHS27kTypesBFSDFSRandomReaction17.4219.3218.38Binding22.3926.5623.42Ptmod6.917.416.97Activation20.0212.6018.59Inhibition7.377.207.69Catalysis18.8825.2220.88Expression7.011.694.07The performance of each method on different PPI types is shown in Fig. 5. Notably, MESM achieved significant performance improvements over other methods on the three PPI types with the lowest proportions (Ptmod, Inhibition, and Expression). For instance, under BFS partition, MESM’s performance improved by 23.32%, 29.72%, and 18.93% for the Ptmod, Inhibition, and Expression types, respectively, compared to BaPPI. These results not only highlight MESM’s advantages in handling scarce type data but also indicate that the model efficiently captures the complex features of these interactions.Fig. 5Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on 7 PPI types using three partitioning algorithms on the SHS27k datasetAs shown in Table 4 and Fig. 6, compared to the two PPI types, Expression and Ptmod, which account for about 7%, the four PPI types—Activation, Binding, Catalysis, and Reaction—with proportions exceeding 15% have a greater impact on MESM’s prediction performance. The higher the proportion of a PPI type, the more the features of protein nodes in its corresponding PPI network graph are updated by the model, considering protein and interaction information specific to that PPI type. However, the PPI type Inhibition, which also accounts for about 7%, has an impact on MESM’s prediction performance that is close to that of the four PPI types with higher proportions. This indicates that the PPI network graphs corresponding to PPI types with lower proportions also contain meaningful information for improving prediction performance. Therefore, MESM can better learn this interaction information from the seven PPI network graphs and integrate them, thereby enhancing the model’s prediction performance.
Fig. 6The performance of MESM on the SHS27k dataset using the BFS partitioning algorithm under Modes A ~ H. The first seven modes (A ~ G) represent the model’s predictive performance when trained without data from the Reaction, Binding, Ptmod, Activation, Inhibition, Catalysis, and Expression interaction types, respectively. For example, Mode A indicates that the model is trained on the SHS27k dataset without using the independent PPI network graph derived from the Reaction interaction type, but instead using only the remaining six independent graphs. Mode H, on the other hand, represents the model being trained using all seven independent PPI network graphs
As shown in Fig. 6, the predictive performance is best in H mode, while the performance decline compared to H mode for the first seven modes is as follows: D (3.92%), B (2.42%), E (2.33%), F (2.09%), A (2.05%), G (1.86%), and C (1.85%). These results indicate that protein features under different interaction types contribute to performance improvement to varying degrees, in the order of Activation, Binding, Inhibition, Catalysis, Reaction, Expression, and Ptmod. The results show that the absence of interaction information corresponding to any PPI type leads to varying degrees of performance decline. Since protein pairs in the dataset have at least one and up to seven PPI types, the composition of edge sets in the PPI network graphs corresponding to different PPI types varies (e.g., a pair of proteins may have an edge in the Activation PPI network graph but not in the Expression PPI network graph). Therefore, when using graph neural networks, the model develops different understandings of the PPI network graphs corresponding to different PPI types, enabling it to focus more on learning the interaction information in the current PPI network graph. By extracting corresponding proteins from the network graphs of different PPI types and combining all seven protein features, MESM can make more confident predictions when determining the interaction probability of protein pairs with multiple interaction types.

Analysis of predictive performance of each model under different interaction types

The ratio of seven PPI types under different partitions is shown in Table 4. It can be observed that under random partitioning, the proportions of the seven PPI types in the test set are similar to those in the SHS27k dataset. However, under BFS and DFS partitions, the occurrence frequencies of each PPI type varied to different extents. This variation may reflect the diversity of sample types and the differences in interaction features under specific partitioning strategies, thereby affecting the model’s learning and prediction capabilities.

Table 4Ratio(%) of 7 PPI types in the test set under three partition schemes in SHS27kTypesBFSDFSRandomReaction17.4219.3218.38Binding22.3926.5623.42Ptmod6.917.416.97Activation20.0212.6018.59Inhibition7.377.207.69Catalysis18.8825.2220.88Expression7.011.694.07

Ratio(%) of 7 PPI types in the test set under three partition schemes in SHS27k

The performance of each method on different PPI types is shown in Fig. 5. Notably, MESM achieved significant performance improvements over other methods on the three PPI types with the lowest proportions (Ptmod, Inhibition, and Expression). For instance, under BFS partition, MESM’s performance improved by 23.32%, 29.72%, and 18.93% for the Ptmod, Inhibition, and Expression types, respectively, compared to BaPPI. These results not only highlight MESM’s advantages in handling scarce type data but also indicate that the model efficiently captures the complex features of these interactions.Fig. 5Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on 7 PPI types using three partitioning algorithms on the SHS27k dataset

Performance comparison of GNN-PPI, MAPE-PPI, BaPPI, and MESM on 7 PPI types using three partitioning algorithms on the SHS27k dataset

As shown in Table 4 and Fig. 6, compared to the two PPI types, Expression and Ptmod, which account for about 7%, the four PPI types—Activation, Binding, Catalysis, and Reaction—with proportions exceeding 15% have a greater impact on MESM’s prediction performance. The higher the proportion of a PPI type, the more the features of protein nodes in its corresponding PPI network graph are updated by the model, considering protein and interaction information specific to that PPI type. However, the PPI type Inhibition, which also accounts for about 7%, has an impact on MESM’s prediction performance that is close to that of the four PPI types with higher proportions. This indicates that the PPI network graphs corresponding to PPI types with lower proportions also contain meaningful information for improving prediction performance. Therefore, MESM can better learn this interaction information from the seven PPI network graphs and integrate them, thereby enhancing the model’s prediction performance.

Fig. 6The performance of MESM on the SHS27k dataset using the BFS partitioning algorithm under Modes A ~ H. The first seven modes (A ~ G) represent the model’s predictive performance when trained without data from the Reaction, Binding, Ptmod, Activation, Inhibition, Catalysis, and Expression interaction types, respectively. For example, Mode A indicates that the model is trained on the SHS27k dataset without using the independent PPI network graph derived from the Reaction interaction type, but instead using only the remaining six independent graphs. Mode H, on the other hand, represents the model being trained using all seven independent PPI network graphs

The performance of MESM on the SHS27k dataset using the BFS partitioning algorithm under Modes A ~ H. The first seven modes (A ~ G) represent the model’s predictive performance when trained without data from the Reaction, Binding, Ptmod, Activation, Inhibition, Catalysis, and Expression interaction types, respectively. For example, Mode A indicates that the model is trained on the SHS27k dataset without using the independent PPI network graph derived from the Reaction interaction type, but instead using only the remaining six independent graphs. Mode H, on the other hand, represents the model being trained using all seven independent PPI network graphs

As shown in Fig. 6, the predictive performance is best in H mode, while the performance decline compared to H mode for the first seven modes is as follows: D (3.92%), B (2.42%), E (2.33%), F (2.09%), A (2.05%), G (1.86%), and C (1.85%). These results indicate that protein features under different interaction types contribute to performance improvement to varying degrees, in the order of Activation, Binding, Inhibition, Catalysis, Reaction, Expression, and Ptmod. The results show that the absence of interaction information corresponding to any PPI type leads to varying degrees of performance decline. Since protein pairs in the dataset have at least one and up to seven PPI types, the composition of edge sets in the PPI network graphs corresponding to different PPI types varies (e.g., a pair of proteins may have an edge in the Activation PPI network graph but not in the Expression PPI network graph). Therefore, when using graph neural networks, the model develops different understandings of the PPI network graphs corresponding to different PPI types, enabling it to focus more on learning the interaction information in the current PPI network graph. By extracting corresponding proteins from the network graphs of different PPI types and combining all seven protein features, MESM can make more confident predictions when determining the interaction probability of protein pairs with multiple interaction types.

Impact of constructing networks for seven PPI types on the predictive performance of MESMAs shown in Fig. 7, we compared the normal version of MESM (NMESM), which does not construct networks for the seven PPI types, with other methods using BFS and DFS partitioning algorithms on the four datasets. The results show that under BFS and DFS partition, NMESM still outperformed other methods that only used the overall PPI network graph. This result further validates the importance of constructing specific PPI type network graphs under different partitioning algorithms for improving model performance, and it also demonstrates the significant impact of the three key components of MESM on its performance.Fig. 7Comparison of the performance of the normal version of MESM (NMESM), GNN-PPI, and MAPE-PPI using BFS and DFS partition on the four datasets

Impact of constructing networks for seven PPI types on the predictive performance of MESM

As shown in Fig. 7, we compared the normal version of MESM (NMESM), which does not construct networks for the seven PPI types, with other methods using BFS and DFS partitioning algorithms on the four datasets. The results show that under BFS and DFS partition, NMESM still outperformed other methods that only used the overall PPI network graph. This result further validates the importance of constructing specific PPI type network graphs under different partitioning algorithms for improving model performance, and it also demonstrates the significant impact of the three key components of MESM on its performance.Fig. 7Comparison of the performance of the normal version of MESM (NMESM), GNN-PPI, and MAPE-PPI using BFS and DFS partition on the four datasets

Comparison of the performance of the normal version of MESM (NMESM), GNN-PPI, and MAPE-PPI using BFS and DFS partition on the four datasets

DiscussionProtein-protein interaction prediction: a multidimensional study based on AlphaFold3 and PAE matrix analysisIn this study, we used the relevant information from the STRING database and employed AlphaFold3 [43] to predict the interactions of two human proteins (9606.ENSP00000229794 and 9606.ENSP00000239223), analyzing the nature of their potential interactions through the Predicted Aligned Error (PAE) matrix. The PAE matrix provides crucial information about the uncertainty in the relative positions of amino acid residues in the structural model. MESM prediction results indicate that there are seven different types of interactions between these two proteins, all of which were identified.As shown in Fig. 8, we observed variations in brightness in different areas of the matrix, reflecting the spatial relationships between aligned residues and their prediction uncertainties. The PAE matrix clearly shows that most areas have relatively low expected position errors, especially near the diagonal, indicating that the interaction predictions at these residue positions are quite reliable. In certain areas, the expected position errors approach zero, demonstrating strong predictive confidence. The high confidence in these regions suggests that there may be strong interactions between the amino acid residues, which could play a crucial role in cellular functions. In contrast, there are areas with higher expected errors in the upper right and lower left of the matrix, where interactions between amino acid residues may exhibit significant uncertainty. Further experimental validation is needed to confirm the relevant hypotheses.Fig. 8Prediction results of protein-protein interactions for human proteins 9606.ENSP00000229794 and 9606.ENSP00000239223 using AlphaFold 3To further validate our findings, we used the STRING database [44] to assess the interaction scores of these two proteins in activation, binding, catalysis, expression, inhibition, post-translational modification (Ptmod), and reaction. The results show that these two proteins scored high in activation, binding, inhibition, and post-translational modification, with scores of 0.912, 0.913, 0.893, and 0.893, respectively, highlighting their potential importance in these biological functions. However, the scores in catalysis, expression, and reaction are significantly lower, at 0.337, 0.167, and 0.165, respectively, indicating that the predictions of these interactions are not strong enough, which may suggest that this type of interaction is not a focus in biological processes. Therefore, combining the analysis of the PAE matrix with the interaction scores provided by the STRING database, we conclude that while certain regions have high predictive confidence, it is reasonable to find significant uncertainties in some types of interactions. This finding emphasizes the importance of performing multidimensional analyses in protein interaction predictions, especially when dealing with complex biological systems.In addition, current PPI prediction tasks typically predict the seven types of interactions using a binary classification approach, where the model’s outcome is either 0 or 1; this method fails to fully reflect the strength and complexity of interactions between proteins, neglecting the potential intermediate states or weak interactions. Therefore, further analysis of low-scoring interaction types will be an important direction for future research. This may involve more in-depth biological experiments to validate and understand the biological significance of these interactions, exploring their potential applications in targeted drug design and disease mechanism research.In-depth research directions for multi-label PPI predictionAlthough MESM performs excellently in multi-label PPI prediction tasks, there are still several considerations worth noting, and we believe that future research can delve deeper into the following aspects:Considering that PPI are influenced by the complexity of various biological processes and their functional associations, while MESM currently models based only on the primary and tertiary structures of proteins integrating protein knowledge graphs [45, 46] may help improve the accuracy of PPI type prediction.Although MESM generates multimodal features by integrating protein sequence, structure, and point cloud data, this fusion approach is relatively simple. It may be considered to adopt attention mechanisms adapted to different modalities to further enhance the effectiveness of cross-modal fusion [47].When extracting protein structural information, we used the K-nearest neighbor (KNN) strategy to construct the edges of the protein-level graph, but did not deeply explore higher-level structural information of proteins. Therefore, incorporating new biological information such as pairwise residue distances and dihedral angles [32] may further capture fine-grained structural features of proteins.Although MESM demonstrates good performance on the four datasets used, the subgraph mechanism incurs significant computational costs, making it unsuitable for larger datasets. Therefore, further optimization of the subgraph mechanism regarding subgraph extraction and usage is a critical step in reducing the computational complexity of the method. For example, leveraging relevant information from the STRING database to guide subgraph extraction and usage can effectively reduce noncritical subgraphs, thereby lowering computational complexity and accelerating model training speed.The STRING database contains rich biologically relevant protein network data for both Homo sapiens and Saccharomyces cerevisiae, including combined scores between proteins and relevant enrichment information. However, MESM does not use this information as edge attributes in the PPI network graph to modulate the strength of information propagation between protein nodes. Meanwhile, existing methods, including MESM, predict protein–protein interaction types using complete protein sequence information, while the STRING database also provides the start and end sequence positions of interacting protein pairs. This may lead the model to learn irrelevant knowledge during training, reducing prediction performance. If this information can be further utilized during training, the model may achieve better PPI prediction performance.

Protein-protein interaction prediction: a multidimensional study based on AlphaFold3 and PAE matrix analysisIn this study, we used the relevant information from the STRING database and employed AlphaFold3 [43] to predict the interactions of two human proteins (9606.ENSP00000229794 and 9606.ENSP00000239223), analyzing the nature of their potential interactions through the Predicted Aligned Error (PAE) matrix. The PAE matrix provides crucial information about the uncertainty in the relative positions of amino acid residues in the structural model. MESM prediction results indicate that there are seven different types of interactions between these two proteins, all of which were identified.As shown in Fig. 8, we observed variations in brightness in different areas of the matrix, reflecting the spatial relationships between aligned residues and their prediction uncertainties. The PAE matrix clearly shows that most areas have relatively low expected position errors, especially near the diagonal, indicating that the interaction predictions at these residue positions are quite reliable. In certain areas, the expected position errors approach zero, demonstrating strong predictive confidence. The high confidence in these regions suggests that there may be strong interactions between the amino acid residues, which could play a crucial role in cellular functions. In contrast, there are areas with higher expected errors in the upper right and lower left of the matrix, where interactions between amino acid residues may exhibit significant uncertainty. Further experimental validation is needed to confirm the relevant hypotheses.Fig. 8Prediction results of protein-protein interactions for human proteins 9606.ENSP00000229794 and 9606.ENSP00000239223 using AlphaFold 3To further validate our findings, we used the STRING database [44] to assess the interaction scores of these two proteins in activation, binding, catalysis, expression, inhibition, post-translational modification (Ptmod), and reaction. The results show that these two proteins scored high in activation, binding, inhibition, and post-translational modification, with scores of 0.912, 0.913, 0.893, and 0.893, respectively, highlighting their potential importance in these biological functions. However, the scores in catalysis, expression, and reaction are significantly lower, at 0.337, 0.167, and 0.165, respectively, indicating that the predictions of these interactions are not strong enough, which may suggest that this type of interaction is not a focus in biological processes. Therefore, combining the analysis of the PAE matrix with the interaction scores provided by the STRING database, we conclude that while certain regions have high predictive confidence, it is reasonable to find significant uncertainties in some types of interactions. This finding emphasizes the importance of performing multidimensional analyses in protein interaction predictions, especially when dealing with complex biological systems.In addition, current PPI prediction tasks typically predict the seven types of interactions using a binary classification approach, where the model’s outcome is either 0 or 1; this method fails to fully reflect the strength and complexity of interactions between proteins, neglecting the potential intermediate states or weak interactions. Therefore, further analysis of low-scoring interaction types will be an important direction for future research. This may involve more in-depth biological experiments to validate and understand the biological significance of these interactions, exploring their potential applications in targeted drug design and disease mechanism research.

Protein-protein interaction prediction: a multidimensional study based on AlphaFold3 and PAE matrix analysis

In this study, we used the relevant information from the STRING database and employed AlphaFold3 [43] to predict the interactions of two human proteins (9606.ENSP00000229794 and 9606.ENSP00000239223), analyzing the nature of their potential interactions through the Predicted Aligned Error (PAE) matrix. The PAE matrix provides crucial information about the uncertainty in the relative positions of amino acid residues in the structural model. MESM prediction results indicate that there are seven different types of interactions between these two proteins, all of which were identified.

As shown in Fig. 8, we observed variations in brightness in different areas of the matrix, reflecting the spatial relationships between aligned residues and their prediction uncertainties. The PAE matrix clearly shows that most areas have relatively low expected position errors, especially near the diagonal, indicating that the interaction predictions at these residue positions are quite reliable. In certain areas, the expected position errors approach zero, demonstrating strong predictive confidence. The high confidence in these regions suggests that there may be strong interactions between the amino acid residues, which could play a crucial role in cellular functions. In contrast, there are areas with higher expected errors in the upper right and lower left of the matrix, where interactions between amino acid residues may exhibit significant uncertainty. Further experimental validation is needed to confirm the relevant hypotheses.Fig. 8Prediction results of protein-protein interactions for human proteins 9606.ENSP00000229794 and 9606.ENSP00000239223 using AlphaFold 3

Prediction results of protein-protein interactions for human proteins 9606.ENSP00000229794 and 9606.ENSP00000239223 using AlphaFold 3

To further validate our findings, we used the STRING database [44] to assess the interaction scores of these two proteins in activation, binding, catalysis, expression, inhibition, post-translational modification (Ptmod), and reaction. The results show that these two proteins scored high in activation, binding, inhibition, and post-translational modification, with scores of 0.912, 0.913, 0.893, and 0.893, respectively, highlighting their potential importance in these biological functions. However, the scores in catalysis, expression, and reaction are significantly lower, at 0.337, 0.167, and 0.165, respectively, indicating that the predictions of these interactions are not strong enough, which may suggest that this type of interaction is not a focus in biological processes. Therefore, combining the analysis of the PAE matrix with the interaction scores provided by the STRING database, we conclude that while certain regions have high predictive confidence, it is reasonable to find significant uncertainties in some types of interactions. This finding emphasizes the importance of performing multidimensional analyses in protein interaction predictions, especially when dealing with complex biological systems.

In addition, current PPI prediction tasks typically predict the seven types of interactions using a binary classification approach, where the model’s outcome is either 0 or 1; this method fails to fully reflect the strength and complexity of interactions between proteins, neglecting the potential intermediate states or weak interactions. Therefore, further analysis of low-scoring interaction types will be an important direction for future research. This may involve more in-depth biological experiments to validate and understand the biological significance of these interactions, exploring their potential applications in targeted drug design and disease mechanism research.

In-depth research directions for multi-label PPI predictionAlthough MESM performs excellently in multi-label PPI prediction tasks, there are still several considerations worth noting, and we believe that future research can delve deeper into the following aspects:Considering that PPI are influenced by the complexity of various biological processes and their functional associations, while MESM currently models based only on the primary and tertiary structures of proteins integrating protein knowledge graphs [45, 46] may help improve the accuracy of PPI type prediction.Although MESM generates multimodal features by integrating protein sequence, structure, and point cloud data, this fusion approach is relatively simple. It may be considered to adopt attention mechanisms adapted to different modalities to further enhance the effectiveness of cross-modal fusion [47].When extracting protein structural information, we used the K-nearest neighbor (KNN) strategy to construct the edges of the protein-level graph, but did not deeply explore higher-level structural information of proteins. Therefore, incorporating new biological information such as pairwise residue distances and dihedral angles [32] may further capture fine-grained structural features of proteins.Although MESM demonstrates good performance on the four datasets used, the subgraph mechanism incurs significant computational costs, making it unsuitable for larger datasets. Therefore, further optimization of the subgraph mechanism regarding subgraph extraction and usage is a critical step in reducing the computational complexity of the method. For example, leveraging relevant information from the STRING database to guide subgraph extraction and usage can effectively reduce noncritical subgraphs, thereby lowering computational complexity and accelerating model training speed.The STRING database contains rich biologically relevant protein network data for both Homo sapiens and Saccharomyces cerevisiae, including combined scores between proteins and relevant enrichment information. However, MESM does not use this information as edge attributes in the PPI network graph to modulate the strength of information propagation between protein nodes. Meanwhile, existing methods, including MESM, predict protein–protein interaction types using complete protein sequence information, while the STRING database also provides the start and end sequence positions of interacting protein pairs. This may lead the model to learn irrelevant knowledge during training, reducing prediction performance. If this information can be further utilized during training, the model may achieve better PPI prediction performance.

In-depth research directions for multi-label PPI prediction

Although MESM performs excellently in multi-label PPI prediction tasks, there are still several considerations worth noting, and we believe that future research can delve deeper into the following aspects:Considering that PPI are influenced by the complexity of various biological processes and their functional associations, while MESM currently models based only on the primary and tertiary structures of proteins integrating protein knowledge graphs [45, 46] may help improve the accuracy of PPI type prediction.Although MESM generates multimodal features by integrating protein sequence, structure, and point cloud data, this fusion approach is relatively simple. It may be considered to adopt attention mechanisms adapted to different modalities to further enhance the effectiveness of cross-modal fusion [47].When extracting protein structural information, we used the K-nearest neighbor (KNN) strategy to construct the edges of the protein-level graph, but did not deeply explore higher-level structural information of proteins. Therefore, incorporating new biological information such as pairwise residue distances and dihedral angles [32] may further capture fine-grained structural features of proteins.Although MESM demonstrates good performance on the four datasets used, the subgraph mechanism incurs significant computational costs, making it unsuitable for larger datasets. Therefore, further optimization of the subgraph mechanism regarding subgraph extraction and usage is a critical step in reducing the computational complexity of the method. For example, leveraging relevant information from the STRING database to guide subgraph extraction and usage can effectively reduce noncritical subgraphs, thereby lowering computational complexity and accelerating model training speed.The STRING database contains rich biologically relevant protein network data for both Homo sapiens and Saccharomyces cerevisiae, including combined scores between proteins and relevant enrichment information. However, MESM does not use this information as edge attributes in the PPI network graph to modulate the strength of information propagation between protein nodes. Meanwhile, existing methods, including MESM, predict protein–protein interaction types using complete protein sequence information, while the STRING database also provides the start and end sequence positions of interacting protein pairs. This may lead the model to learn irrelevant knowledge during training, reducing prediction performance. If this information can be further utilized during training, the model may achieve better PPI prediction performance.

Considering that PPI are influenced by the complexity of various biological processes and their functional associations, while MESM currently models based only on the primary and tertiary structures of proteins integrating protein knowledge graphs [45, 46] may help improve the accuracy of PPI type prediction.

Although MESM generates multimodal features by integrating protein sequence, structure, and point cloud data, this fusion approach is relatively simple. It may be considered to adopt attention mechanisms adapted to different modalities to further enhance the effectiveness of cross-modal fusion [47].

When extracting protein structural information, we used the K-nearest neighbor (KNN) strategy to construct the edges of the protein-level graph, but did not deeply explore higher-level structural information of proteins. Therefore, incorporating new biological information such as pairwise residue distances and dihedral angles [32] may further capture fine-grained structural features of proteins.

Although MESM demonstrates good performance on the four datasets used, the subgraph mechanism incurs significant computational costs, making it unsuitable for larger datasets. Therefore, further optimization of the subgraph mechanism regarding subgraph extraction and usage is a critical step in reducing the computational complexity of the method. For example, leveraging relevant information from the STRING database to guide subgraph extraction and usage can effectively reduce noncritical subgraphs, thereby lowering computational complexity and accelerating model training speed.

The STRING database contains rich biologically relevant protein network data for both Homo sapiens and Saccharomyces cerevisiae, including combined scores between proteins and relevant enrichment information. However, MESM does not use this information as edge attributes in the PPI network graph to modulate the strength of information propagation between protein nodes. Meanwhile, existing methods, including MESM, predict protein–protein interaction types using complete protein sequence information, while the STRING database also provides the start and end sequence positions of interacting protein pairs. This may lead the model to learn irrelevant knowledge during training, reducing prediction performance. If this information can be further utilized during training, the model may achieve better PPI prediction performance.

ConclusionsOverall, this paper introduces MESM, a novel protein–protein interaction (PPI) prediction model based on multimodal protein data and graph neural networks. The model effectively extracts protein features from various sources using a pre-training approach, including sequence information, structural information, and spatial information. Based on the PPI network graph, MESM utilizes two core layers: GraphGPS and SubgraphGCN. GraphGPS is responsible for initially extracting features related to the graph structure, while SubgraphGCN further enhances the ability to extract local subgraph features by aggregating information from adjacent nodes, thereby refining protein features with greater interactive significance. Experimental results show that MESM outperforms other methods in generalization capability for unknown proteins and scarce types, enabling more accurate identification of potential interactions and providing a deeper understanding of interaction information in the PPI network graph. Therefore, MESM can be regarded as an effective and reliable tool for multi-label protein–protein interaction prediction tasks.

Conclusions

Overall, this paper introduces MESM, a novel protein–protein interaction (PPI) prediction model based on multimodal protein data and graph neural networks. The model effectively extracts protein features from various sources using a pre-training approach, including sequence information, structural information, and spatial information. Based on the PPI network graph, MESM utilizes two core layers: GraphGPS and SubgraphGCN. GraphGPS is responsible for initially extracting features related to the graph structure, while SubgraphGCN further enhances the ability to extract local subgraph features by aggregating information from adjacent nodes, thereby refining protein features with greater interactive significance. Experimental results show that MESM outperforms other methods in generalization capability for unknown proteins and scarce types, enabling more accurate identification of potential interactions and providing a deeper understanding of interaction information in the PPI network graph. Therefore, MESM can be regarded as an effective and reliable tool for multi-label protein–protein interaction prediction tasks.

MethodsDatasetIn this study, we utilized multi-label protein–protein interaction data from SHS27k, SHS148k [48], SYS30k, and SYS60k [41] datasets to evaluate the performance of MESM in comparison with other PPI prediction methods. The SHS27k and SHS148k datasets were constructed by randomly selecting proteins with sequence similarity below 40% from the human PPI subset data of the STRING database [44], resulting in 1690 and 5189 proteins, respectively. The two datasets contain 7624 and 44,488 multi-label PPIs, respectively. To investigate MESM’s performance in predicting PPIs across different species, we selected SYS30k and SYS60k datasets. These two datasets were constructed based on the data construction strategy of SHS27k and SHS148k, randomly selecting 2685 and 3549 yeast proteins from the yeast PPI subset data of the STRING database, with 10,806 and 22,707 multi-label PPIs, respectively. The protein data used for training, including sequence information and PDB structure, are obtained from Uniprot [49] and AlphaFold [50]. However, some proteins lack structural data in these protein databases, necessitating the exclusion of approximately 2.8% of proteins to ensure data integrity [51].As shown in Table 5, there are a total of seven types of PPIs in these four PPI datasets: Reaction, Binding, Post-translational modification (Ptmod), Activation, Inhibition, Catalysis, and Expression. It can be seen that the distribution of various PPI types across the datasets typically exhibits imbalance, with significant differences in sample sizes, where certain types have far more data samples than others. For example, in the SHS27k dataset, the sample sizes for Reaction, Binding, Activation, and Catalysis greatly exceed those of other types, each accounting for 18% or more, while the total proportion of Ptmod, Inhibition, and Expression is only 19.08%. This imbalanced distribution may result in trained models performing well in classification for types with larger sample sizes, but the classification performance significantly declines for certain PPI types with fewer samples (such as Expression and Ptmod).
Table 5Number and ratio of seven PPI types in four datasetsPPI typesSHS27kSHS148kSYS30kSYS60kNumberRatioNumberRatioNumberRatioNumberRatioReaction312018.45%1778817.83%590921.28%1166120.84%Binding393023.24%2294322.99%893032.17%1869933.42%Ptmod12337.29%90599.08%18726.74%36526.53%Activation320018.92%1844618.49%322411.61%643511.5%Inhibition13718.11%87638.78%10053.62%20823.72%Catalysis343620.32%1956019.6%544819.62%1071819.15%Expression6223.68%32233.23%13754.95%27094.84%Unsupervised pre-training model for extracting multimodal protein featuresExisting PPI prediction methods typically adopt an end-to-end learning strategy [35], where the model is trained directly from protein feature extraction to PPI prediction. Although this approach allows the model to learn directly from the data, end-to-end learning requires simultaneous completion of feature extraction and task optimization, which is often challenging to achieve effectively, especially when data is insufficient. Furthermore, in protein–protein prediction tasks, graph neural network-based models generally require the complete PPI network graph as input, with larger datasets resulting in greater computational burdens [39]. Therefore, we adopt a stepwise approach for extracting protein features using a pre-training method. As shown in Fig. 1a, we adjust the model framework based on the pre-training model MPRL [52] for the protein–protein interaction prediction task to capture more expressive multimodal protein features.MPRL utilizes the primary and tertiary structures of proteins for unsupervised multimodal protein representation learning. For protein \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}\in P$$\end{document}Pi∈P, its primary structure is represented as its amino acid sequence \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S$$\end{document}S, written as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\left({v}_{1},{v}_{2},\cdots ,{v}_{M}\right)$$\end{document}S=v1,v2,⋯,vM, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${v}_{M}$$\end{document}vM denotes the m-th amino acid residue in the sequence, and M is the total number of amino acid residues in the sequence. Each \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${v}_{m}$$\end{document}vm belongs to the amino acid set V which contains 20 amino acid types. The tertiary structure of protein \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi is the structure formed in three-dimensional space by the folding of the amino acid chain \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{i}$$\end{document}Si through non-covalent interactions (such as hydrogen bonds, hydrophobic interactions, etc.), describing the atomic positions and folding states of the protein in three-dimensional space, represented as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${T}_{i}=\left({x}_{1},{y}_{1},{z}_{1};{x}_{2},{x}_{2},{z}_{2};\cdots ;{x}_{M},{y}_{M},{z}_{M}\right)$$\end{document}Ti=x1,y1,z1;x2,x2,z2;⋯;xM,yM,zM, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({x}_{m},{y}_{m},{z}_{m}\right)$$\end{document}xm,ym,zm denotes the spatial coordinates of the m-th amino acid residue. Protein sequence features provide fundamental insights into protein information but are limited in capturing complex spatial and dynamic characteristics, failing to represent spatial conformations. In contrast, constructing a topological network through local connections between residues characterizes the three-dimensional folding state of proteins but struggles to capture long-range spatial correlations. Meanwhile, three-dimensional point cloud spatial features precisely describe the global geometric morphology of proteins through spatial coordinates of residues, enhancing the expression of spatial information but lacking fine-grained representation of residue features. These three types of data provide complementary information at the levels of molecular sequence, structure, and global spatial distribution, addressing the limitations of single data sources in terms of information completeness. Therefore, compared to relying solely on a single protein feature (e.g., sequence features), multimodal features can more effectively enhance protein representation learning and model performance in PPI prediction tasks through complementary advantages among various features.Sequence feature extractionWe designed a model called Sequence Variational Autoencoder (SVAE) based on Variational Autoencoder (VAE) [53] to extract sequence features. SVAE extracts short-range patterns through 1D convolution and combines bidirectional GRU to capture long-range dependencies across sequences, adapting to the complex structure and functional properties of proteins. Meanwhile, global average pooling compresses variable-length sequences, making them suitable for proteins of different lengths. Leveraging VAE, SVAE maps high-dimensional protein features into a continuous, smooth, and structured latent space, enabling the generation of biologically plausible sequence features while preserving key functional characteristics.As shown in Fig. 9a, SVAE mainly consists of two parts:Fig. 9a The architecture of SVAE. SVAE is used to extract sequence features. It consists of two parts: an encoder and a decoder. The model is jointly optimized through reconstruction loss and KL divergence for unsupervised learning and data generation. b The architecture of VGAE. VGAE utilizes GCN to encode local interactions and spatial relationships in protein structure graphs. It predicts the existence probability of edges through an inner product decoder and optimizes the model by minimizing reconstruction loss for unsupervised learning. c The architecture of PAE. PAE extracts global features of the 3D point clouds of proteins using PointNet. It maps these features to a low-dimensional latent space and calculates the similarity between two point clouds using Chamfer distance for unsupervised learning and 3D point cloud reconstructionEncoder: The encoder extracts sequence data into fixed-dimensional features \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times s\times d}$$\end{document}X∈Rn×s×d through convolutional layers, bidirectional gated recurrent units (BiGRU), and fully connected (FC) layers, where n, s and d represent the number of proteins, sequence length, and amino acid residue feature dimensions, respectively. It outputs mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{log}(\sigma )$$\end{document}log(σ) of the latent space to generate the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, where b and d represent the batch size and sequence feature dimension, respectively.Decoder: Starting from the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z$$\end{document}Z, the decoder generates reconstructed data of the input sequence features through fully connected layers and the nonlinear activation function ReLU.The loss function of SVAE can be defined as Eq. 1:1\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=L_{recon}+L_{KL}$$\end{document}L=Lrecon+LKLwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon represents the reconstruction loss, which measures the error between the model′s reconstructed data and the original data; while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL represents the KL divergence, which constrains the distribution of the latent variable Z to bring q(Z|X) close to the standard normal distribution p(Z) = N(0,1). The reconstruction loss \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon can be expressed as Eq. 2:2\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{recon}=\mathrm{MSE}\;(\mathrm x,\hat{\mathrm x})=\frac1N\sum\limits_{i=1}^N\left(x_{i\;\;}-\;\hat{x}_i\right)^2$$\end{document}Lrecon=MSE(x,x^)=1N∑i=1Nxi-x^i2where x denotes the input original data, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{x}$$\end{document}x^ represents the output reconstructed by the model, and N is the number of samples. The KL divergence \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL can be expressed as Eq. 3:3\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{KL}}=-\frac{1}{2}\left(1+\text{log}\left({\sigma }^{2}\right)-{\mu }^{2}-{\sigma }^{2}\right)\end{array}$$\end{document}LKL=-121+logσ2-μ2-σ2Structure feature extractionTo construct the protein structure graph, we use one-hot encoding to specify a specific amino acid type for each residue and employ the K-nearest neighbors (KNN) strategy to build edges based on the coordinates of the alpha carbon atoms of the amino acid residues within the protein.In this process, we calculate the Euclidean distance between each node and other nodes, connecting it to the nearest five neighbors to construct the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}=\left(S,{E}_{r}\right)$$\end{document}Gr=S,Er, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{r}=\{({R}_{i},{R}_{j})|d({R}_{i},{R}_{j})\le d,{R}_{i},{R}_{j}\in R,i\ne j\}$$\end{document}Er={(Ri,Rj)|d(Ri,Rj)≤d,Ri,Rj∈R,i≠j} denotes the set of edges where the Euclidean distance between the alpha carbon atoms of residues \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{i}$$\end{document}Ri and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{j}$$\end{document}Rj is less than the K-nearest neighbors threshold d.To learn and capture the local interactions and spatial proximity within the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}$$\end{document}Gr, we utilize a model based on Variational Autoencoder (VAE) for unsupervised learning of graph-structured data, referred to as Variational Graph Autoencoder (VGAE) [54]. VGAE utilizes GCN [55] to extract structural features of protein graphs, encoding amino acid residues and their interactions into latent representations, and models uncertainty through variational inference to adapt to the diversity of protein structures. Its self-supervised learning effectively reconstructs protein graphs, and combined with negative sampling, enhances the discrimination of true connections, improving generalization capabilities.As shown in Fig. 9b, VGAE primarily consists of two parts:GCN Encoder: The GCN layers encode the input graph data, mapping the structural information of the graph into latent space, and output mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$log(\sigma )$$\end{document}log(σ) in the latent space. Through reparameterization, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}σ are used to obtain the latent representation \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, Where b and d are the product of the batch size and the number of amino acid residues in the protein and the dimension of the one-hot encoding, respectivelyInner Product Decoder: The decoder is responsible for predicting the probability of edge existence from the node representations Z in the latent space, which was shown in Eq. 4:4\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{A}}_{ij}=Sig({Z}_{i}{Z}_{j})\end{array}$$\end{document}A^ij=Sig(ZiZj)where Sig is the Sigmoid activation function, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\widehat{A}}_{ij}$$\end{document}A^ij represents the predicted probability of the edge. \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{i}$$\end{document}Zi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{j}$$\end{document}Zj represent the representation vectors of nodes i and j in the latent space.The training objective of the model is to minimize the reconstruction loss, which measures the alignment between the predicted edges (connection probabilities) and the actual edges. The reconstruction loss is calculated from the given positive edges (edges that exist, with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Sig}({Z}_{i}{Z}_{j})$$\end{document}Sig(ZiZj)) and negative edges (nonexistent edges, generated through negative sampling with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)$$\end{document}1-SigZiZj). It is assessed using binary cross-entropy (BCE) to determine the presence or absence of edges, which is defined as Eq. 5:5\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}L=-\frac{1}{\left|{\mathcal{E}}^{+}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{+}}\text{logSig}\left({Z}_{i}{Z}_{j}\right) -\frac{1}{\left|{\mathcal{E}}^{-}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{-}}\text{log}\left(1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)\right)\end{array}$$\end{document}L=-1E+∑i,j∈E+logSigZiZj-1E-∑i,j∈E-log1-SigZiZjwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{+}$$\end{document}E+ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{-}$$\end{document}E- represent the sets of positive and negative edges respectively.Point cloud feature extractionSuppose the protein consists of N atoms; then the point cloud of the protein can be defined as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{PC}={\{\text{pc}}_{\text{i}}\left|{\text{pc}}_{\text{i}}=\left({\text{x}}_{\text{i}}, {\text{y}}_{\text{i}}, {\text{z}}_{\text{i}}\right), \text{i}=1, 2, \cdots , \text{N}\right\}$$\end{document}PC={pcipci=xi,yi,zi,i=1,2,⋯,N. To ensure that all point clouds have a consistent spatial distribution and size, standardization operations such as centering, scaling, and fixed point counts are performed on the point cloud data. The point cloud data is represented as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times d}$$\end{document}X∈Rn×d, where n and d represent the three spatial dimensions and the number of points in each spatial dimension, respectively. The specific steps are as follows:
Calculate the centroid of the point cloud by Eq. 6:6\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}c=\frac{1}{N}\sum_{i=1}^{N}{pc}_{i}\end{array}$$\end{document}c=1N∑i=1NpciTranslate the point cloud to the origin by Eq. 7:7\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}{\prime}={pc}_{i}-c\end{array}$$\end{document}pci′=pci-cCalculate the maximum norm of the point cloud by Eq. 8:8\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{r}_{\text{max}}=\underset{i}{\text{max}}\parallel {pc}_{i}{\prime}\parallel \end{array}$$\end{document}rmax=maxi‖pci′‖Scale the point cloud to fit within the unit sphere by Eq. 9:9\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}^{{\prime}{\prime}}=\frac{{pc}_{i}{\prime}}{{r}_{\text{max}}}\end{array}$$\end{document}pci′′=pci′rmaxEnsure each point cloud has the same number of points M: If N < M, fill the point count with zero vectors to reach M; if N > M, truncate the point cloud to M points. In the experiment, we set M to 2048.
We employed an unsupervised learning model based on Autoencoder called PAE [56] to reconstruct the 3D point cloud representation of proteins, thereby capturing their 3D spatial information, including spatial arrangement, folding patterns, and dynamic conformational changes, as well as geometric details and global structures. PAE utilizes the PointNet encoder to extract local and global features of protein point clouds and integrates a Spatial Transformer Network (STN) for rotational alignment to enhance structural consistency, while optimizing reconstruction accuracy through Chamfer distance to better preserve the geometric structure and topological relationships of proteins.As shown in Fig. 9c, PAE consists of two core components:Encoder: Using PointNet [57] to extract global features from the point cloud, we further map these features to a low-dimensional latent space \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h\in {\mathbb{R}}^{b\times d}$$\end{document}h∈Rb×d via fully connected layers. Here, b and d denote the batch size and the feature dimension of the point cloud, respectively.Decoder: The decoder remaps the k-dimensional representation from the encoder back to the spatial coordinates of the point cloud data.The model uses Chamfer Distance (CD) [58] to compute the square of the Euclidean distance from each point in one point set to the nearest point in the other point set and sums the distances to measure the similarity between the two point clouds, which is shown in Eq. 10:10\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}CD\left({S}_{1},{S}_{2}\right)=\sum_{x\in {S}_{1}} \underset{y\in {S}_{2}}{min} {\parallel x-y \parallel }_{2}^{2}+\sum_{y\in {S}_{2}} \underset{x\in {S}_{1}}{min} {\parallel x-y \parallel }_{2}^{2}\end{array}$$\end{document}CDS1,S2=∑x∈S1miny∈S2‖x-y‖22+∑y∈S2minx∈S1‖x-y‖22Here, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{1}$$\end{document}S1 and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{2}$$\end{document}S2 represent the point sets of the point clouds.Multimodal feature extractionAfter obtaining the 640-dimensional protein sequence features, protein structural features, and three-dimensional spatial features generated by VAE, VGAE, and PAE, we perform z-score normalization on the feature vectors. This eliminates the dimensional influences among different features, allowing the features to be compared on the same scale to achieve balanced effects in multi-modal fusion. As shown in Fig. 10, we also employ AE for unsupervised learning of multi-modal protein features, called Fusion Autoencoder (FAE), with specific steps as follows:Fig. 10The architecture of FAE. FAE performs unsupervised learning by compressing multimodal feature vectors and minimizing reconstruction loss, optimizing the feature fusion process to retain original information(1) Encoder: The encoder compresses the input multi-modal feature vector Z into a low-dimensional latent space, achieving feature fusion and dimensionality reduction, and outputs the latent representation, which is shown in Eq. 11:11\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}H=ReLU\left(\text{Linear}\left(\text{Tanh}\left(\text{Linear}\left(\text{Z}\right)\right)\right)\right)\end{array}$$\end{document}H=ReLULinearTanhLinearZwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, b and d represent the batch size and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$640\times 3$$\end{document}640×3 dimensions, while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\in {\mathbb{R}}^{b\times k}$$\end{document}H∈Rb×k, k is 1024 dimensions.(2) Decoder: The decoder maps the encoded latent representation h back to the original feature space, ensuring that the feature fusion process retains the original information and outputs the reconstructed feature vector, which is shown in Eq. 12:12\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\widehat{Z}=Linear\left(\text{ReLU}\left(\text{Linear}\left(\text{H}\right)\right)\right)\end{array}$$\end{document}Z^=LinearReLULinearHFAE uses mean squared error (MSE) loss as the reconstruction objective, minimizing the difference between the original input Z and the reconstructed output \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{Z}$$\end{document}Z^. MSE loss can be defined as Eq. 13:13\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}{\left({Z}_{i}-{\widehat{Z}}_{i}\right)}^{2}\end{array}$$\end{document}LMSE=1N∑i=1NZi-Z^i2By ensuring the effectiveness of the compressed representations and optimizing the reconstruction quality, this model effectively improves the feature fusion process.GraphGPSGraph position encoding [59, 60] aims to provide effective embedding representations for graphs or their subgraphs, enhancing nodes’ understanding of their positions within the overall graph. This encoding method enhances the model’s representational capability by capturing the relationships and structural features among nodes, particularly when handling complex graph data. When two nodes share similar subgraphs or structures, their structural encodings should remain close, which is crucial for many graph neural network tasks. In particular, local structure encoding (Local SE) [61] allows each node to understand its surrounding substructure. This encoding method not only facilitates the transmission of information but also effectively captures the m-hop subgraph features around the nodes. Specifically, local structure encoding enhances nodes’ perception of their surroundings by identifying features related to the nodes, such as node degrees, diagonal elements of the random walk matrix, and predefined substructural statistics. With a structural encoding of radius m, the more similar the m-hop subgraphs between two nodes, the closer their local structure encodings will be. This proximity not only reflects the similarity between nodes but also provides a rich informational foundation for downstream networks. Meanwhile, the design of local structure encoding effectively suppresses the effects of noise and redundant information, thereby enhancing the robustness and performance of the model. We generated local structure encodings according to LSPE [61]. This structural encoding computation effectively integrates the information of node i and its neighboring nodes, allowing for an expression of the node’s local features. These local features play a crucial role in graph learning, helping the model understand the position of the node within the overall structure.When the paths between nodes are long, information may encounter compression during transmission, leading to loss of information. Additionally, the local attention mechanism aims to achieve global information interaction between nodes by directly connecting them, effectively alleviating the issue of information compression. As shown in Fig. 11, GraphGPS [59] combines local GCN and global attention mechanisms [62], enhancing GCN’s performance in processing graph data by introducing local structure encoding and global feature capture.Fig. 11The architecture of GraphGPS. GraphGPS combines local graph convolutional networks with global attention mechanisms to alleviate the information compressing problem by directly connecting nodes. This effectively aggregates local and global features, enhancing the performance of graph data processing and strengthening the model’s understanding and learning ability of graph structuresThe overall computation formula of GraphGPS can be defined as Eq. 14:14\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{X}}^{{\ell}+1}=GP{\text{S}}^{{\ell}}\left({\text{X}}^{{\ell}},\text{A}\right)\end{array}$$\end{document}Xℓ+1=GPSℓXℓ,Awhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{X}}^l$$\end{document}Xl represents the node feature matrix at layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}l, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {A}$$\end{document}A denotes the adjacency matrix. The specific computation process is as follows:(1) The local graph convolutional network effectively aggregates feature information from each node and its neighbors to update the node’s feature representation, which is shown in Eq. 15:15\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}_{M}^{{\ell}+1}={\text{GCN}}^{{\ell}}\left({\text{X}}^{{\ell}},\text{A}\right)\end{array}$$\end{document}HMℓ+1=GCNℓXℓ,A(2) Use the global attention mechanism to aggregate node features, allowing the model to consider the features of all nodes comprehensively, thus capturing global information, which is shown in Eq. 16:16\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}_{T}^{{\ell}+1}={\text{GlobalAttn}}^{{\ell}}\left({\text{H}}^{{\ell}}\right)\end{array}$$\end{document}HTℓ+1=GlobalAttnℓHℓ(3) Combine local features and global features through MLP to generate the final node feature representation, which is shown in Eq. 17:17\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}^{{\ell}+1}={\text{MLP}}^{{\ell}}\left({\text{H}}_{M}^{{\ell}+1}+{\text{H}}_{T}^{{\ell}+1}\right)\end{array}$$\end{document}Hℓ+1=MLPℓHMℓ+1+HTℓ+1This approach helps GraphGPS understand the overall structure of the graph at a higher level during information propagation and feature learning, better combining local neighborhood information with global context and enhancing the model’s learning capacity for graph data.GAT [63] assigns dynamic weights to nodes by aggregating node information, further refining the global and local features output by GraphGPS and extracting more meaningful features from local structures to capture finer-grained interactions and relationships.First, for node i and one of its neighboring nodes j, apply the shared weight matrix W, as shown in Eq. 18:18\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm h'_{\mathrm i\;}\;={\mathrm{Wx}}_{\mathrm i},\;\mathrm h'_{\mathrm j}\;={\mathrm{Wx}}_{\mathrm j}$$\end{document}hi′=Wxi,hj′=WxjNext, calculate the raw attention score for nodes i and j, which can be defined as Eq. 19:
19\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{e_{ij}}=\mathrm{LeakyReLU}\;\left(\mathrm a^\top\cdot\left[h'_i+h'_j\right]\right)$$\end{document}eij=LeakyReLUa⊤·hi′+hj′
where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{a}$$\end{document}a is a learnable parameter vector, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{i}'+{\text{h}}_{j}'$$\end{document}hi′+hj′ represents the sum of the two features, while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\cdot )$$\end{document}(·) denotes the dot product operation.The raw attention coefficients are normalized over all neighboring nodes of node i using the softmax function to obtain the normalized attention coefficients \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}$$\end{document}αij, as shown in Eq. 20:20\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\alpha }_{ij}=\frac{\text{exp}\left({e}_{ij}\right)}{\sum_{k\in \mathcal{N}\left(i\right)\cup i} \text{exp}\left({e}_{ik}\right)}\end{array}$$\end{document}αij=expeij∑k∈Ni∪iexpeikFinally, perform weighted aggregation using \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}$$\end{document}αij and the transformed features of its neighboring nodes to obtain the output feature of node i, as shown in Eq. 21:21\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{x}'_{i}\;={\sum_{\text{j}\in N(i)\cup i\;}} \alpha_{ij}h'_{\mathrm j}$$\end{document}xi′=∑j∈N(i)∪iαijhj′SubgraphGCNMessage Passing Neural Networks (MPNNs) [64], as one of the classic implementations of Graph Neural Networks (GNNs), has been widely applied to protein–protein interaction prediction tasks and has demonstrated exceptional performance. In layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}+1$$\end{document}ℓ+1 the feature of node v is calculated as Eq.  22:22\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}h_v^{\left(\ell+1\right)}=\gamma\left(h_v^{\left(\ell\right)},\text{AGGREGATE}\left(\{\phi\left(h_j^{\left(\ell\right)},e_{vj}\right)\mid j\in\mathcal N\left(v\right)\}\right)\right)\end{array}$$\end{document}hvℓ+1=γhvℓ,AGGREGATE{ϕhjℓ,evj∣j∈Nv}Here, denotes the message generation process; AGGREGATE indicates the function used to aggregate messages from neighboring nodes (e.g., sum, mean, or max), and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}γ is the function that updates the node features. The choice of different message functions, aggregation functions, and update functions affects the expressive power and performance of MPNNs. When all three functions are injective (one-to-one mappings), MPNNs achieve the same expressive power as the 1-Weisfeiler-Lehman (1-WL) isomorphism test [36].During the computation of MPNN, each node follows a star pattern during message aggregation, where the central node of the graph is connected to all its neighbors and aggregates messages solely from its direct neighbors. The star pattern around the central node v is defined as Eq. 23:23\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Star\left(v\right)=\left({\mathcal{N}}_{1}\left(v\right),\{\left(v,j\right)\in \mathcal{E}|j\in \mathcal{N}\left(v\right)\}\right)\end{array}$$\end{document}Starv=N1v,{v,j∈E|j∈Nv}where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{N}}_{1}\left(v\right)$$\end{document}N1v signifies the subgraph centered at node v, including v and all nodes and edges reachable within one hop. When handling non-isomorphic regular graphs, MPNN may struggle to differentiate these graphs, weakening its discriminative ability. To address this issue, Subgraph-1-WL extends the 1-WL algorithm by generalizing the concept of the star graph Star(v) to the k-hop neighborhood \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G\left[{N}_{k}\left(v\right)\right]$$\end{document}GNkv centered at v, thereby improving the expressive power for graph isomorphism detection. SubgraphGCN [65] combines Subgraph-1-WL and GCN, with its internal node feature update process defined as Eq. 24:24\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{h}_{v}^{\left({\ell}+1\right)}={\text{GCN}}^{\left({\ell}\right)}\left(Su{b}^{\left({\ell}\right)}\left[v\right]\right),\hspace{1em}l=0,\dots ,L-1\end{array}$$\end{document}hvℓ+1=GCNℓSubℓv,l=0,⋯,L-1Here, indicates the one-hop subgraph centered at node v. The subgraph generation process primarily includes the following steps: First, based on the edge indices and the number of nodes in the original graph, a 1-hop neighborhood expansion is used to generate local subgraphs for each node. Simultaneously, a Boolean mask matrix is generated to indicate which nodes belong to each subgraph, and the hop counts between nodes are recorded. Next, the dense mask is converted into a sparse format for node and edge indices. Finally, the edges of all subgraphs are merged into a global edge index, while node IDs are remapped to avoid overlaps. After obtaining all subgraphs corresponding to the PPI network graph, SubgraphGCN extracts node features from the global features, integrating the central node features, mean-pooled topological features within the subgraph, and globally aggregated contextual features across subgraphs, resulting in richer and more comprehensive feature representations.As shown in Fig. 12, SubgraphGCN computes three types of encodings:Fig. 12The architecture of SubgraphGCN. SubgraphGCN decomposes large graphs into smaller subgraphs by combining centroid encoding, subgraph encoding, and context encoding, utilizing the 1-hop egonet aggregation method. This enhances the model’s ability to recognize node connection patterns and improves computational efficiency(1) Centroid encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|Centroid}$$\end{document}hv(ℓ+1)|Centroid focuses on representing the features of the central node within the subgraph. It efficiently encodes the local characteristics of node v by embedding within the node's star-shaped subgraph. This method effectively integrates the information from direct neighbors, enhancing the self-representation of the node, defined as Eq. 25:25\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\text{|}Centroid}=\;\mathrm{Emb}\left(v\mid\text{Sub}^{\left(\ell+1\right)}\left[v\right]\right)\end{array}$$\end{document}hv(ℓ+1)|Centroid=Embv∣Subℓ+1v(2) The subgraph encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|\text{Subgraph}}$$\end{document}hv(ℓ+1)|Subgraph aggregates features from the subgraph surrounding node v to form a comprehensive node representation. By applying the mean pooling operation, the subgraph encoding integrates rich information from neighboring nodes, thus providing an in-depth characterization of complex relationships between nodes, defined as Eq. 26:26\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\vert \text{Subgraph}}=\text{GCN}^{\left(\ell\right)}\left(\text{Sub}^{\left(\ell\right)}\left[v\right]\right)=MEAN\left(\left\{\text{Emb}\left(i\mid\text{Sub}^{\left(\ell\right)}\left[v\right]\right)\mid i\in{\mathcal N}_k\left(v\right)\right\}\right)\end{array}$$\end{document}hv(ℓ+1)|Subgraph=GCNℓSubℓv=MEANEmbi∣Subℓv∣i∈Nkv(3) The context encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|\text{Context}}$$\end{document}hv(ℓ+1)|Context aims to analyze the node within different subgraph contexts to capture broader feature information. By summarizing the embeddings of the node in various contexts, context encoding ensures that the node representation encompasses information from multiple perspectives, enhancing overall expressive capability, defined as Eq. 27:27\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\vert^{Context}}=MEAN\left(\left\{\text{Emb}\left(v\mid\text{Sub}^{\left(\ell\right)}\left[j\right]\right)\mid\forall js.t.\in{\mathcal N}_k\left(j\right)\right\}\right)\end{array}$$\end{document}hvContext=MEANEmbv∣Subℓj∣∀js.t.∈NkjBy combining centroid encoding, subgraph encoding, and context encoding, SubgraphGCN for layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}+1$$\end{document}ℓ+1 can be defined as Eq. 28:28\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{h}}_{v}^{\left({\ell}+1\right)}=SUM\left({\text{h}}_{v}^{({\ell}+1)|\text{Centroid}},{\text{h}}_{v}^{({\ell}+1)|\text{Subgraph}}, {\text{h}}_{v}^{({\ell}+1)|\text{Context}}\right)\end{array}$$\end{document}hvℓ+1=SUMhv(ℓ+1)|Centroid,hv(ℓ+1)|Subgraph,hv(ℓ+1)|Contextwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textit{Emb }\left(i\mid\text{Sub}^{\left(\ell\right)}\left[j\right]\text{ }\right)$$\end{document}Embi∣Subℓj represents the embedding of node when applying GCN at layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}$$\end{document}ℓ.By using the aggregation method of the 1-hop egonet instead of the traditional star pattern, SubgraphGCN can effectively recognize different connection patterns exhibited by nodes with the same degree across different graphs, significantly enhancing the model’s discriminative power. This innovative approach not only enhances the understanding of relationships between nodes but also helps capture more fine-grained structural features, effectively reflecting the complexity of the graph. The advantages of SubgraphGCN are also evident in its intelligent decomposition of the entire large graph into smaller, more manageable subgraphs. Through this decomposition, the model can leverage GCN for more detailed feature aggregation and learning within relatively smaller scopes, improving computational efficiency and allowing the model to delve deeper into the potential information of each subgraph. This method ensures that key features can be comprehensively and effectively extracted when processing raw heterogeneous graphs, providing strong support for subsequent analysis and predictions. SubgraphGCN fully leverages the unique advantages of graph neural networks in modeling graph structural information. By systematically performing key steps such as subgraph extraction, node feature processing, and feature aggregation, the model can better understand the relationship between local and global structures. This detailed and comprehensive feature extraction capability is particularly important for PPI prediction tasks.Classifier and loss functionLet \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=\{{P}_{1},\cdots ,{P}_{N}\}$$\end{document}P={P1,⋯,PN} be the set of proteins. \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E=\left\{\left({P}_{i}, {P}_{j}\right) \right| i\ne j\}$$\end{document}E=Pi,Pji≠j} is the collection of PPIs. A set of type labels \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=\left\{{L}_{1}, {L}_{2},\cdots , {L}_{7}\right\}$$\end{document}L=L1,L2,⋯,L7 corresponds to each protein pair \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({P}_{i},{P}_{j}\right)$$\end{document}Pi,Pj. If there are multiple interaction types between \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{P}}_{\text{i}}$$\end{document}Pi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj, the corresponding position in the type label set L is set to 1; otherwise, it is set to 0. We treat proteins and PPIs as nodes and edges, respectively, thus constructing a comprehensive PPI network graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G=\left(P, E\right)$$\end{document}G=P,E. Since each PPI has at least one and at most seven types of PPI interactions, we extract the corresponding edge set \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{k}=\left\{\left({P}_{i}, {P}_{j}\right) \right| i\ne j, {L}_{k}=1\}$$\end{document}Ek=Pi,Pji≠j,Lk=1} to construct seven independent PPI network graphs \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{k}=\left(P,{E}_{k}\right)$$\end{document}Gk=P,Ek.As shown in Fig. 13, the final protein representation is \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}=\left[X,{z}_{1},{z}_{2},\cdots ,{z}_{7}\right]$$\end{document}Pi=X,z1,z2,⋯,z7, where X represents the multimodal protein features generated by the pre-trained model FAE, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}_{1},{z}_{2},\cdots ,{z}_{7}$$\end{document}z1,z2,⋯,z7 denote the protein features learned by MESM from seven independent PPI network graphs \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{k}$$\end{document}Gk, with [] indicating horizontal concatenation. This representation not only preserves the original protein information but also integrates unique protein association information from different interaction perspectives, further enhancing MESM’s ability to learn interactions among these seven proteins.Fig. 13The architecture of Classifier. By horizontally concatenating multimodal protein features with features from seven different PPI network graphs, a joint representation is constructed, and interaction features of protein pairs are extracted through element-wise multiplication. Finally, a multilayer perceptron is used to estimate the interaction probabilities between proteinsWe perform element-wise multiplication on \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi  and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj  to construct the joint representation between protein pairs, as shown in Eq. 29:
29\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm H}_{{\mathrm P}_{\mathrm i},{\mathrm P}_{\mathrm j}}={\mathrm P}_{\mathrm i}\odot{\mathrm P}_{\mathrm j}$$\end{document}HPi,Pj=Pi⊙PjThis element-wise multiplication operation effectively combines the feature information of the two proteins, thus extracting their interaction features. This joint representation is then fed into MLP to estimate the interaction probability between proteins \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj. The final output is defined as Eq. 30:30\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{\text{y}}}_{\left({\text{P}}_{\text{i}},{\text{P}}_{\text{j}}\right)}=Sig\left(\text{MLP}\left({{\text{H}}_{{\text{P}}_{\text{i}},{\text{P}}_{\text{j}}}}\right)\right)\end{array}$$\end{document}y^Pi,Pj=SigMLPHPi,PjTo train this model, we adopt the ZLPR (zero-bounded log-sum-exp & pairwise rank-based) [66] loss function, which is a loss function for multi-label classification problems and can be viewed as a natural generalization of “Softmax and cross-entropy.” The form of the ZLPR loss function is defined as Eq. 31:31\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\mathcal{L}}_{\text{zlpr}}=\text{log}\left(1+\sum_{\text{i}\in {\Omega }_{\text{pos}}} {\text{e}}^{-{\text{s}}_{\text{i}}}\right)+\text{log}\left(1+\sum_{\text{j}\in {\Omega }_{\text{neg}}} {\text{e}}^{{\text{s}}_{\text{j}}}\right)\end{array}$$\end{document}Lzlpr=log1+∑i∈Ωpose-si+log1+∑j∈ΩnegesjHere, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}$$\end{document}si is the output score of the model for the ith class(\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\lambda }_{i}$$\end{document}λi; during prediction, if \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}>0$$\end{document}si>0, it indicates that \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\lambda }_{i}$$\end{document}λi could be the target class, while if \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}<0$$\end{document}si<0, it indicates otherwise. The ZLPR loss function is characterized by its ability to effectively capture dependencies between labels, allowing for a reasonable balance of influence between positive and negative classes in the model.Experimental settings and evaluation metricsIn this study, we employed the Adam algorithm [67] to optimize the training process for the pre-training of multimodal proteins. The entire training process lasted for 200 epochs, with the learning rate set to 0.001 to ensure effective learning of the model. For different network architectures, we adopted varying batch sizes, with the batch sizes for SVAE, VGAE, PAE, and FAE set to 256, 256, 32, and 1024, respectively. In constructing the PPI prediction network, we utilized the AdamW optimization algorithm [68], which combines momentum updates with weight decay from the Adam algorithm, effectively preventing overfitting. The learning rate was set to 0.0001, and training was conducted for a total of 100 epochs to ensure that the model could fully learn the features of interactions between proteins. Additionally, we used a learning rate scheduler [68] that optimized the learning rate during the model training process through an initial linear increase strategy followed by gradual decay, allowing the model to better adapt to the needs of parameter updates at different training stages. We selected a 1024-dimensional protein feature representation and a 20-dimensional structural encoding dimension to capture the diversity and complexity of proteins. For each different type of PPI network graph, we used a single layer of GraphGPS, GAT, SubgraphGCN, and GCN, leveraging these graph neural networks to capture local and global graph structural information more effectively, thus enhancing the modeling capability for complex protein networks.MESM was implemented in Python 3.10, leveraging PyTorch 2.2 for deep learning operations and PyTorch Geometric (PyG) 2.6 for graph-based computations. All experiments were conducted on an NVIDIA A100 GPU under a Linux operating system. We sequentially trained MESM on four datasets, with the model containing a total of 172.49 million trainable parameters. Table 6 presents the computational cost of MESM, showing that as the PPI network becomes more complex, the model requires more training time and greater GPU memory. More specific implementation details about the dataset and code can be obtained from 10.5281/zenodo.15321475.
Table 6The computational cost of training MESMDatasetRuntime (seconds)GPU memory (GB)SHS27k2988.33SHS148k137428.83SYS30k40611.88SYS60k76118.9In terms of dataset partitioning, we adopted three different partitioning strategies [35]: Breadth-First Search (BFS), Depth-First Search (DFS), and Random division, splitting the PPI dataset into training and test sets in an 8:2 ratio. During the dataset partitioning process, we used a random seed to ensure that the combinations of the training and test sets were different during each training session, thereby enhancing the model’s generalization ability. Each experiment was repeated five times to obtain the mathematical expectation and standard deviation of the results, which will serve as indicators of predictive performance.Since the distribution of different PPI types in our dataset is imbalanced, we chose Micro-F1 as the multi-label classification evaluation metric for handling imbalanced datasets. Micro-F1 effectively consolidates the predictive performance across multiple classes, particularly excelling when dealing with imbalanced samples. To this end, the precision and recall for the ith class are defined in Eq. 32 and 33:32\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Precision_{i}=\frac{{\text{TP}}_{i}}{{\text{TP}}_{i}+{\text{FP}}_{i}}\end{array}$$\end{document}Precisioni=TPiTPi+FPi33\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Recall_{i}=\frac{{\text{TP}}_{i}}{{\text{TP}}_{i}+{\text{FN}}_{i}}\end{array}$$\end{document}Recalli=TPiTPi+FNiwhere TP (True Positive) denotes the number of samples correctly classified as positive, FP (False Positive) denotes the number of samples incorrectly classified as positive, and FN (False Negative) denotes the number of samples that are actually positive but not classified as such.In our research task, the definition of Micro-F1 is defined in Eq. 34:\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Precision}}_{\text{Micro}}=\frac{\sum_{i=1}^{7}T{P}_{i}}{\sum_{i=1}^{7}T{P}_{i}+\sum_{i=1}^{7}F{P}_{i}}$$\end{document}PrecisionMicro=∑i=17TPi∑i=17TPi+∑i=17FPi34\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Recall_{\text{Micro}}=\frac{\sum_{i=1}^{7}T{P}_{i}}{\sum_{i=1}^{7}T{P}_{i}+\sum_{i=1}^{7}{FN}_{i}}\end{array}$$\end{document}RecallMicro=∑i=17TPi∑i=17TPi+∑i=17FNi\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Micro}-\text{F}1=2\times \frac{{\text{Precision}}_{\text{Micro}}\times {\text{Recall}}_{\text{Micro}}}{{\text{Precision}}_{\text{Micro}}+{\text{Recall}}_{\text{Micro}}}$$\end{document}Micro-F1=2×PrecisionMicro×RecallMicroPrecisionMicro+RecallMicroBy utilizing these metrics, we can comprehensively evaluate the model’s performance in a multi-label environment, ensuring that it performs well not only in specific classes but also enhances overall generalization capability.

DatasetIn this study, we utilized multi-label protein–protein interaction data from SHS27k, SHS148k [48], SYS30k, and SYS60k [41] datasets to evaluate the performance of MESM in comparison with other PPI prediction methods. The SHS27k and SHS148k datasets were constructed by randomly selecting proteins with sequence similarity below 40% from the human PPI subset data of the STRING database [44], resulting in 1690 and 5189 proteins, respectively. The two datasets contain 7624 and 44,488 multi-label PPIs, respectively. To investigate MESM’s performance in predicting PPIs across different species, we selected SYS30k and SYS60k datasets. These two datasets were constructed based on the data construction strategy of SHS27k and SHS148k, randomly selecting 2685 and 3549 yeast proteins from the yeast PPI subset data of the STRING database, with 10,806 and 22,707 multi-label PPIs, respectively. The protein data used for training, including sequence information and PDB structure, are obtained from Uniprot [49] and AlphaFold [50]. However, some proteins lack structural data in these protein databases, necessitating the exclusion of approximately 2.8% of proteins to ensure data integrity [51].As shown in Table 5, there are a total of seven types of PPIs in these four PPI datasets: Reaction, Binding, Post-translational modification (Ptmod), Activation, Inhibition, Catalysis, and Expression. It can be seen that the distribution of various PPI types across the datasets typically exhibits imbalance, with significant differences in sample sizes, where certain types have far more data samples than others. For example, in the SHS27k dataset, the sample sizes for Reaction, Binding, Activation, and Catalysis greatly exceed those of other types, each accounting for 18% or more, while the total proportion of Ptmod, Inhibition, and Expression is only 19.08%. This imbalanced distribution may result in trained models performing well in classification for types with larger sample sizes, but the classification performance significantly declines for certain PPI types with fewer samples (such as Expression and Ptmod).
Table 5Number and ratio of seven PPI types in four datasetsPPI typesSHS27kSHS148kSYS30kSYS60kNumberRatioNumberRatioNumberRatioNumberRatioReaction312018.45%1778817.83%590921.28%1166120.84%Binding393023.24%2294322.99%893032.17%1869933.42%Ptmod12337.29%90599.08%18726.74%36526.53%Activation320018.92%1844618.49%322411.61%643511.5%Inhibition13718.11%87638.78%10053.62%20823.72%Catalysis343620.32%1956019.6%544819.62%1071819.15%Expression6223.68%32233.23%13754.95%27094.84%

In this study, we utilized multi-label protein–protein interaction data from SHS27k, SHS148k [48], SYS30k, and SYS60k [41] datasets to evaluate the performance of MESM in comparison with other PPI prediction methods. The SHS27k and SHS148k datasets were constructed by randomly selecting proteins with sequence similarity below 40% from the human PPI subset data of the STRING database [44], resulting in 1690 and 5189 proteins, respectively. The two datasets contain 7624 and 44,488 multi-label PPIs, respectively. To investigate MESM’s performance in predicting PPIs across different species, we selected SYS30k and SYS60k datasets. These two datasets were constructed based on the data construction strategy of SHS27k and SHS148k, randomly selecting 2685 and 3549 yeast proteins from the yeast PPI subset data of the STRING database, with 10,806 and 22,707 multi-label PPIs, respectively. The protein data used for training, including sequence information and PDB structure, are obtained from Uniprot [49] and AlphaFold [50]. However, some proteins lack structural data in these protein databases, necessitating the exclusion of approximately 2.8% of proteins to ensure data integrity [51].

As shown in Table 5, there are a total of seven types of PPIs in these four PPI datasets: Reaction, Binding, Post-translational modification (Ptmod), Activation, Inhibition, Catalysis, and Expression. It can be seen that the distribution of various PPI types across the datasets typically exhibits imbalance, with significant differences in sample sizes, where certain types have far more data samples than others. For example, in the SHS27k dataset, the sample sizes for Reaction, Binding, Activation, and Catalysis greatly exceed those of other types, each accounting for 18% or more, while the total proportion of Ptmod, Inhibition, and Expression is only 19.08%. This imbalanced distribution may result in trained models performing well in classification for types with larger sample sizes, but the classification performance significantly declines for certain PPI types with fewer samples (such as Expression and Ptmod).

Table 5Number and ratio of seven PPI types in four datasetsPPI typesSHS27kSHS148kSYS30kSYS60kNumberRatioNumberRatioNumberRatioNumberRatioReaction312018.45%1778817.83%590921.28%1166120.84%Binding393023.24%2294322.99%893032.17%1869933.42%Ptmod12337.29%90599.08%18726.74%36526.53%Activation320018.92%1844618.49%322411.61%643511.5%Inhibition13718.11%87638.78%10053.62%20823.72%Catalysis343620.32%1956019.6%544819.62%1071819.15%Expression6223.68%32233.23%13754.95%27094.84%

Number and ratio of seven PPI types in four datasets

Unsupervised pre-training model for extracting multimodal protein featuresExisting PPI prediction methods typically adopt an end-to-end learning strategy [35], where the model is trained directly from protein feature extraction to PPI prediction. Although this approach allows the model to learn directly from the data, end-to-end learning requires simultaneous completion of feature extraction and task optimization, which is often challenging to achieve effectively, especially when data is insufficient. Furthermore, in protein–protein prediction tasks, graph neural network-based models generally require the complete PPI network graph as input, with larger datasets resulting in greater computational burdens [39]. Therefore, we adopt a stepwise approach for extracting protein features using a pre-training method. As shown in Fig. 1a, we adjust the model framework based on the pre-training model MPRL [52] for the protein–protein interaction prediction task to capture more expressive multimodal protein features.MPRL utilizes the primary and tertiary structures of proteins for unsupervised multimodal protein representation learning. For protein \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}\in P$$\end{document}Pi∈P, its primary structure is represented as its amino acid sequence \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S$$\end{document}S, written as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\left({v}_{1},{v}_{2},\cdots ,{v}_{M}\right)$$\end{document}S=v1,v2,⋯,vM, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${v}_{M}$$\end{document}vM denotes the m-th amino acid residue in the sequence, and M is the total number of amino acid residues in the sequence. Each \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${v}_{m}$$\end{document}vm belongs to the amino acid set V which contains 20 amino acid types. The tertiary structure of protein \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi is the structure formed in three-dimensional space by the folding of the amino acid chain \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{i}$$\end{document}Si through non-covalent interactions (such as hydrogen bonds, hydrophobic interactions, etc.), describing the atomic positions and folding states of the protein in three-dimensional space, represented as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${T}_{i}=\left({x}_{1},{y}_{1},{z}_{1};{x}_{2},{x}_{2},{z}_{2};\cdots ;{x}_{M},{y}_{M},{z}_{M}\right)$$\end{document}Ti=x1,y1,z1;x2,x2,z2;⋯;xM,yM,zM, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({x}_{m},{y}_{m},{z}_{m}\right)$$\end{document}xm,ym,zm denotes the spatial coordinates of the m-th amino acid residue. Protein sequence features provide fundamental insights into protein information but are limited in capturing complex spatial and dynamic characteristics, failing to represent spatial conformations. In contrast, constructing a topological network through local connections between residues characterizes the three-dimensional folding state of proteins but struggles to capture long-range spatial correlations. Meanwhile, three-dimensional point cloud spatial features precisely describe the global geometric morphology of proteins through spatial coordinates of residues, enhancing the expression of spatial information but lacking fine-grained representation of residue features. These three types of data provide complementary information at the levels of molecular sequence, structure, and global spatial distribution, addressing the limitations of single data sources in terms of information completeness. Therefore, compared to relying solely on a single protein feature (e.g., sequence features), multimodal features can more effectively enhance protein representation learning and model performance in PPI prediction tasks through complementary advantages among various features.Sequence feature extractionWe designed a model called Sequence Variational Autoencoder (SVAE) based on Variational Autoencoder (VAE) [53] to extract sequence features. SVAE extracts short-range patterns through 1D convolution and combines bidirectional GRU to capture long-range dependencies across sequences, adapting to the complex structure and functional properties of proteins. Meanwhile, global average pooling compresses variable-length sequences, making them suitable for proteins of different lengths. Leveraging VAE, SVAE maps high-dimensional protein features into a continuous, smooth, and structured latent space, enabling the generation of biologically plausible sequence features while preserving key functional characteristics.As shown in Fig. 9a, SVAE mainly consists of two parts:Fig. 9a The architecture of SVAE. SVAE is used to extract sequence features. It consists of two parts: an encoder and a decoder. The model is jointly optimized through reconstruction loss and KL divergence for unsupervised learning and data generation. b The architecture of VGAE. VGAE utilizes GCN to encode local interactions and spatial relationships in protein structure graphs. It predicts the existence probability of edges through an inner product decoder and optimizes the model by minimizing reconstruction loss for unsupervised learning. c The architecture of PAE. PAE extracts global features of the 3D point clouds of proteins using PointNet. It maps these features to a low-dimensional latent space and calculates the similarity between two point clouds using Chamfer distance for unsupervised learning and 3D point cloud reconstructionEncoder: The encoder extracts sequence data into fixed-dimensional features \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times s\times d}$$\end{document}X∈Rn×s×d through convolutional layers, bidirectional gated recurrent units (BiGRU), and fully connected (FC) layers, where n, s and d represent the number of proteins, sequence length, and amino acid residue feature dimensions, respectively. It outputs mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{log}(\sigma )$$\end{document}log(σ) of the latent space to generate the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, where b and d represent the batch size and sequence feature dimension, respectively.Decoder: Starting from the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z$$\end{document}Z, the decoder generates reconstructed data of the input sequence features through fully connected layers and the nonlinear activation function ReLU.The loss function of SVAE can be defined as Eq. 1:1\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=L_{recon}+L_{KL}$$\end{document}L=Lrecon+LKLwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon represents the reconstruction loss, which measures the error between the model′s reconstructed data and the original data; while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL represents the KL divergence, which constrains the distribution of the latent variable Z to bring q(Z|X) close to the standard normal distribution p(Z) = N(0,1). The reconstruction loss \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon can be expressed as Eq. 2:2\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{recon}=\mathrm{MSE}\;(\mathrm x,\hat{\mathrm x})=\frac1N\sum\limits_{i=1}^N\left(x_{i\;\;}-\;\hat{x}_i\right)^2$$\end{document}Lrecon=MSE(x,x^)=1N∑i=1Nxi-x^i2where x denotes the input original data, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{x}$$\end{document}x^ represents the output reconstructed by the model, and N is the number of samples. The KL divergence \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL can be expressed as Eq. 3:3\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{KL}}=-\frac{1}{2}\left(1+\text{log}\left({\sigma }^{2}\right)-{\mu }^{2}-{\sigma }^{2}\right)\end{array}$$\end{document}LKL=-121+logσ2-μ2-σ2Structure feature extractionTo construct the protein structure graph, we use one-hot encoding to specify a specific amino acid type for each residue and employ the K-nearest neighbors (KNN) strategy to build edges based on the coordinates of the alpha carbon atoms of the amino acid residues within the protein.In this process, we calculate the Euclidean distance between each node and other nodes, connecting it to the nearest five neighbors to construct the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}=\left(S,{E}_{r}\right)$$\end{document}Gr=S,Er, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{r}=\{({R}_{i},{R}_{j})|d({R}_{i},{R}_{j})\le d,{R}_{i},{R}_{j}\in R,i\ne j\}$$\end{document}Er={(Ri,Rj)|d(Ri,Rj)≤d,Ri,Rj∈R,i≠j} denotes the set of edges where the Euclidean distance between the alpha carbon atoms of residues \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{i}$$\end{document}Ri and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{j}$$\end{document}Rj is less than the K-nearest neighbors threshold d.To learn and capture the local interactions and spatial proximity within the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}$$\end{document}Gr, we utilize a model based on Variational Autoencoder (VAE) for unsupervised learning of graph-structured data, referred to as Variational Graph Autoencoder (VGAE) [54]. VGAE utilizes GCN [55] to extract structural features of protein graphs, encoding amino acid residues and their interactions into latent representations, and models uncertainty through variational inference to adapt to the diversity of protein structures. Its self-supervised learning effectively reconstructs protein graphs, and combined with negative sampling, enhances the discrimination of true connections, improving generalization capabilities.As shown in Fig. 9b, VGAE primarily consists of two parts:GCN Encoder: The GCN layers encode the input graph data, mapping the structural information of the graph into latent space, and output mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$log(\sigma )$$\end{document}log(σ) in the latent space. Through reparameterization, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}σ are used to obtain the latent representation \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, Where b and d are the product of the batch size and the number of amino acid residues in the protein and the dimension of the one-hot encoding, respectivelyInner Product Decoder: The decoder is responsible for predicting the probability of edge existence from the node representations Z in the latent space, which was shown in Eq. 4:4\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{A}}_{ij}=Sig({Z}_{i}{Z}_{j})\end{array}$$\end{document}A^ij=Sig(ZiZj)where Sig is the Sigmoid activation function, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\widehat{A}}_{ij}$$\end{document}A^ij represents the predicted probability of the edge. \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{i}$$\end{document}Zi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{j}$$\end{document}Zj represent the representation vectors of nodes i and j in the latent space.The training objective of the model is to minimize the reconstruction loss, which measures the alignment between the predicted edges (connection probabilities) and the actual edges. The reconstruction loss is calculated from the given positive edges (edges that exist, with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Sig}({Z}_{i}{Z}_{j})$$\end{document}Sig(ZiZj)) and negative edges (nonexistent edges, generated through negative sampling with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)$$\end{document}1-SigZiZj). It is assessed using binary cross-entropy (BCE) to determine the presence or absence of edges, which is defined as Eq. 5:5\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}L=-\frac{1}{\left|{\mathcal{E}}^{+}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{+}}\text{logSig}\left({Z}_{i}{Z}_{j}\right) -\frac{1}{\left|{\mathcal{E}}^{-}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{-}}\text{log}\left(1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)\right)\end{array}$$\end{document}L=-1E+∑i,j∈E+logSigZiZj-1E-∑i,j∈E-log1-SigZiZjwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{+}$$\end{document}E+ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{-}$$\end{document}E- represent the sets of positive and negative edges respectively.Point cloud feature extractionSuppose the protein consists of N atoms; then the point cloud of the protein can be defined as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{PC}={\{\text{pc}}_{\text{i}}\left|{\text{pc}}_{\text{i}}=\left({\text{x}}_{\text{i}}, {\text{y}}_{\text{i}}, {\text{z}}_{\text{i}}\right), \text{i}=1, 2, \cdots , \text{N}\right\}$$\end{document}PC={pcipci=xi,yi,zi,i=1,2,⋯,N. To ensure that all point clouds have a consistent spatial distribution and size, standardization operations such as centering, scaling, and fixed point counts are performed on the point cloud data. The point cloud data is represented as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times d}$$\end{document}X∈Rn×d, where n and d represent the three spatial dimensions and the number of points in each spatial dimension, respectively. The specific steps are as follows:
Calculate the centroid of the point cloud by Eq. 6:6\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}c=\frac{1}{N}\sum_{i=1}^{N}{pc}_{i}\end{array}$$\end{document}c=1N∑i=1NpciTranslate the point cloud to the origin by Eq. 7:7\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}{\prime}={pc}_{i}-c\end{array}$$\end{document}pci′=pci-cCalculate the maximum norm of the point cloud by Eq. 8:8\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{r}_{\text{max}}=\underset{i}{\text{max}}\parallel {pc}_{i}{\prime}\parallel \end{array}$$\end{document}rmax=maxi‖pci′‖Scale the point cloud to fit within the unit sphere by Eq. 9:9\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}^{{\prime}{\prime}}=\frac{{pc}_{i}{\prime}}{{r}_{\text{max}}}\end{array}$$\end{document}pci′′=pci′rmaxEnsure each point cloud has the same number of points M: If N < M, fill the point count with zero vectors to reach M; if N > M, truncate the point cloud to M points. In the experiment, we set M to 2048.
We employed an unsupervised learning model based on Autoencoder called PAE [56] to reconstruct the 3D point cloud representation of proteins, thereby capturing their 3D spatial information, including spatial arrangement, folding patterns, and dynamic conformational changes, as well as geometric details and global structures. PAE utilizes the PointNet encoder to extract local and global features of protein point clouds and integrates a Spatial Transformer Network (STN) for rotational alignment to enhance structural consistency, while optimizing reconstruction accuracy through Chamfer distance to better preserve the geometric structure and topological relationships of proteins.As shown in Fig. 9c, PAE consists of two core components:Encoder: Using PointNet [57] to extract global features from the point cloud, we further map these features to a low-dimensional latent space \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h\in {\mathbb{R}}^{b\times d}$$\end{document}h∈Rb×d via fully connected layers. Here, b and d denote the batch size and the feature dimension of the point cloud, respectively.Decoder: The decoder remaps the k-dimensional representation from the encoder back to the spatial coordinates of the point cloud data.The model uses Chamfer Distance (CD) [58] to compute the square of the Euclidean distance from each point in one point set to the nearest point in the other point set and sums the distances to measure the similarity between the two point clouds, which is shown in Eq. 10:10\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}CD\left({S}_{1},{S}_{2}\right)=\sum_{x\in {S}_{1}} \underset{y\in {S}_{2}}{min} {\parallel x-y \parallel }_{2}^{2}+\sum_{y\in {S}_{2}} \underset{x\in {S}_{1}}{min} {\parallel x-y \parallel }_{2}^{2}\end{array}$$\end{document}CDS1,S2=∑x∈S1miny∈S2‖x-y‖22+∑y∈S2minx∈S1‖x-y‖22Here, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{1}$$\end{document}S1 and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{2}$$\end{document}S2 represent the point sets of the point clouds.Multimodal feature extractionAfter obtaining the 640-dimensional protein sequence features, protein structural features, and three-dimensional spatial features generated by VAE, VGAE, and PAE, we perform z-score normalization on the feature vectors. This eliminates the dimensional influences among different features, allowing the features to be compared on the same scale to achieve balanced effects in multi-modal fusion. As shown in Fig. 10, we also employ AE for unsupervised learning of multi-modal protein features, called Fusion Autoencoder (FAE), with specific steps as follows:Fig. 10The architecture of FAE. FAE performs unsupervised learning by compressing multimodal feature vectors and minimizing reconstruction loss, optimizing the feature fusion process to retain original information(1) Encoder: The encoder compresses the input multi-modal feature vector Z into a low-dimensional latent space, achieving feature fusion and dimensionality reduction, and outputs the latent representation, which is shown in Eq. 11:11\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}H=ReLU\left(\text{Linear}\left(\text{Tanh}\left(\text{Linear}\left(\text{Z}\right)\right)\right)\right)\end{array}$$\end{document}H=ReLULinearTanhLinearZwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, b and d represent the batch size and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$640\times 3$$\end{document}640×3 dimensions, while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\in {\mathbb{R}}^{b\times k}$$\end{document}H∈Rb×k, k is 1024 dimensions.(2) Decoder: The decoder maps the encoded latent representation h back to the original feature space, ensuring that the feature fusion process retains the original information and outputs the reconstructed feature vector, which is shown in Eq. 12:12\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\widehat{Z}=Linear\left(\text{ReLU}\left(\text{Linear}\left(\text{H}\right)\right)\right)\end{array}$$\end{document}Z^=LinearReLULinearHFAE uses mean squared error (MSE) loss as the reconstruction objective, minimizing the difference between the original input Z and the reconstructed output \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{Z}$$\end{document}Z^. MSE loss can be defined as Eq. 13:13\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}{\left({Z}_{i}-{\widehat{Z}}_{i}\right)}^{2}\end{array}$$\end{document}LMSE=1N∑i=1NZi-Z^i2By ensuring the effectiveness of the compressed representations and optimizing the reconstruction quality, this model effectively improves the feature fusion process.

Unsupervised pre-training model for extracting multimodal protein features

Existing PPI prediction methods typically adopt an end-to-end learning strategy [35], where the model is trained directly from protein feature extraction to PPI prediction. Although this approach allows the model to learn directly from the data, end-to-end learning requires simultaneous completion of feature extraction and task optimization, which is often challenging to achieve effectively, especially when data is insufficient. Furthermore, in protein–protein prediction tasks, graph neural network-based models generally require the complete PPI network graph as input, with larger datasets resulting in greater computational burdens [39]. Therefore, we adopt a stepwise approach for extracting protein features using a pre-training method. As shown in Fig. 1a, we adjust the model framework based on the pre-training model MPRL [52] for the protein–protein interaction prediction task to capture more expressive multimodal protein features.

MPRL utilizes the primary and tertiary structures of proteins for unsupervised multimodal protein representation learning. For protein \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}\in P$$\end{document}Pi∈P, its primary structure is represented as its amino acid sequence \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S$$\end{document}S, written as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S=\left({v}_{1},{v}_{2},\cdots ,{v}_{M}\right)$$\end{document}S=v1,v2,⋯,vM, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${v}_{M}$$\end{document}vM denotes the m-th amino acid residue in the sequence, and M is the total number of amino acid residues in the sequence. Each \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${v}_{m}$$\end{document}vm belongs to the amino acid set V which contains 20 amino acid types. The tertiary structure of protein \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi is the structure formed in three-dimensional space by the folding of the amino acid chain \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{i}$$\end{document}Si through non-covalent interactions (such as hydrogen bonds, hydrophobic interactions, etc.), describing the atomic positions and folding states of the protein in three-dimensional space, represented as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${T}_{i}=\left({x}_{1},{y}_{1},{z}_{1};{x}_{2},{x}_{2},{z}_{2};\cdots ;{x}_{M},{y}_{M},{z}_{M}\right)$$\end{document}Ti=x1,y1,z1;x2,x2,z2;⋯;xM,yM,zM, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({x}_{m},{y}_{m},{z}_{m}\right)$$\end{document}xm,ym,zm denotes the spatial coordinates of the m-th amino acid residue. Protein sequence features provide fundamental insights into protein information but are limited in capturing complex spatial and dynamic characteristics, failing to represent spatial conformations. In contrast, constructing a topological network through local connections between residues characterizes the three-dimensional folding state of proteins but struggles to capture long-range spatial correlations. Meanwhile, three-dimensional point cloud spatial features precisely describe the global geometric morphology of proteins through spatial coordinates of residues, enhancing the expression of spatial information but lacking fine-grained representation of residue features. These three types of data provide complementary information at the levels of molecular sequence, structure, and global spatial distribution, addressing the limitations of single data sources in terms of information completeness. Therefore, compared to relying solely on a single protein feature (e.g., sequence features), multimodal features can more effectively enhance protein representation learning and model performance in PPI prediction tasks through complementary advantages among various features.

Sequence feature extractionWe designed a model called Sequence Variational Autoencoder (SVAE) based on Variational Autoencoder (VAE) [53] to extract sequence features. SVAE extracts short-range patterns through 1D convolution and combines bidirectional GRU to capture long-range dependencies across sequences, adapting to the complex structure and functional properties of proteins. Meanwhile, global average pooling compresses variable-length sequences, making them suitable for proteins of different lengths. Leveraging VAE, SVAE maps high-dimensional protein features into a continuous, smooth, and structured latent space, enabling the generation of biologically plausible sequence features while preserving key functional characteristics.As shown in Fig. 9a, SVAE mainly consists of two parts:Fig. 9a The architecture of SVAE. SVAE is used to extract sequence features. It consists of two parts: an encoder and a decoder. The model is jointly optimized through reconstruction loss and KL divergence for unsupervised learning and data generation. b The architecture of VGAE. VGAE utilizes GCN to encode local interactions and spatial relationships in protein structure graphs. It predicts the existence probability of edges through an inner product decoder and optimizes the model by minimizing reconstruction loss for unsupervised learning. c The architecture of PAE. PAE extracts global features of the 3D point clouds of proteins using PointNet. It maps these features to a low-dimensional latent space and calculates the similarity between two point clouds using Chamfer distance for unsupervised learning and 3D point cloud reconstructionEncoder: The encoder extracts sequence data into fixed-dimensional features \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times s\times d}$$\end{document}X∈Rn×s×d through convolutional layers, bidirectional gated recurrent units (BiGRU), and fully connected (FC) layers, where n, s and d represent the number of proteins, sequence length, and amino acid residue feature dimensions, respectively. It outputs mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{log}(\sigma )$$\end{document}log(σ) of the latent space to generate the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, where b and d represent the batch size and sequence feature dimension, respectively.Decoder: Starting from the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z$$\end{document}Z, the decoder generates reconstructed data of the input sequence features through fully connected layers and the nonlinear activation function ReLU.The loss function of SVAE can be defined as Eq. 1:1\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=L_{recon}+L_{KL}$$\end{document}L=Lrecon+LKLwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon represents the reconstruction loss, which measures the error between the model′s reconstructed data and the original data; while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL represents the KL divergence, which constrains the distribution of the latent variable Z to bring q(Z|X) close to the standard normal distribution p(Z) = N(0,1). The reconstruction loss \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon can be expressed as Eq. 2:2\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{recon}=\mathrm{MSE}\;(\mathrm x,\hat{\mathrm x})=\frac1N\sum\limits_{i=1}^N\left(x_{i\;\;}-\;\hat{x}_i\right)^2$$\end{document}Lrecon=MSE(x,x^)=1N∑i=1Nxi-x^i2where x denotes the input original data, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{x}$$\end{document}x^ represents the output reconstructed by the model, and N is the number of samples. The KL divergence \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL can be expressed as Eq. 3:3\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{KL}}=-\frac{1}{2}\left(1+\text{log}\left({\sigma }^{2}\right)-{\mu }^{2}-{\sigma }^{2}\right)\end{array}$$\end{document}LKL=-121+logσ2-μ2-σ2

Sequence feature extraction

We designed a model called Sequence Variational Autoencoder (SVAE) based on Variational Autoencoder (VAE) [53] to extract sequence features. SVAE extracts short-range patterns through 1D convolution and combines bidirectional GRU to capture long-range dependencies across sequences, adapting to the complex structure and functional properties of proteins. Meanwhile, global average pooling compresses variable-length sequences, making them suitable for proteins of different lengths. Leveraging VAE, SVAE maps high-dimensional protein features into a continuous, smooth, and structured latent space, enabling the generation of biologically plausible sequence features while preserving key functional characteristics.

As shown in Fig. 9a, SVAE mainly consists of two parts:Fig. 9a The architecture of SVAE. SVAE is used to extract sequence features. It consists of two parts: an encoder and a decoder. The model is jointly optimized through reconstruction loss and KL divergence for unsupervised learning and data generation. b The architecture of VGAE. VGAE utilizes GCN to encode local interactions and spatial relationships in protein structure graphs. It predicts the existence probability of edges through an inner product decoder and optimizes the model by minimizing reconstruction loss for unsupervised learning. c The architecture of PAE. PAE extracts global features of the 3D point clouds of proteins using PointNet. It maps these features to a low-dimensional latent space and calculates the similarity between two point clouds using Chamfer distance for unsupervised learning and 3D point cloud reconstructionEncoder: The encoder extracts sequence data into fixed-dimensional features \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times s\times d}$$\end{document}X∈Rn×s×d through convolutional layers, bidirectional gated recurrent units (BiGRU), and fully connected (FC) layers, where n, s and d represent the number of proteins, sequence length, and amino acid residue feature dimensions, respectively. It outputs mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{log}(\sigma )$$\end{document}log(σ) of the latent space to generate the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, where b and d represent the batch size and sequence feature dimension, respectively.Decoder: Starting from the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z$$\end{document}Z, the decoder generates reconstructed data of the input sequence features through fully connected layers and the nonlinear activation function ReLU.

a The architecture of SVAE. SVAE is used to extract sequence features. It consists of two parts: an encoder and a decoder. The model is jointly optimized through reconstruction loss and KL divergence for unsupervised learning and data generation. b The architecture of VGAE. VGAE utilizes GCN to encode local interactions and spatial relationships in protein structure graphs. It predicts the existence probability of edges through an inner product decoder and optimizes the model by minimizing reconstruction loss for unsupervised learning. c The architecture of PAE. PAE extracts global features of the 3D point clouds of proteins using PointNet. It maps these features to a low-dimensional latent space and calculates the similarity between two point clouds using Chamfer distance for unsupervised learning and 3D point cloud reconstruction

Encoder: The encoder extracts sequence data into fixed-dimensional features \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times s\times d}$$\end{document}X∈Rn×s×d through convolutional layers, bidirectional gated recurrent units (BiGRU), and fully connected (FC) layers, where n, s and d represent the number of proteins, sequence length, and amino acid residue feature dimensions, respectively. It outputs mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{log}(\sigma )$$\end{document}log(σ) of the latent space to generate the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, where b and d represent the batch size and sequence feature dimension, respectively.

Decoder: Starting from the latent variable \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z$$\end{document}Z, the decoder generates reconstructed data of the input sequence features through fully connected layers and the nonlinear activation function ReLU.

The loss function of SVAE can be defined as Eq. 1:1\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=L_{recon}+L_{KL}$$\end{document}L=Lrecon+LKLwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon represents the reconstruction loss, which measures the error between the model′s reconstructed data and the original data; while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL represents the KL divergence, which constrains the distribution of the latent variable Z to bring q(Z|X) close to the standard normal distribution p(Z) = N(0,1). The reconstruction loss \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{recon}}$$\end{document}Lrecon can be expressed as Eq. 2:2\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{recon}=\mathrm{MSE}\;(\mathrm x,\hat{\mathrm x})=\frac1N\sum\limits_{i=1}^N\left(x_{i\;\;}-\;\hat{x}_i\right)^2$$\end{document}Lrecon=MSE(x,x^)=1N∑i=1Nxi-x^i2where x denotes the input original data, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{x}$$\end{document}x^ represents the output reconstructed by the model, and N is the number of samples. The KL divergence \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{\text{KL}}$$\end{document}LKL can be expressed as Eq. 3:3\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{KL}}=-\frac{1}{2}\left(1+\text{log}\left({\sigma }^{2}\right)-{\mu }^{2}-{\sigma }^{2}\right)\end{array}$$\end{document}LKL=-121+logσ2-μ2-σ2

Structure feature extractionTo construct the protein structure graph, we use one-hot encoding to specify a specific amino acid type for each residue and employ the K-nearest neighbors (KNN) strategy to build edges based on the coordinates of the alpha carbon atoms of the amino acid residues within the protein.In this process, we calculate the Euclidean distance between each node and other nodes, connecting it to the nearest five neighbors to construct the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}=\left(S,{E}_{r}\right)$$\end{document}Gr=S,Er, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{r}=\{({R}_{i},{R}_{j})|d({R}_{i},{R}_{j})\le d,{R}_{i},{R}_{j}\in R,i\ne j\}$$\end{document}Er={(Ri,Rj)|d(Ri,Rj)≤d,Ri,Rj∈R,i≠j} denotes the set of edges where the Euclidean distance between the alpha carbon atoms of residues \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{i}$$\end{document}Ri and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{j}$$\end{document}Rj is less than the K-nearest neighbors threshold d.To learn and capture the local interactions and spatial proximity within the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}$$\end{document}Gr, we utilize a model based on Variational Autoencoder (VAE) for unsupervised learning of graph-structured data, referred to as Variational Graph Autoencoder (VGAE) [54]. VGAE utilizes GCN [55] to extract structural features of protein graphs, encoding amino acid residues and their interactions into latent representations, and models uncertainty through variational inference to adapt to the diversity of protein structures. Its self-supervised learning effectively reconstructs protein graphs, and combined with negative sampling, enhances the discrimination of true connections, improving generalization capabilities.As shown in Fig. 9b, VGAE primarily consists of two parts:GCN Encoder: The GCN layers encode the input graph data, mapping the structural information of the graph into latent space, and output mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$log(\sigma )$$\end{document}log(σ) in the latent space. Through reparameterization, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}σ are used to obtain the latent representation \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, Where b and d are the product of the batch size and the number of amino acid residues in the protein and the dimension of the one-hot encoding, respectivelyInner Product Decoder: The decoder is responsible for predicting the probability of edge existence from the node representations Z in the latent space, which was shown in Eq. 4:4\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{A}}_{ij}=Sig({Z}_{i}{Z}_{j})\end{array}$$\end{document}A^ij=Sig(ZiZj)where Sig is the Sigmoid activation function, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\widehat{A}}_{ij}$$\end{document}A^ij represents the predicted probability of the edge. \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{i}$$\end{document}Zi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{j}$$\end{document}Zj represent the representation vectors of nodes i and j in the latent space.The training objective of the model is to minimize the reconstruction loss, which measures the alignment between the predicted edges (connection probabilities) and the actual edges. The reconstruction loss is calculated from the given positive edges (edges that exist, with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Sig}({Z}_{i}{Z}_{j})$$\end{document}Sig(ZiZj)) and negative edges (nonexistent edges, generated through negative sampling with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)$$\end{document}1-SigZiZj). It is assessed using binary cross-entropy (BCE) to determine the presence or absence of edges, which is defined as Eq. 5:5\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}L=-\frac{1}{\left|{\mathcal{E}}^{+}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{+}}\text{logSig}\left({Z}_{i}{Z}_{j}\right) -\frac{1}{\left|{\mathcal{E}}^{-}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{-}}\text{log}\left(1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)\right)\end{array}$$\end{document}L=-1E+∑i,j∈E+logSigZiZj-1E-∑i,j∈E-log1-SigZiZjwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{+}$$\end{document}E+ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{-}$$\end{document}E- represent the sets of positive and negative edges respectively.

Structure feature extraction

To construct the protein structure graph, we use one-hot encoding to specify a specific amino acid type for each residue and employ the K-nearest neighbors (KNN) strategy to build edges based on the coordinates of the alpha carbon atoms of the amino acid residues within the protein.

In this process, we calculate the Euclidean distance between each node and other nodes, connecting it to the nearest five neighbors to construct the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}=\left(S,{E}_{r}\right)$$\end{document}Gr=S,Er, where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{r}=\{({R}_{i},{R}_{j})|d({R}_{i},{R}_{j})\le d,{R}_{i},{R}_{j}\in R,i\ne j\}$$\end{document}Er={(Ri,Rj)|d(Ri,Rj)≤d,Ri,Rj∈R,i≠j} denotes the set of edges where the Euclidean distance between the alpha carbon atoms of residues \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{i}$$\end{document}Ri and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{j}$$\end{document}Rj is less than the K-nearest neighbors threshold d.

To learn and capture the local interactions and spatial proximity within the protein structure graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{r}$$\end{document}Gr, we utilize a model based on Variational Autoencoder (VAE) for unsupervised learning of graph-structured data, referred to as Variational Graph Autoencoder (VGAE) [54]. VGAE utilizes GCN [55] to extract structural features of protein graphs, encoding amino acid residues and their interactions into latent representations, and models uncertainty through variational inference to adapt to the diversity of protein structures. Its self-supervised learning effectively reconstructs protein graphs, and combined with negative sampling, enhances the discrimination of true connections, improving generalization capabilities.

As shown in Fig. 9b, VGAE primarily consists of two parts:GCN Encoder: The GCN layers encode the input graph data, mapping the structural information of the graph into latent space, and output mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$log(\sigma )$$\end{document}log(σ) in the latent space. Through reparameterization, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}σ are used to obtain the latent representation \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, Where b and d are the product of the batch size and the number of amino acid residues in the protein and the dimension of the one-hot encoding, respectivelyInner Product Decoder: The decoder is responsible for predicting the probability of edge existence from the node representations Z in the latent space, which was shown in Eq. 4:4\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{A}}_{ij}=Sig({Z}_{i}{Z}_{j})\end{array}$$\end{document}A^ij=Sig(ZiZj)where Sig is the Sigmoid activation function, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\widehat{A}}_{ij}$$\end{document}A^ij represents the predicted probability of the edge. \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{i}$$\end{document}Zi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${Z}_{j}$$\end{document}Zj represent the representation vectors of nodes i and j in the latent space.

GCN Encoder: The GCN layers encode the input graph data, mapping the structural information of the graph into latent space, and output mean \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and log-variance \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$log(\sigma )$$\end{document}log(σ) in the latent space. Through reparameterization, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu$$\end{document}μ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}σ are used to obtain the latent representation \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, Where b and d are the product of the batch size and the number of amino acid residues in the protein and the dimension of the one-hot encoding, respectively

Inner Product Decoder: The decoder is responsible for predicting the probability of edge existence from the node representations Z in the latent space, which was shown in Eq. 4:4\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{A}}_{ij}=Sig({Z}_{i}{Z}_{j})\end{array}$$\end{document}A^ij=Sig(ZiZj)

The training objective of the model is to minimize the reconstruction loss, which measures the alignment between the predicted edges (connection probabilities) and the actual edges. The reconstruction loss is calculated from the given positive edges (edges that exist, with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Sig}({Z}_{i}{Z}_{j})$$\end{document}Sig(ZiZj)) and negative edges (nonexistent edges, generated through negative sampling with loss calculated by \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)$$\end{document}1-SigZiZj). It is assessed using binary cross-entropy (BCE) to determine the presence or absence of edges, which is defined as Eq. 5:5\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}L=-\frac{1}{\left|{\mathcal{E}}^{+}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{+}}\text{logSig}\left({Z}_{i}{Z}_{j}\right) -\frac{1}{\left|{\mathcal{E}}^{-}\right|}{\sum }_{\left(i,j\right)\in {\mathcal{E}}^{-}}\text{log}\left(1-\text{Sig}\left({Z}_{i}{Z}_{j}\right)\right)\end{array}$$\end{document}L=-1E+∑i,j∈E+logSigZiZj-1E-∑i,j∈E-log1-SigZiZjwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{+}$$\end{document}E+ and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{E}}^{-}$$\end{document}E- represent the sets of positive and negative edges respectively.

Point cloud feature extractionSuppose the protein consists of N atoms; then the point cloud of the protein can be defined as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{PC}={\{\text{pc}}_{\text{i}}\left|{\text{pc}}_{\text{i}}=\left({\text{x}}_{\text{i}}, {\text{y}}_{\text{i}}, {\text{z}}_{\text{i}}\right), \text{i}=1, 2, \cdots , \text{N}\right\}$$\end{document}PC={pcipci=xi,yi,zi,i=1,2,⋯,N. To ensure that all point clouds have a consistent spatial distribution and size, standardization operations such as centering, scaling, and fixed point counts are performed on the point cloud data. The point cloud data is represented as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times d}$$\end{document}X∈Rn×d, where n and d represent the three spatial dimensions and the number of points in each spatial dimension, respectively. The specific steps are as follows:
Calculate the centroid of the point cloud by Eq. 6:6\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}c=\frac{1}{N}\sum_{i=1}^{N}{pc}_{i}\end{array}$$\end{document}c=1N∑i=1NpciTranslate the point cloud to the origin by Eq. 7:7\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}{\prime}={pc}_{i}-c\end{array}$$\end{document}pci′=pci-cCalculate the maximum norm of the point cloud by Eq. 8:8\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{r}_{\text{max}}=\underset{i}{\text{max}}\parallel {pc}_{i}{\prime}\parallel \end{array}$$\end{document}rmax=maxi‖pci′‖Scale the point cloud to fit within the unit sphere by Eq. 9:9\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}^{{\prime}{\prime}}=\frac{{pc}_{i}{\prime}}{{r}_{\text{max}}}\end{array}$$\end{document}pci′′=pci′rmaxEnsure each point cloud has the same number of points M: If N < M, fill the point count with zero vectors to reach M; if N > M, truncate the point cloud to M points. In the experiment, we set M to 2048.
We employed an unsupervised learning model based on Autoencoder called PAE [56] to reconstruct the 3D point cloud representation of proteins, thereby capturing their 3D spatial information, including spatial arrangement, folding patterns, and dynamic conformational changes, as well as geometric details and global structures. PAE utilizes the PointNet encoder to extract local and global features of protein point clouds and integrates a Spatial Transformer Network (STN) for rotational alignment to enhance structural consistency, while optimizing reconstruction accuracy through Chamfer distance to better preserve the geometric structure and topological relationships of proteins.As shown in Fig. 9c, PAE consists of two core components:Encoder: Using PointNet [57] to extract global features from the point cloud, we further map these features to a low-dimensional latent space \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h\in {\mathbb{R}}^{b\times d}$$\end{document}h∈Rb×d via fully connected layers. Here, b and d denote the batch size and the feature dimension of the point cloud, respectively.Decoder: The decoder remaps the k-dimensional representation from the encoder back to the spatial coordinates of the point cloud data.The model uses Chamfer Distance (CD) [58] to compute the square of the Euclidean distance from each point in one point set to the nearest point in the other point set and sums the distances to measure the similarity between the two point clouds, which is shown in Eq. 10:10\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}CD\left({S}_{1},{S}_{2}\right)=\sum_{x\in {S}_{1}} \underset{y\in {S}_{2}}{min} {\parallel x-y \parallel }_{2}^{2}+\sum_{y\in {S}_{2}} \underset{x\in {S}_{1}}{min} {\parallel x-y \parallel }_{2}^{2}\end{array}$$\end{document}CDS1,S2=∑x∈S1miny∈S2‖x-y‖22+∑y∈S2minx∈S1‖x-y‖22Here, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{1}$$\end{document}S1 and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{2}$$\end{document}S2 represent the point sets of the point clouds.

Point cloud feature extraction

Suppose the protein consists of N atoms; then the point cloud of the protein can be defined as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{PC}={\{\text{pc}}_{\text{i}}\left|{\text{pc}}_{\text{i}}=\left({\text{x}}_{\text{i}}, {\text{y}}_{\text{i}}, {\text{z}}_{\text{i}}\right), \text{i}=1, 2, \cdots , \text{N}\right\}$$\end{document}PC={pcipci=xi,yi,zi,i=1,2,⋯,N. To ensure that all point clouds have a consistent spatial distribution and size, standardization operations such as centering, scaling, and fixed point counts are performed on the point cloud data. The point cloud data is represented as \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X\in {\mathbb{R}}^{n\times d}$$\end{document}X∈Rn×d, where n and d represent the three spatial dimensions and the number of points in each spatial dimension, respectively. The specific steps are as follows:

Calculate the centroid of the point cloud by Eq. 6:6\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}c=\frac{1}{N}\sum_{i=1}^{N}{pc}_{i}\end{array}$$\end{document}c=1N∑i=1NpciTranslate the point cloud to the origin by Eq. 7:7\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}{\prime}={pc}_{i}-c\end{array}$$\end{document}pci′=pci-cCalculate the maximum norm of the point cloud by Eq. 8:8\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{r}_{\text{max}}=\underset{i}{\text{max}}\parallel {pc}_{i}{\prime}\parallel \end{array}$$\end{document}rmax=maxi‖pci′‖Scale the point cloud to fit within the unit sphere by Eq. 9:9\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}^{{\prime}{\prime}}=\frac{{pc}_{i}{\prime}}{{r}_{\text{max}}}\end{array}$$\end{document}pci′′=pci′rmaxEnsure each point cloud has the same number of points M: If N < M, fill the point count with zero vectors to reach M; if N > M, truncate the point cloud to M points. In the experiment, we set M to 2048.

Calculate the centroid of the point cloud by Eq. 6:6\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}c=\frac{1}{N}\sum_{i=1}^{N}{pc}_{i}\end{array}$$\end{document}c=1N∑i=1Npci

Translate the point cloud to the origin by Eq. 7:7\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}{\prime}={pc}_{i}-c\end{array}$$\end{document}pci′=pci-c

Calculate the maximum norm of the point cloud by Eq. 8:8\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{r}_{\text{max}}=\underset{i}{\text{max}}\parallel {pc}_{i}{\prime}\parallel \end{array}$$\end{document}rmax=maxi‖pci′‖

Scale the point cloud to fit within the unit sphere by Eq. 9:9\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{pc}_{i}^{{\prime}{\prime}}=\frac{{pc}_{i}{\prime}}{{r}_{\text{max}}}\end{array}$$\end{document}pci′′=pci′rmax

Ensure each point cloud has the same number of points M: If N < M, fill the point count with zero vectors to reach M; if N > M, truncate the point cloud to M points. In the experiment, we set M to 2048.

We employed an unsupervised learning model based on Autoencoder called PAE [56] to reconstruct the 3D point cloud representation of proteins, thereby capturing their 3D spatial information, including spatial arrangement, folding patterns, and dynamic conformational changes, as well as geometric details and global structures. PAE utilizes the PointNet encoder to extract local and global features of protein point clouds and integrates a Spatial Transformer Network (STN) for rotational alignment to enhance structural consistency, while optimizing reconstruction accuracy through Chamfer distance to better preserve the geometric structure and topological relationships of proteins.

As shown in Fig. 9c, PAE consists of two core components:Encoder: Using PointNet [57] to extract global features from the point cloud, we further map these features to a low-dimensional latent space \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h\in {\mathbb{R}}^{b\times d}$$\end{document}h∈Rb×d via fully connected layers. Here, b and d denote the batch size and the feature dimension of the point cloud, respectively.Decoder: The decoder remaps the k-dimensional representation from the encoder back to the spatial coordinates of the point cloud data.

Encoder: Using PointNet [57] to extract global features from the point cloud, we further map these features to a low-dimensional latent space \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$h\in {\mathbb{R}}^{b\times d}$$\end{document}h∈Rb×d via fully connected layers. Here, b and d denote the batch size and the feature dimension of the point cloud, respectively.

Decoder: The decoder remaps the k-dimensional representation from the encoder back to the spatial coordinates of the point cloud data.

The model uses Chamfer Distance (CD) [58] to compute the square of the Euclidean distance from each point in one point set to the nearest point in the other point set and sums the distances to measure the similarity between the two point clouds, which is shown in Eq. 10:10\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}CD\left({S}_{1},{S}_{2}\right)=\sum_{x\in {S}_{1}} \underset{y\in {S}_{2}}{min} {\parallel x-y \parallel }_{2}^{2}+\sum_{y\in {S}_{2}} \underset{x\in {S}_{1}}{min} {\parallel x-y \parallel }_{2}^{2}\end{array}$$\end{document}CDS1,S2=∑x∈S1miny∈S2‖x-y‖22+∑y∈S2minx∈S1‖x-y‖22

Here, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{1}$$\end{document}S1 and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S}_{2}$$\end{document}S2 represent the point sets of the point clouds.

Multimodal feature extractionAfter obtaining the 640-dimensional protein sequence features, protein structural features, and three-dimensional spatial features generated by VAE, VGAE, and PAE, we perform z-score normalization on the feature vectors. This eliminates the dimensional influences among different features, allowing the features to be compared on the same scale to achieve balanced effects in multi-modal fusion. As shown in Fig. 10, we also employ AE for unsupervised learning of multi-modal protein features, called Fusion Autoencoder (FAE), with specific steps as follows:Fig. 10The architecture of FAE. FAE performs unsupervised learning by compressing multimodal feature vectors and minimizing reconstruction loss, optimizing the feature fusion process to retain original information(1) Encoder: The encoder compresses the input multi-modal feature vector Z into a low-dimensional latent space, achieving feature fusion and dimensionality reduction, and outputs the latent representation, which is shown in Eq. 11:11\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}H=ReLU\left(\text{Linear}\left(\text{Tanh}\left(\text{Linear}\left(\text{Z}\right)\right)\right)\right)\end{array}$$\end{document}H=ReLULinearTanhLinearZwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, b and d represent the batch size and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$640\times 3$$\end{document}640×3 dimensions, while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\in {\mathbb{R}}^{b\times k}$$\end{document}H∈Rb×k, k is 1024 dimensions.(2) Decoder: The decoder maps the encoded latent representation h back to the original feature space, ensuring that the feature fusion process retains the original information and outputs the reconstructed feature vector, which is shown in Eq. 12:12\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\widehat{Z}=Linear\left(\text{ReLU}\left(\text{Linear}\left(\text{H}\right)\right)\right)\end{array}$$\end{document}Z^=LinearReLULinearHFAE uses mean squared error (MSE) loss as the reconstruction objective, minimizing the difference between the original input Z and the reconstructed output \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{Z}$$\end{document}Z^. MSE loss can be defined as Eq. 13:13\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}{\left({Z}_{i}-{\widehat{Z}}_{i}\right)}^{2}\end{array}$$\end{document}LMSE=1N∑i=1NZi-Z^i2By ensuring the effectiveness of the compressed representations and optimizing the reconstruction quality, this model effectively improves the feature fusion process.

Multimodal feature extraction

After obtaining the 640-dimensional protein sequence features, protein structural features, and three-dimensional spatial features generated by VAE, VGAE, and PAE, we perform z-score normalization on the feature vectors. This eliminates the dimensional influences among different features, allowing the features to be compared on the same scale to achieve balanced effects in multi-modal fusion. As shown in Fig. 10, we also employ AE for unsupervised learning of multi-modal protein features, called Fusion Autoencoder (FAE), with specific steps as follows:Fig. 10The architecture of FAE. FAE performs unsupervised learning by compressing multimodal feature vectors and minimizing reconstruction loss, optimizing the feature fusion process to retain original information

The architecture of FAE. FAE performs unsupervised learning by compressing multimodal feature vectors and minimizing reconstruction loss, optimizing the feature fusion process to retain original information

(1) Encoder: The encoder compresses the input multi-modal feature vector Z into a low-dimensional latent space, achieving feature fusion and dimensionality reduction, and outputs the latent representation, which is shown in Eq. 11:11\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}H=ReLU\left(\text{Linear}\left(\text{Tanh}\left(\text{Linear}\left(\text{Z}\right)\right)\right)\right)\end{array}$$\end{document}H=ReLULinearTanhLinearZwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z\in {\mathbb{R}}^{b\times d}$$\end{document}Z∈Rb×d, b and d represent the batch size and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$640\times 3$$\end{document}640×3 dimensions, while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\in {\mathbb{R}}^{b\times k}$$\end{document}H∈Rb×k, k is 1024 dimensions.

(2) Decoder: The decoder maps the encoded latent representation h back to the original feature space, ensuring that the feature fusion process retains the original information and outputs the reconstructed feature vector, which is shown in Eq. 12:12\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\widehat{Z}=Linear\left(\text{ReLU}\left(\text{Linear}\left(\text{H}\right)\right)\right)\end{array}$$\end{document}Z^=LinearReLULinearH

FAE uses mean squared error (MSE) loss as the reconstruction objective, minimizing the difference between the original input Z and the reconstructed output \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\widehat{Z}$$\end{document}Z^. MSE loss can be defined as Eq. 13:13\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{L}_{\text{MSE}}=\frac{1}{N}\sum_{i=1}^{N}{\left({Z}_{i}-{\widehat{Z}}_{i}\right)}^{2}\end{array}$$\end{document}LMSE=1N∑i=1NZi-Z^i2

By ensuring the effectiveness of the compressed representations and optimizing the reconstruction quality, this model effectively improves the feature fusion process.

GraphGPSGraph position encoding [59, 60] aims to provide effective embedding representations for graphs or their subgraphs, enhancing nodes’ understanding of their positions within the overall graph. This encoding method enhances the model’s representational capability by capturing the relationships and structural features among nodes, particularly when handling complex graph data. When two nodes share similar subgraphs or structures, their structural encodings should remain close, which is crucial for many graph neural network tasks. In particular, local structure encoding (Local SE) [61] allows each node to understand its surrounding substructure. This encoding method not only facilitates the transmission of information but also effectively captures the m-hop subgraph features around the nodes. Specifically, local structure encoding enhances nodes’ perception of their surroundings by identifying features related to the nodes, such as node degrees, diagonal elements of the random walk matrix, and predefined substructural statistics. With a structural encoding of radius m, the more similar the m-hop subgraphs between two nodes, the closer their local structure encodings will be. This proximity not only reflects the similarity between nodes but also provides a rich informational foundation for downstream networks. Meanwhile, the design of local structure encoding effectively suppresses the effects of noise and redundant information, thereby enhancing the robustness and performance of the model. We generated local structure encodings according to LSPE [61]. This structural encoding computation effectively integrates the information of node i and its neighboring nodes, allowing for an expression of the node’s local features. These local features play a crucial role in graph learning, helping the model understand the position of the node within the overall structure.When the paths between nodes are long, information may encounter compression during transmission, leading to loss of information. Additionally, the local attention mechanism aims to achieve global information interaction between nodes by directly connecting them, effectively alleviating the issue of information compression. As shown in Fig. 11, GraphGPS [59] combines local GCN and global attention mechanisms [62], enhancing GCN’s performance in processing graph data by introducing local structure encoding and global feature capture.Fig. 11The architecture of GraphGPS. GraphGPS combines local graph convolutional networks with global attention mechanisms to alleviate the information compressing problem by directly connecting nodes. This effectively aggregates local and global features, enhancing the performance of graph data processing and strengthening the model’s understanding and learning ability of graph structuresThe overall computation formula of GraphGPS can be defined as Eq. 14:14\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{X}}^{{\ell}+1}=GP{\text{S}}^{{\ell}}\left({\text{X}}^{{\ell}},\text{A}\right)\end{array}$$\end{document}Xℓ+1=GPSℓXℓ,Awhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{X}}^l$$\end{document}Xl represents the node feature matrix at layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}l, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {A}$$\end{document}A denotes the adjacency matrix. The specific computation process is as follows:(1) The local graph convolutional network effectively aggregates feature information from each node and its neighbors to update the node’s feature representation, which is shown in Eq. 15:15\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}_{M}^{{\ell}+1}={\text{GCN}}^{{\ell}}\left({\text{X}}^{{\ell}},\text{A}\right)\end{array}$$\end{document}HMℓ+1=GCNℓXℓ,A(2) Use the global attention mechanism to aggregate node features, allowing the model to consider the features of all nodes comprehensively, thus capturing global information, which is shown in Eq. 16:16\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}_{T}^{{\ell}+1}={\text{GlobalAttn}}^{{\ell}}\left({\text{H}}^{{\ell}}\right)\end{array}$$\end{document}HTℓ+1=GlobalAttnℓHℓ(3) Combine local features and global features through MLP to generate the final node feature representation, which is shown in Eq. 17:17\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}^{{\ell}+1}={\text{MLP}}^{{\ell}}\left({\text{H}}_{M}^{{\ell}+1}+{\text{H}}_{T}^{{\ell}+1}\right)\end{array}$$\end{document}Hℓ+1=MLPℓHMℓ+1+HTℓ+1This approach helps GraphGPS understand the overall structure of the graph at a higher level during information propagation and feature learning, better combining local neighborhood information with global context and enhancing the model’s learning capacity for graph data.GAT [63] assigns dynamic weights to nodes by aggregating node information, further refining the global and local features output by GraphGPS and extracting more meaningful features from local structures to capture finer-grained interactions and relationships.First, for node i and one of its neighboring nodes j, apply the shared weight matrix W, as shown in Eq. 18:18\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm h'_{\mathrm i\;}\;={\mathrm{Wx}}_{\mathrm i},\;\mathrm h'_{\mathrm j}\;={\mathrm{Wx}}_{\mathrm j}$$\end{document}hi′=Wxi,hj′=WxjNext, calculate the raw attention score for nodes i and j, which can be defined as Eq. 19:
19\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{e_{ij}}=\mathrm{LeakyReLU}\;\left(\mathrm a^\top\cdot\left[h'_i+h'_j\right]\right)$$\end{document}eij=LeakyReLUa⊤·hi′+hj′
where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{a}$$\end{document}a is a learnable parameter vector, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{i}'+{\text{h}}_{j}'$$\end{document}hi′+hj′ represents the sum of the two features, while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\cdot )$$\end{document}(·) denotes the dot product operation.The raw attention coefficients are normalized over all neighboring nodes of node i using the softmax function to obtain the normalized attention coefficients \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}$$\end{document}αij, as shown in Eq. 20:20\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\alpha }_{ij}=\frac{\text{exp}\left({e}_{ij}\right)}{\sum_{k\in \mathcal{N}\left(i\right)\cup i} \text{exp}\left({e}_{ik}\right)}\end{array}$$\end{document}αij=expeij∑k∈Ni∪iexpeikFinally, perform weighted aggregation using \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}$$\end{document}αij and the transformed features of its neighboring nodes to obtain the output feature of node i, as shown in Eq. 21:21\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{x}'_{i}\;={\sum_{\text{j}\in N(i)\cup i\;}} \alpha_{ij}h'_{\mathrm j}$$\end{document}xi′=∑j∈N(i)∪iαijhj′

Graph position encoding [59, 60] aims to provide effective embedding representations for graphs or their subgraphs, enhancing nodes’ understanding of their positions within the overall graph. This encoding method enhances the model’s representational capability by capturing the relationships and structural features among nodes, particularly when handling complex graph data. When two nodes share similar subgraphs or structures, their structural encodings should remain close, which is crucial for many graph neural network tasks. In particular, local structure encoding (Local SE) [61] allows each node to understand its surrounding substructure. This encoding method not only facilitates the transmission of information but also effectively captures the m-hop subgraph features around the nodes. Specifically, local structure encoding enhances nodes’ perception of their surroundings by identifying features related to the nodes, such as node degrees, diagonal elements of the random walk matrix, and predefined substructural statistics. With a structural encoding of radius m, the more similar the m-hop subgraphs between two nodes, the closer their local structure encodings will be. This proximity not only reflects the similarity between nodes but also provides a rich informational foundation for downstream networks. Meanwhile, the design of local structure encoding effectively suppresses the effects of noise and redundant information, thereby enhancing the robustness and performance of the model. We generated local structure encodings according to LSPE [61]. This structural encoding computation effectively integrates the information of node i and its neighboring nodes, allowing for an expression of the node’s local features. These local features play a crucial role in graph learning, helping the model understand the position of the node within the overall structure.

When the paths between nodes are long, information may encounter compression during transmission, leading to loss of information. Additionally, the local attention mechanism aims to achieve global information interaction between nodes by directly connecting them, effectively alleviating the issue of information compression. As shown in Fig. 11, GraphGPS [59] combines local GCN and global attention mechanisms [62], enhancing GCN’s performance in processing graph data by introducing local structure encoding and global feature capture.Fig. 11The architecture of GraphGPS. GraphGPS combines local graph convolutional networks with global attention mechanisms to alleviate the information compressing problem by directly connecting nodes. This effectively aggregates local and global features, enhancing the performance of graph data processing and strengthening the model’s understanding and learning ability of graph structures

The architecture of GraphGPS. GraphGPS combines local graph convolutional networks with global attention mechanisms to alleviate the information compressing problem by directly connecting nodes. This effectively aggregates local and global features, enhancing the performance of graph data processing and strengthening the model’s understanding and learning ability of graph structures

The overall computation formula of GraphGPS can be defined as Eq. 14:14\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{X}}^{{\ell}+1}=GP{\text{S}}^{{\ell}}\left({\text{X}}^{{\ell}},\text{A}\right)\end{array}$$\end{document}Xℓ+1=GPSℓXℓ,Awhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{X}}^l$$\end{document}Xl represents the node feature matrix at layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}l, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm {A}$$\end{document}A denotes the adjacency matrix. The specific computation process is as follows:

(1) The local graph convolutional network effectively aggregates feature information from each node and its neighbors to update the node’s feature representation, which is shown in Eq. 15:15\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}_{M}^{{\ell}+1}={\text{GCN}}^{{\ell}}\left({\text{X}}^{{\ell}},\text{A}\right)\end{array}$$\end{document}HMℓ+1=GCNℓXℓ,A

(2) Use the global attention mechanism to aggregate node features, allowing the model to consider the features of all nodes comprehensively, thus capturing global information, which is shown in Eq. 16:16\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}_{T}^{{\ell}+1}={\text{GlobalAttn}}^{{\ell}}\left({\text{H}}^{{\ell}}\right)\end{array}$$\end{document}HTℓ+1=GlobalAttnℓHℓ

(3) Combine local features and global features through MLP to generate the final node feature representation, which is shown in Eq. 17:17\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{H}}^{{\ell}+1}={\text{MLP}}^{{\ell}}\left({\text{H}}_{M}^{{\ell}+1}+{\text{H}}_{T}^{{\ell}+1}\right)\end{array}$$\end{document}Hℓ+1=MLPℓHMℓ+1+HTℓ+1

This approach helps GraphGPS understand the overall structure of the graph at a higher level during information propagation and feature learning, better combining local neighborhood information with global context and enhancing the model’s learning capacity for graph data.

GAT [63] assigns dynamic weights to nodes by aggregating node information, further refining the global and local features output by GraphGPS and extracting more meaningful features from local structures to capture finer-grained interactions and relationships.

First, for node i and one of its neighboring nodes j, apply the shared weight matrix W, as shown in Eq. 18:18\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm h'_{\mathrm i\;}\;={\mathrm{Wx}}_{\mathrm i},\;\mathrm h'_{\mathrm j}\;={\mathrm{Wx}}_{\mathrm j}$$\end{document}hi′=Wxi,hj′=Wxj

Next, calculate the raw attention score for nodes i and j, which can be defined as Eq. 19:

19\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{e_{ij}}=\mathrm{LeakyReLU}\;\left(\mathrm a^\top\cdot\left[h'_i+h'_j\right]\right)$$\end{document}eij=LeakyReLUa⊤·hi′+hj′

where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{a}$$\end{document}a is a learnable parameter vector, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{i}'+{\text{h}}_{j}'$$\end{document}hi′+hj′ represents the sum of the two features, while \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(\cdot )$$\end{document}(·) denotes the dot product operation.

The raw attention coefficients are normalized over all neighboring nodes of node i using the softmax function to obtain the normalized attention coefficients \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}$$\end{document}αij, as shown in Eq. 20:20\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\alpha }_{ij}=\frac{\text{exp}\left({e}_{ij}\right)}{\sum_{k\in \mathcal{N}\left(i\right)\cup i} \text{exp}\left({e}_{ik}\right)}\end{array}$$\end{document}αij=expeij∑k∈Ni∪iexpeik

Finally, perform weighted aggregation using \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\alpha }_{ij}$$\end{document}αij and the transformed features of its neighboring nodes to obtain the output feature of node i, as shown in Eq. 21:21\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathrm{x}'_{i}\;={\sum_{\text{j}\in N(i)\cup i\;}} \alpha_{ij}h'_{\mathrm j}$$\end{document}xi′=∑j∈N(i)∪iαijhj′

SubgraphGCNMessage Passing Neural Networks (MPNNs) [64], as one of the classic implementations of Graph Neural Networks (GNNs), has been widely applied to protein–protein interaction prediction tasks and has demonstrated exceptional performance. In layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}+1$$\end{document}ℓ+1 the feature of node v is calculated as Eq.  22:22\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}h_v^{\left(\ell+1\right)}=\gamma\left(h_v^{\left(\ell\right)},\text{AGGREGATE}\left(\{\phi\left(h_j^{\left(\ell\right)},e_{vj}\right)\mid j\in\mathcal N\left(v\right)\}\right)\right)\end{array}$$\end{document}hvℓ+1=γhvℓ,AGGREGATE{ϕhjℓ,evj∣j∈Nv}Here, denotes the message generation process; AGGREGATE indicates the function used to aggregate messages from neighboring nodes (e.g., sum, mean, or max), and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}γ is the function that updates the node features. The choice of different message functions, aggregation functions, and update functions affects the expressive power and performance of MPNNs. When all three functions are injective (one-to-one mappings), MPNNs achieve the same expressive power as the 1-Weisfeiler-Lehman (1-WL) isomorphism test [36].During the computation of MPNN, each node follows a star pattern during message aggregation, where the central node of the graph is connected to all its neighbors and aggregates messages solely from its direct neighbors. The star pattern around the central node v is defined as Eq. 23:23\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Star\left(v\right)=\left({\mathcal{N}}_{1}\left(v\right),\{\left(v,j\right)\in \mathcal{E}|j\in \mathcal{N}\left(v\right)\}\right)\end{array}$$\end{document}Starv=N1v,{v,j∈E|j∈Nv}where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{N}}_{1}\left(v\right)$$\end{document}N1v signifies the subgraph centered at node v, including v and all nodes and edges reachable within one hop. When handling non-isomorphic regular graphs, MPNN may struggle to differentiate these graphs, weakening its discriminative ability. To address this issue, Subgraph-1-WL extends the 1-WL algorithm by generalizing the concept of the star graph Star(v) to the k-hop neighborhood \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G\left[{N}_{k}\left(v\right)\right]$$\end{document}GNkv centered at v, thereby improving the expressive power for graph isomorphism detection. SubgraphGCN [65] combines Subgraph-1-WL and GCN, with its internal node feature update process defined as Eq. 24:24\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{h}_{v}^{\left({\ell}+1\right)}={\text{GCN}}^{\left({\ell}\right)}\left(Su{b}^{\left({\ell}\right)}\left[v\right]\right),\hspace{1em}l=0,\dots ,L-1\end{array}$$\end{document}hvℓ+1=GCNℓSubℓv,l=0,⋯,L-1Here, indicates the one-hop subgraph centered at node v. The subgraph generation process primarily includes the following steps: First, based on the edge indices and the number of nodes in the original graph, a 1-hop neighborhood expansion is used to generate local subgraphs for each node. Simultaneously, a Boolean mask matrix is generated to indicate which nodes belong to each subgraph, and the hop counts between nodes are recorded. Next, the dense mask is converted into a sparse format for node and edge indices. Finally, the edges of all subgraphs are merged into a global edge index, while node IDs are remapped to avoid overlaps. After obtaining all subgraphs corresponding to the PPI network graph, SubgraphGCN extracts node features from the global features, integrating the central node features, mean-pooled topological features within the subgraph, and globally aggregated contextual features across subgraphs, resulting in richer and more comprehensive feature representations.As shown in Fig. 12, SubgraphGCN computes three types of encodings:Fig. 12The architecture of SubgraphGCN. SubgraphGCN decomposes large graphs into smaller subgraphs by combining centroid encoding, subgraph encoding, and context encoding, utilizing the 1-hop egonet aggregation method. This enhances the model’s ability to recognize node connection patterns and improves computational efficiency(1) Centroid encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|Centroid}$$\end{document}hv(ℓ+1)|Centroid focuses on representing the features of the central node within the subgraph. It efficiently encodes the local characteristics of node v by embedding within the node's star-shaped subgraph. This method effectively integrates the information from direct neighbors, enhancing the self-representation of the node, defined as Eq. 25:25\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\text{|}Centroid}=\;\mathrm{Emb}\left(v\mid\text{Sub}^{\left(\ell+1\right)}\left[v\right]\right)\end{array}$$\end{document}hv(ℓ+1)|Centroid=Embv∣Subℓ+1v(2) The subgraph encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|\text{Subgraph}}$$\end{document}hv(ℓ+1)|Subgraph aggregates features from the subgraph surrounding node v to form a comprehensive node representation. By applying the mean pooling operation, the subgraph encoding integrates rich information from neighboring nodes, thus providing an in-depth characterization of complex relationships between nodes, defined as Eq. 26:26\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\vert \text{Subgraph}}=\text{GCN}^{\left(\ell\right)}\left(\text{Sub}^{\left(\ell\right)}\left[v\right]\right)=MEAN\left(\left\{\text{Emb}\left(i\mid\text{Sub}^{\left(\ell\right)}\left[v\right]\right)\mid i\in{\mathcal N}_k\left(v\right)\right\}\right)\end{array}$$\end{document}hv(ℓ+1)|Subgraph=GCNℓSubℓv=MEANEmbi∣Subℓv∣i∈Nkv(3) The context encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|\text{Context}}$$\end{document}hv(ℓ+1)|Context aims to analyze the node within different subgraph contexts to capture broader feature information. By summarizing the embeddings of the node in various contexts, context encoding ensures that the node representation encompasses information from multiple perspectives, enhancing overall expressive capability, defined as Eq. 27:27\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\vert^{Context}}=MEAN\left(\left\{\text{Emb}\left(v\mid\text{Sub}^{\left(\ell\right)}\left[j\right]\right)\mid\forall js.t.\in{\mathcal N}_k\left(j\right)\right\}\right)\end{array}$$\end{document}hvContext=MEANEmbv∣Subℓj∣∀js.t.∈NkjBy combining centroid encoding, subgraph encoding, and context encoding, SubgraphGCN for layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}+1$$\end{document}ℓ+1 can be defined as Eq. 28:28\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{h}}_{v}^{\left({\ell}+1\right)}=SUM\left({\text{h}}_{v}^{({\ell}+1)|\text{Centroid}},{\text{h}}_{v}^{({\ell}+1)|\text{Subgraph}}, {\text{h}}_{v}^{({\ell}+1)|\text{Context}}\right)\end{array}$$\end{document}hvℓ+1=SUMhv(ℓ+1)|Centroid,hv(ℓ+1)|Subgraph,hv(ℓ+1)|Contextwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textit{Emb }\left(i\mid\text{Sub}^{\left(\ell\right)}\left[j\right]\text{ }\right)$$\end{document}Embi∣Subℓj represents the embedding of node when applying GCN at layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}$$\end{document}ℓ.By using the aggregation method of the 1-hop egonet instead of the traditional star pattern, SubgraphGCN can effectively recognize different connection patterns exhibited by nodes with the same degree across different graphs, significantly enhancing the model’s discriminative power. This innovative approach not only enhances the understanding of relationships between nodes but also helps capture more fine-grained structural features, effectively reflecting the complexity of the graph. The advantages of SubgraphGCN are also evident in its intelligent decomposition of the entire large graph into smaller, more manageable subgraphs. Through this decomposition, the model can leverage GCN for more detailed feature aggregation and learning within relatively smaller scopes, improving computational efficiency and allowing the model to delve deeper into the potential information of each subgraph. This method ensures that key features can be comprehensively and effectively extracted when processing raw heterogeneous graphs, providing strong support for subsequent analysis and predictions. SubgraphGCN fully leverages the unique advantages of graph neural networks in modeling graph structural information. By systematically performing key steps such as subgraph extraction, node feature processing, and feature aggregation, the model can better understand the relationship between local and global structures. This detailed and comprehensive feature extraction capability is particularly important for PPI prediction tasks.

SubgraphGCN

Message Passing Neural Networks (MPNNs) [64], as one of the classic implementations of Graph Neural Networks (GNNs), has been widely applied to protein–protein interaction prediction tasks and has demonstrated exceptional performance. In layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}+1$$\end{document}ℓ+1 the feature of node v is calculated as Eq.  22:22\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}h_v^{\left(\ell+1\right)}=\gamma\left(h_v^{\left(\ell\right)},\text{AGGREGATE}\left(\{\phi\left(h_j^{\left(\ell\right)},e_{vj}\right)\mid j\in\mathcal N\left(v\right)\}\right)\right)\end{array}$$\end{document}hvℓ+1=γhvℓ,AGGREGATE{ϕhjℓ,evj∣j∈Nv}

Here, denotes the message generation process; AGGREGATE indicates the function used to aggregate messages from neighboring nodes (e.g., sum, mean, or max), and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma$$\end{document}γ is the function that updates the node features. The choice of different message functions, aggregation functions, and update functions affects the expressive power and performance of MPNNs. When all three functions are injective (one-to-one mappings), MPNNs achieve the same expressive power as the 1-Weisfeiler-Lehman (1-WL) isomorphism test [36].

During the computation of MPNN, each node follows a star pattern during message aggregation, where the central node of the graph is connected to all its neighbors and aggregates messages solely from its direct neighbors. The star pattern around the central node v is defined as Eq. 23:23\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Star\left(v\right)=\left({\mathcal{N}}_{1}\left(v\right),\{\left(v,j\right)\in \mathcal{E}|j\in \mathcal{N}\left(v\right)\}\right)\end{array}$$\end{document}Starv=N1v,{v,j∈E|j∈Nv}where \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{N}}_{1}\left(v\right)$$\end{document}N1v signifies the subgraph centered at node v, including v and all nodes and edges reachable within one hop. When handling non-isomorphic regular graphs, MPNN may struggle to differentiate these graphs, weakening its discriminative ability. To address this issue, Subgraph-1-WL extends the 1-WL algorithm by generalizing the concept of the star graph Star(v) to the k-hop neighborhood \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G\left[{N}_{k}\left(v\right)\right]$$\end{document}GNkv centered at v, thereby improving the expressive power for graph isomorphism detection. SubgraphGCN [65] combines Subgraph-1-WL and GCN, with its internal node feature update process defined as Eq. 24:24\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{h}_{v}^{\left({\ell}+1\right)}={\text{GCN}}^{\left({\ell}\right)}\left(Su{b}^{\left({\ell}\right)}\left[v\right]\right),\hspace{1em}l=0,\dots ,L-1\end{array}$$\end{document}hvℓ+1=GCNℓSubℓv,l=0,⋯,L-1

Here, indicates the one-hop subgraph centered at node v. The subgraph generation process primarily includes the following steps: First, based on the edge indices and the number of nodes in the original graph, a 1-hop neighborhood expansion is used to generate local subgraphs for each node. Simultaneously, a Boolean mask matrix is generated to indicate which nodes belong to each subgraph, and the hop counts between nodes are recorded. Next, the dense mask is converted into a sparse format for node and edge indices. Finally, the edges of all subgraphs are merged into a global edge index, while node IDs are remapped to avoid overlaps. After obtaining all subgraphs corresponding to the PPI network graph, SubgraphGCN extracts node features from the global features, integrating the central node features, mean-pooled topological features within the subgraph, and globally aggregated contextual features across subgraphs, resulting in richer and more comprehensive feature representations.

As shown in Fig. 12, SubgraphGCN computes three types of encodings:Fig. 12The architecture of SubgraphGCN. SubgraphGCN decomposes large graphs into smaller subgraphs by combining centroid encoding, subgraph encoding, and context encoding, utilizing the 1-hop egonet aggregation method. This enhances the model’s ability to recognize node connection patterns and improves computational efficiency

The architecture of SubgraphGCN. SubgraphGCN decomposes large graphs into smaller subgraphs by combining centroid encoding, subgraph encoding, and context encoding, utilizing the 1-hop egonet aggregation method. This enhances the model’s ability to recognize node connection patterns and improves computational efficiency

(1) Centroid encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|Centroid}$$\end{document}hv(ℓ+1)|Centroid focuses on representing the features of the central node within the subgraph. It efficiently encodes the local characteristics of node v by embedding within the node's star-shaped subgraph. This method effectively integrates the information from direct neighbors, enhancing the self-representation of the node, defined as Eq. 25:25\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\text{|}Centroid}=\;\mathrm{Emb}\left(v\mid\text{Sub}^{\left(\ell+1\right)}\left[v\right]\right)\end{array}$$\end{document}hv(ℓ+1)|Centroid=Embv∣Subℓ+1v

(2) The subgraph encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|\text{Subgraph}}$$\end{document}hv(ℓ+1)|Subgraph aggregates features from the subgraph surrounding node v to form a comprehensive node representation. By applying the mean pooling operation, the subgraph encoding integrates rich information from neighboring nodes, thus providing an in-depth characterization of complex relationships between nodes, defined as Eq. 26:26\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\vert \text{Subgraph}}=\text{GCN}^{\left(\ell\right)}\left(\text{Sub}^{\left(\ell\right)}\left[v\right]\right)=MEAN\left(\left\{\text{Emb}\left(i\mid\text{Sub}^{\left(\ell\right)}\left[v\right]\right)\mid i\in{\mathcal N}_k\left(v\right)\right\}\right)\end{array}$$\end{document}hv(ℓ+1)|Subgraph=GCNℓSubℓv=MEANEmbi∣Subℓv∣i∈Nkv

(3) The context encoding \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{h}}_{v}^{({\ell}+1)|\text{Context}}$$\end{document}hv(ℓ+1)|Context aims to analyze the node within different subgraph contexts to capture broader feature information. By summarizing the embeddings of the node in various contexts, context encoding ensures that the node representation encompasses information from multiple perspectives, enhancing overall expressive capability, defined as Eq. 27:27\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}\text{h}_v^{(\ell+1)\vert^{Context}}=MEAN\left(\left\{\text{Emb}\left(v\mid\text{Sub}^{\left(\ell\right)}\left[j\right]\right)\mid\forall js.t.\in{\mathcal N}_k\left(j\right)\right\}\right)\end{array}$$\end{document}hvContext=MEANEmbv∣Subℓj∣∀js.t.∈Nkj

By combining centroid encoding, subgraph encoding, and context encoding, SubgraphGCN for layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}+1$$\end{document}ℓ+1 can be defined as Eq. 28:28\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\text{h}}_{v}^{\left({\ell}+1\right)}=SUM\left({\text{h}}_{v}^{({\ell}+1)|\text{Centroid}},{\text{h}}_{v}^{({\ell}+1)|\text{Subgraph}}, {\text{h}}_{v}^{({\ell}+1)|\text{Context}}\right)\end{array}$$\end{document}hvℓ+1=SUMhv(ℓ+1)|Centroid,hv(ℓ+1)|Subgraph,hv(ℓ+1)|Contextwhere \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textit{Emb }\left(i\mid\text{Sub}^{\left(\ell\right)}\left[j\right]\text{ }\right)$$\end{document}Embi∣Subℓj represents the embedding of node when applying GCN at layer \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\ell}$$\end{document}ℓ.

By using the aggregation method of the 1-hop egonet instead of the traditional star pattern, SubgraphGCN can effectively recognize different connection patterns exhibited by nodes with the same degree across different graphs, significantly enhancing the model’s discriminative power. This innovative approach not only enhances the understanding of relationships between nodes but also helps capture more fine-grained structural features, effectively reflecting the complexity of the graph. The advantages of SubgraphGCN are also evident in its intelligent decomposition of the entire large graph into smaller, more manageable subgraphs. Through this decomposition, the model can leverage GCN for more detailed feature aggregation and learning within relatively smaller scopes, improving computational efficiency and allowing the model to delve deeper into the potential information of each subgraph. This method ensures that key features can be comprehensively and effectively extracted when processing raw heterogeneous graphs, providing strong support for subsequent analysis and predictions. SubgraphGCN fully leverages the unique advantages of graph neural networks in modeling graph structural information. By systematically performing key steps such as subgraph extraction, node feature processing, and feature aggregation, the model can better understand the relationship between local and global structures. This detailed and comprehensive feature extraction capability is particularly important for PPI prediction tasks.

Classifier and loss functionLet \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=\{{P}_{1},\cdots ,{P}_{N}\}$$\end{document}P={P1,⋯,PN} be the set of proteins. \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E=\left\{\left({P}_{i}, {P}_{j}\right) \right| i\ne j\}$$\end{document}E=Pi,Pji≠j} is the collection of PPIs. A set of type labels \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=\left\{{L}_{1}, {L}_{2},\cdots , {L}_{7}\right\}$$\end{document}L=L1,L2,⋯,L7 corresponds to each protein pair \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({P}_{i},{P}_{j}\right)$$\end{document}Pi,Pj. If there are multiple interaction types between \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{P}}_{\text{i}}$$\end{document}Pi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj, the corresponding position in the type label set L is set to 1; otherwise, it is set to 0. We treat proteins and PPIs as nodes and edges, respectively, thus constructing a comprehensive PPI network graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G=\left(P, E\right)$$\end{document}G=P,E. Since each PPI has at least one and at most seven types of PPI interactions, we extract the corresponding edge set \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{k}=\left\{\left({P}_{i}, {P}_{j}\right) \right| i\ne j, {L}_{k}=1\}$$\end{document}Ek=Pi,Pji≠j,Lk=1} to construct seven independent PPI network graphs \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{k}=\left(P,{E}_{k}\right)$$\end{document}Gk=P,Ek.As shown in Fig. 13, the final protein representation is \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}=\left[X,{z}_{1},{z}_{2},\cdots ,{z}_{7}\right]$$\end{document}Pi=X,z1,z2,⋯,z7, where X represents the multimodal protein features generated by the pre-trained model FAE, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}_{1},{z}_{2},\cdots ,{z}_{7}$$\end{document}z1,z2,⋯,z7 denote the protein features learned by MESM from seven independent PPI network graphs \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{k}$$\end{document}Gk, with [] indicating horizontal concatenation. This representation not only preserves the original protein information but also integrates unique protein association information from different interaction perspectives, further enhancing MESM’s ability to learn interactions among these seven proteins.Fig. 13The architecture of Classifier. By horizontally concatenating multimodal protein features with features from seven different PPI network graphs, a joint representation is constructed, and interaction features of protein pairs are extracted through element-wise multiplication. Finally, a multilayer perceptron is used to estimate the interaction probabilities between proteinsWe perform element-wise multiplication on \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi  and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj  to construct the joint representation between protein pairs, as shown in Eq. 29:
29\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm H}_{{\mathrm P}_{\mathrm i},{\mathrm P}_{\mathrm j}}={\mathrm P}_{\mathrm i}\odot{\mathrm P}_{\mathrm j}$$\end{document}HPi,Pj=Pi⊙PjThis element-wise multiplication operation effectively combines the feature information of the two proteins, thus extracting their interaction features. This joint representation is then fed into MLP to estimate the interaction probability between proteins \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj. The final output is defined as Eq. 30:30\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{\text{y}}}_{\left({\text{P}}_{\text{i}},{\text{P}}_{\text{j}}\right)}=Sig\left(\text{MLP}\left({{\text{H}}_{{\text{P}}_{\text{i}},{\text{P}}_{\text{j}}}}\right)\right)\end{array}$$\end{document}y^Pi,Pj=SigMLPHPi,PjTo train this model, we adopt the ZLPR (zero-bounded log-sum-exp & pairwise rank-based) [66] loss function, which is a loss function for multi-label classification problems and can be viewed as a natural generalization of “Softmax and cross-entropy.” The form of the ZLPR loss function is defined as Eq. 31:31\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\mathcal{L}}_{\text{zlpr}}=\text{log}\left(1+\sum_{\text{i}\in {\Omega }_{\text{pos}}} {\text{e}}^{-{\text{s}}_{\text{i}}}\right)+\text{log}\left(1+\sum_{\text{j}\in {\Omega }_{\text{neg}}} {\text{e}}^{{\text{s}}_{\text{j}}}\right)\end{array}$$\end{document}Lzlpr=log1+∑i∈Ωpose-si+log1+∑j∈ΩnegesjHere, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}$$\end{document}si is the output score of the model for the ith class(\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\lambda }_{i}$$\end{document}λi; during prediction, if \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}>0$$\end{document}si>0, it indicates that \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\lambda }_{i}$$\end{document}λi could be the target class, while if \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}<0$$\end{document}si<0, it indicates otherwise. The ZLPR loss function is characterized by its ability to effectively capture dependencies between labels, allowing for a reasonable balance of influence between positive and negative classes in the model.

Classifier and loss function

Let \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P=\{{P}_{1},\cdots ,{P}_{N}\}$$\end{document}P={P1,⋯,PN} be the set of proteins. \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E=\left\{\left({P}_{i}, {P}_{j}\right) \right| i\ne j\}$$\end{document}E=Pi,Pji≠j} is the collection of PPIs. A set of type labels \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L=\left\{{L}_{1}, {L}_{2},\cdots , {L}_{7}\right\}$$\end{document}L=L1,L2,⋯,L7 corresponds to each protein pair \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left({P}_{i},{P}_{j}\right)$$\end{document}Pi,Pj. If there are multiple interaction types between \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{P}}_{\text{i}}$$\end{document}Pi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj, the corresponding position in the type label set L is set to 1; otherwise, it is set to 0. We treat proteins and PPIs as nodes and edges, respectively, thus constructing a comprehensive PPI network graph \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$G=\left(P, E\right)$$\end{document}G=P,E. Since each PPI has at least one and at most seven types of PPI interactions, we extract the corresponding edge set \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${E}_{k}=\left\{\left({P}_{i}, {P}_{j}\right) \right| i\ne j, {L}_{k}=1\}$$\end{document}Ek=Pi,Pji≠j,Lk=1} to construct seven independent PPI network graphs \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{k}=\left(P,{E}_{k}\right)$$\end{document}Gk=P,Ek.

As shown in Fig. 13, the final protein representation is \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}=\left[X,{z}_{1},{z}_{2},\cdots ,{z}_{7}\right]$$\end{document}Pi=X,z1,z2,⋯,z7, where X represents the multimodal protein features generated by the pre-trained model FAE, and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}_{1},{z}_{2},\cdots ,{z}_{7}$$\end{document}z1,z2,⋯,z7 denote the protein features learned by MESM from seven independent PPI network graphs \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${G}_{k}$$\end{document}Gk, with [] indicating horizontal concatenation. This representation not only preserves the original protein information but also integrates unique protein association information from different interaction perspectives, further enhancing MESM’s ability to learn interactions among these seven proteins.Fig. 13The architecture of Classifier. By horizontally concatenating multimodal protein features with features from seven different PPI network graphs, a joint representation is constructed, and interaction features of protein pairs are extracted through element-wise multiplication. Finally, a multilayer perceptron is used to estimate the interaction probabilities between proteins

The architecture of Classifier. By horizontally concatenating multimodal protein features with features from seven different PPI network graphs, a joint representation is constructed, and interaction features of protein pairs are extracted through element-wise multiplication. Finally, a multilayer perceptron is used to estimate the interaction probabilities between proteins

We perform element-wise multiplication on \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi  and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj  to construct the joint representation between protein pairs, as shown in Eq. 29:

29\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathrm H}_{{\mathrm P}_{\mathrm i},{\mathrm P}_{\mathrm j}}={\mathrm P}_{\mathrm i}\odot{\mathrm P}_{\mathrm j}$$\end{document}HPi,Pj=Pi⊙Pj

This element-wise multiplication operation effectively combines the feature information of the two proteins, thus extracting their interaction features. This joint representation is then fed into MLP to estimate the interaction probability between proteins \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{i}$$\end{document}Pi and \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${P}_{j}$$\end{document}Pj. The final output is defined as Eq. 30:30\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\widehat{\text{y}}}_{\left({\text{P}}_{\text{i}},{\text{P}}_{\text{j}}\right)}=Sig\left(\text{MLP}\left({{\text{H}}_{{\text{P}}_{\text{i}},{\text{P}}_{\text{j}}}}\right)\right)\end{array}$$\end{document}y^Pi,Pj=SigMLPHPi,Pj

To train this model, we adopt the ZLPR (zero-bounded log-sum-exp & pairwise rank-based) [66] loss function, which is a loss function for multi-label classification problems and can be viewed as a natural generalization of “Softmax and cross-entropy.” The form of the ZLPR loss function is defined as Eq. 31:31\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}{\mathcal{L}}_{\text{zlpr}}=\text{log}\left(1+\sum_{\text{i}\in {\Omega }_{\text{pos}}} {\text{e}}^{-{\text{s}}_{\text{i}}}\right)+\text{log}\left(1+\sum_{\text{j}\in {\Omega }_{\text{neg}}} {\text{e}}^{{\text{s}}_{\text{j}}}\right)\end{array}$$\end{document}Lzlpr=log1+∑i∈Ωpose-si+log1+∑j∈Ωnegesj

Here, \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}$$\end{document}si is the output score of the model for the ith class(\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\lambda }_{i}$$\end{document}λi; during prediction, if \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}>0$$\end{document}si>0, it indicates that \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\lambda }_{i}$$\end{document}λi could be the target class, while if \documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${s}_{i}<0$$\end{document}si<0, it indicates otherwise. The ZLPR loss function is characterized by its ability to effectively capture dependencies between labels, allowing for a reasonable balance of influence between positive and negative classes in the model.

Experimental settings and evaluation metricsIn this study, we employed the Adam algorithm [67] to optimize the training process for the pre-training of multimodal proteins. The entire training process lasted for 200 epochs, with the learning rate set to 0.001 to ensure effective learning of the model. For different network architectures, we adopted varying batch sizes, with the batch sizes for SVAE, VGAE, PAE, and FAE set to 256, 256, 32, and 1024, respectively. In constructing the PPI prediction network, we utilized the AdamW optimization algorithm [68], which combines momentum updates with weight decay from the Adam algorithm, effectively preventing overfitting. The learning rate was set to 0.0001, and training was conducted for a total of 100 epochs to ensure that the model could fully learn the features of interactions between proteins. Additionally, we used a learning rate scheduler [68] that optimized the learning rate during the model training process through an initial linear increase strategy followed by gradual decay, allowing the model to better adapt to the needs of parameter updates at different training stages. We selected a 1024-dimensional protein feature representation and a 20-dimensional structural encoding dimension to capture the diversity and complexity of proteins. For each different type of PPI network graph, we used a single layer of GraphGPS, GAT, SubgraphGCN, and GCN, leveraging these graph neural networks to capture local and global graph structural information more effectively, thus enhancing the modeling capability for complex protein networks.MESM was implemented in Python 3.10, leveraging PyTorch 2.2 for deep learning operations and PyTorch Geometric (PyG) 2.6 for graph-based computations. All experiments were conducted on an NVIDIA A100 GPU under a Linux operating system. We sequentially trained MESM on four datasets, with the model containing a total of 172.49 million trainable parameters. Table 6 presents the computational cost of MESM, showing that as the PPI network becomes more complex, the model requires more training time and greater GPU memory. More specific implementation details about the dataset and code can be obtained from 10.5281/zenodo.15321475.
Table 6The computational cost of training MESMDatasetRuntime (seconds)GPU memory (GB)SHS27k2988.33SHS148k137428.83SYS30k40611.88SYS60k76118.9In terms of dataset partitioning, we adopted three different partitioning strategies [35]: Breadth-First Search (BFS), Depth-First Search (DFS), and Random division, splitting the PPI dataset into training and test sets in an 8:2 ratio. During the dataset partitioning process, we used a random seed to ensure that the combinations of the training and test sets were different during each training session, thereby enhancing the model’s generalization ability. Each experiment was repeated five times to obtain the mathematical expectation and standard deviation of the results, which will serve as indicators of predictive performance.Since the distribution of different PPI types in our dataset is imbalanced, we chose Micro-F1 as the multi-label classification evaluation metric for handling imbalanced datasets. Micro-F1 effectively consolidates the predictive performance across multiple classes, particularly excelling when dealing with imbalanced samples. To this end, the precision and recall for the ith class are defined in Eq. 32 and 33:32\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Precision_{i}=\frac{{\text{TP}}_{i}}{{\text{TP}}_{i}+{\text{FP}}_{i}}\end{array}$$\end{document}Precisioni=TPiTPi+FPi33\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Recall_{i}=\frac{{\text{TP}}_{i}}{{\text{TP}}_{i}+{\text{FN}}_{i}}\end{array}$$\end{document}Recalli=TPiTPi+FNiwhere TP (True Positive) denotes the number of samples correctly classified as positive, FP (False Positive) denotes the number of samples incorrectly classified as positive, and FN (False Negative) denotes the number of samples that are actually positive but not classified as such.In our research task, the definition of Micro-F1 is defined in Eq. 34:\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Precision}}_{\text{Micro}}=\frac{\sum_{i=1}^{7}T{P}_{i}}{\sum_{i=1}^{7}T{P}_{i}+\sum_{i=1}^{7}F{P}_{i}}$$\end{document}PrecisionMicro=∑i=17TPi∑i=17TPi+∑i=17FPi34\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Recall_{\text{Micro}}=\frac{\sum_{i=1}^{7}T{P}_{i}}{\sum_{i=1}^{7}T{P}_{i}+\sum_{i=1}^{7}{FN}_{i}}\end{array}$$\end{document}RecallMicro=∑i=17TPi∑i=17TPi+∑i=17FNi\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Micro}-\text{F}1=2\times \frac{{\text{Precision}}_{\text{Micro}}\times {\text{Recall}}_{\text{Micro}}}{{\text{Precision}}_{\text{Micro}}+{\text{Recall}}_{\text{Micro}}}$$\end{document}Micro-F1=2×PrecisionMicro×RecallMicroPrecisionMicro+RecallMicroBy utilizing these metrics, we can comprehensively evaluate the model’s performance in a multi-label environment, ensuring that it performs well not only in specific classes but also enhances overall generalization capability.

Experimental settings and evaluation metrics

In this study, we employed the Adam algorithm [67] to optimize the training process for the pre-training of multimodal proteins. The entire training process lasted for 200 epochs, with the learning rate set to 0.001 to ensure effective learning of the model. For different network architectures, we adopted varying batch sizes, with the batch sizes for SVAE, VGAE, PAE, and FAE set to 256, 256, 32, and 1024, respectively. In constructing the PPI prediction network, we utilized the AdamW optimization algorithm [68], which combines momentum updates with weight decay from the Adam algorithm, effectively preventing overfitting. The learning rate was set to 0.0001, and training was conducted for a total of 100 epochs to ensure that the model could fully learn the features of interactions between proteins. Additionally, we used a learning rate scheduler [68] that optimized the learning rate during the model training process through an initial linear increase strategy followed by gradual decay, allowing the model to better adapt to the needs of parameter updates at different training stages. We selected a 1024-dimensional protein feature representation and a 20-dimensional structural encoding dimension to capture the diversity and complexity of proteins. For each different type of PPI network graph, we used a single layer of GraphGPS, GAT, SubgraphGCN, and GCN, leveraging these graph neural networks to capture local and global graph structural information more effectively, thus enhancing the modeling capability for complex protein networks.

MESM was implemented in Python 3.10, leveraging PyTorch 2.2 for deep learning operations and PyTorch Geometric (PyG) 2.6 for graph-based computations. All experiments were conducted on an NVIDIA A100 GPU under a Linux operating system. We sequentially trained MESM on four datasets, with the model containing a total of 172.49 million trainable parameters.

Table 6 presents the computational cost of MESM, showing that as the PPI network becomes more complex, the model requires more training time and greater GPU memory. More specific implementation details about the dataset and code can be obtained from 10.5281/zenodo.15321475.
Table 6The computational cost of training MESMDatasetRuntime (seconds)GPU memory (GB)SHS27k2988.33SHS148k137428.83SYS30k40611.88SYS60k76118.9

The computational cost of training MESM

In terms of dataset partitioning, we adopted three different partitioning strategies [35]: Breadth-First Search (BFS), Depth-First Search (DFS), and Random division, splitting the PPI dataset into training and test sets in an 8:2 ratio. During the dataset partitioning process, we used a random seed to ensure that the combinations of the training and test sets were different during each training session, thereby enhancing the model’s generalization ability. Each experiment was repeated five times to obtain the mathematical expectation and standard deviation of the results, which will serve as indicators of predictive performance.

Since the distribution of different PPI types in our dataset is imbalanced, we chose Micro-F1 as the multi-label classification evaluation metric for handling imbalanced datasets. Micro-F1 effectively consolidates the predictive performance across multiple classes, particularly excelling when dealing with imbalanced samples. To this end, the precision and recall for the ith class are defined in Eq. 32 and 33:32\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Precision_{i}=\frac{{\text{TP}}_{i}}{{\text{TP}}_{i}+{\text{FP}}_{i}}\end{array}$$\end{document}Precisioni=TPiTPi+FPi33\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Recall_{i}=\frac{{\text{TP}}_{i}}{{\text{TP}}_{i}+{\text{FN}}_{i}}\end{array}$$\end{document}Recalli=TPiTPi+FNiwhere TP (True Positive) denotes the number of samples correctly classified as positive, FP (False Positive) denotes the number of samples incorrectly classified as positive, and FN (False Negative) denotes the number of samples that are actually positive but not classified as such.

In our research task, the definition of Micro-F1 is defined in Eq. 34:\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Precision}}_{\text{Micro}}=\frac{\sum_{i=1}^{7}T{P}_{i}}{\sum_{i=1}^{7}T{P}_{i}+\sum_{i=1}^{7}F{P}_{i}}$$\end{document}PrecisionMicro=∑i=17TPi∑i=17TPi+∑i=17FPi34\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{c}Recall_{\text{Micro}}=\frac{\sum_{i=1}^{7}T{P}_{i}}{\sum_{i=1}^{7}T{P}_{i}+\sum_{i=1}^{7}{FN}_{i}}\end{array}$$\end{document}RecallMicro=∑i=17TPi∑i=17TPi+∑i=17FNi\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text{Micro}-\text{F}1=2\times \frac{{\text{Precision}}_{\text{Micro}}\times {\text{Recall}}_{\text{Micro}}}{{\text{Precision}}_{\text{Micro}}+{\text{Recall}}_{\text{Micro}}}$$\end{document}Micro-F1=2×PrecisionMicro×RecallMicroPrecisionMicro+RecallMicro

By utilizing these metrics, we can comprehensively evaluate the model’s performance in a multi-label environment, ensuring that it performs well not only in specific classes but also enhances overall generalization capability.

Supplementary Information
Additonal file 1: Additional file 1: Table 1. The performance of MESM and other methods on SHS27k and SHS148k under different evaluation metrics. Table 2. The performance of MESM and other methods on SYS30k and SYS60k under different evaluation metrics

Supplementary Information

Additonal file 1: Additional file 1: Table 1. The performance of MESM and other methods on SHS27k and SHS148k under different evaluation metrics. Table 2. The performance of MESM and other methods on SYS30k and SYS60k under different evaluation metrics

Additonal file 1: Additional file 1: Table 1. The performance of MESM and other methods on SHS27k and SHS148k under different evaluation metrics. Table 2. The performance of MESM and other methods on SYS30k and SYS60k under different evaluation metrics
